{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ensemble Methods in Python - solutions.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"9UaGzQvJrli7","colab_type":"code","colab":{}},"source":["# https://www.datacamp.com/courses/ensemble-methods-in-python"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RsbKdJoir62k","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3cQ0L878r648","colab_type":"code","colab":{}},"source":["from sklearn.tree import DecisionTreeRegressor"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xyi2zsqRroCC","colab_type":"text"},"source":["**Course Description**\n","\n","Continue your machine learning journey by diving into the wonderful world of ensemble learning methods! These are an exciting class of machine learning techniques that combine multiple individual algorithms to boost performance and solve complex problems at scale across different industries. Ensemble techniques regularly win online machine learning competitions as well! In this course, you’ll learn all about these advanced ensemble techniques, such as bagging, boosting, and stacking. You’ll apply them to real-world datasets using cutting edge Python machine learning libraries such as scikit-learn, XGBoost, CatBoost, and mlxtend."]},{"cell_type":"markdown","metadata":{"id":"m-ZBsQp5roEp","colab_type":"text"},"source":["## 1. Combining Multiple Models\n","\n","Do you struggle to determine which of the models you built is the best for your problem? You should give up on that, and use them all instead! In this chapter, you'll learn how to **combine multiple models into one using \"Voting\" and \"Averaging\"**. You'll use these to predict the ratings of apps on the Google Play Store, whether or not a Pokémon is legendary, and which characters are going to die in Game of Thrones!"]},{"cell_type":"markdown","metadata":{"id":"HP28mzz8vg9j","colab_type":"text"},"source":["#### Predicting the rating of an app\n","\n","Having explored the Google apps dataset in the previous exercise, let's now build a model that predicts the rating of an app given a subset of its features.\n","\n","To do this, you'll use scikit-learn's DecisionTreeRegressor. As decision trees are the building blocks of many ensemble models, refreshing your memory of how they work will serve you well throughout this course.\n","\n","We'll use the MAE (mean absolute error) as the evaluation metric. This metric is highly interpretable, as it represents the average absolute difference between actual and predicted ratings."]},{"cell_type":"code","metadata":{"id":"Dq80Am5Sr7Qs","colab_type":"code","colab":{}},"source":["# Split into train (80%) and test(20%) sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Instantiate the regressor\n","reg_dt = DecisionTreeRegressor(min_samples_leaf=3, min_samples_split=9, random_state=500)\n","\n","# Fit to the training set\n","reg_dt.fit(X_train, y_train)\n","\n","# Evaluate the performance of the model on the test set\n","y_pred = reg_dt.predict(X_test)\n","print('MAE: {:.3f}'.format(mean_absolute_error(y_test, y_pred)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BTjnWFcxzaxf","colab_type":"text"},"source":["**Observation:**\n","\n","As the MAE is around 0.61, the error of the model is, on average, less than one rating point away from the actual value! Having refreshed our understanding of how to build scikit-learn models, let's now jump into our first ensemble method: Voting!"]},{"cell_type":"markdown","metadata":{"id":"vD49H1mwzhbu","colab_type":"text"},"source":["#### Voting: Choosing the best model\n","\n","In this exercise, you'll compare different classifiers and choose the one that performs the best.\n","\n","Three individual classifiers have been fitted to the training set:\n","\n","- clf_lr is a logistic regression.\n","- clf_dt is a decision tree.\n","- clf_knn is a 5-nearest neighbors classifier."]},{"cell_type":"code","metadata":{"id":"zkm9mDtCvw9T","colab_type":"code","colab":{}},"source":["# Predict the labels of the test set\n","pred_lr = clf_lr.predict(X_test)\n","pred_dt = clf_dt.predict(X_test)\n","pred_knn = clf_knn.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGdYSFEEvxA0","colab_type":"code","colab":{}},"source":["# Evaluate the performance of each model\n","score_lr = f1_score(y_test, pred_lr)\n","score_dt = f1_score(y_test, pred_dt)\n","score_knn = f1_score(y_test, pred_knn)\n","\n","# Print the scores\n","print(score_lr)\n","print(score_dt)\n","print(score_knn)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jH8EZlYSz4Fg","colab_type":"text"},"source":["#### Assembling your first ensemble\n","\n","Your job is to leverage the voting ensemble technique using the sklearn API. It's up to you to instantiate the individual models and pass them as parameters to build your first voting classifier."]},{"cell_type":"code","metadata":{"id":"sxsof7rWzzKB","colab_type":"code","colab":{}},"source":["# Instantiate the individual models\n","clf_knn = KNeighborsClassifier(n_neighbors=5)\n","clf_lr = LogisticRegression(class_weight='balanced')\n","clf_dt = DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9, random_state=500)\n","\n","# Create and fit the voting classifier\n","clf_vote = VotingClassifier(\n","    estimators=[('knn', clf_knn), ('lr', clf_lr), ('dt', clf_dt)]\n",")\n","clf_vote.fit(X_train, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"szcjdpUqz_uq","colab_type":"text"},"source":["#### Evaluating your ensemble\n","\n","Remember to use f1_score() to evaluate the performance. In addition, you'll create a classification report on the test set (X_test, y_test) using the classifiation_report() function."]},{"cell_type":"code","metadata":{"id":"tyeWQ-cXvxDT","colab_type":"code","colab":{}},"source":["# Calculate the predictions using the voting classifier\n","pred_vote = clf_vote.predict(X_test)\n","\n","# Calculate the F1-Score of the voting classifier\n","score_vote = f1_score(y_test, pred_vote)\n","print('F1-Score: {:.3f}'.format(score_vote))\n","\n","# Calculate the classification report\n","report = classification_report(y_test, pred_vote)\n","print(report)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3x1dNGVg0MCt","colab_type":"text"},"source":["**Observation:**\n","\n","The performance is not higher, but is at least as good as the best individual model - the decision tree. This may be because the other two models had a performance around 10% lower than the decision tree."]},{"cell_type":"markdown","metadata":{"id":"j-VJXDdG0PgN","colab_type":"text"},"source":["### Averaging"]},{"cell_type":"markdown","metadata":{"id":"epdSjz6A0WO0","colab_type":"text"},"source":["#### Predicting GoT deaths\n","\n","Let's now build an ensemble model using the averaging technique. The following individual models have been built:\n","\n","Logistic Regression (clf_lr).\n","Decision Tree (clf_dt).\n","Support Vector Machine (clf_svm).\n","As the target is binary, all these models might have good individual performance. Your objective is to combine them using averaging. Recall from the video that this is the same as a soft voting approach, so you should still use the VotingClassifier()."]},{"cell_type":"code","metadata":{"id":"UZVUit180za7","colab_type":"code","colab":{}},"source":["# Build the individual models\n","clf_lr = LogisticRegression(class_weight='balanced')\n","clf_dt = DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9, random_state=500)\n","clf_svm = SVC(probability=True, class_weight='balanced', random_state=500)\n","\n","# List of (string, estimator) tuples\n","estimators = [('lr', clf_lr), ('dt', clf_dt), ('svm', clf_svm)]\n","\n","# Build and fit an averaging classifier\n","clf_avg = VotingClassifier(estimators, voting='soft')\n","clf_avg.fit(X_train, y_train)\n","\n","# Evaluate model performance\n","acc_avg = accuracy_score(y_test,  clf_avg.predict(X_test))\n","print('Accuracy: {:.2f}'.format(acc_avg))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KLFxmfH701m4","colab_type":"text"},"source":["#### Soft vs. hard voting\n","\n","Three individual classifiers have been instantiated for you:\n","\n","- A DecisionTreeClassifier (clf_dt).\n","- A LogisticRegression (clf_lr).\n","-A KNeighborsClassifier (clf_knn).\n","\n","Your task is to try both voting and averaging to determine which is better."]},{"cell_type":"code","metadata":{"id":"ssLXsep60FJ8","colab_type":"code","colab":{}},"source":["# List of (string, estimator) tuples\n","estimators = [('dt', clf_dt), ('lr', clf_lr), ('knn', clf_knn)]\n","\n","# Build and fit a voting classifier\n","clf_vote = VotingClassifier(estimators)\n","clf_vote.fit(X_train, y_train)\n","\n","# Build and fit an averaging classifier\n","clf_avg = VotingClassifier(estimators, voting='soft')\n","clf_avg.fit(X_train, y_train)\n","\n","# Evaluate the performance of both models\n","acc_vote = accuracy_score(y_test, clf_vote.predict(X_test))\n","acc_avg = accuracy_score(y_test,  clf_avg.predict(X_test))\n","print('Voting: {:.2f}, Averaging: {:.2f}'.format(acc_vote, acc_avg))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"09HPK8fcr7W9","colab_type":"text"},"source":["## 2. Bagging\n","\n","Bagging is the ensemble method behind powerful machine learning algorithms such as random forests. In this chapter you'll learn the theory behind this technique and build your own bagging models using scikit-learn."]},{"cell_type":"markdown","metadata":{"id":"btEH109X1Fvq","colab_type":"text"},"source":["### The strength of “weak” models\n"]},{"cell_type":"markdown","metadata":{"id":"Lfhk4EEg1IQM","colab_type":"text"},"source":["#### Restricted and unrestricted decision trees\n","\n","Here, you will build two separate decision tree classifiers. In the first, you will specify the parameters `min_sample_leaf` and `min_sample_split`, but not a maximum depth, so that the tree can fully develop without any restrictions.\n","\n","In the second, you will specify some constraints by limiting the depth of the decision tree. By then comparing the two models, you'll better understand the notion of a \"weak\" learner."]},{"cell_type":"code","metadata":{"id":"8mdHMHZX0FMC","colab_type":"code","colab":{}},"source":["# Build unrestricted decision tree\n","clf = DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9, random_state=500)\n","clf.fit(X_train, y_train)\n","\n","# Predict the labels\n","pred = clf.predict(X_test)\n","\n","# Print the confusion matrix\n","cm = confusion_matrix(y_test, pred)\n","print('Confusion matrix:\\n', cm)\n","\n","# Print the F1 score\n","score = f1_score(y_test, pred)\n","print('F1-Score: {:.3f}'.format(score))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O2nBGomzr_zN","colab_type":"code","colab":{}},"source":["# Build restricted decision tree\n","clf = DecisionTreeClassifier(max_depth=4, max_features=2, random_state=500)\n","clf.fit(X_train, y_train)\n","\n","# Predict the labels\n","pred = clf.predict(X_test)\n","\n","# Print the confusion matrix\n","cm = confusion_matrix(y_test, pred)\n","print('Confusion matrix:\\n', cm)\n","\n","# Print the F1 score\n","score = f1_score(y_test, pred)\n","print('F1-Score: {:.3f}'.format(score))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rTRj5LZf1eWm","colab_type":"text"},"source":["**Observation:**\n","\n","Model A is a fine-tuned decision tree, with a decent performance on its own. Model B is 'weak', restricted in height and with performance just above 50%."]},{"cell_type":"markdown","metadata":{"id":"IesveZmw1iM_","colab_type":"text"},"source":["### Bootstrap aggregating\n"]},{"cell_type":"markdown","metadata":{"id":"PIL-7p7u1ksX","colab_type":"text"},"source":["#### Training with bootstrapping\n","\n","Let's now build a \"weak\" decision tree classifier and train it on a sample of the training set drawn with replacement. This will help you understand what happens on every iteration of a bagging ensemble."]},{"cell_type":"code","metadata":{"id":"mVq3nfe01dz-","colab_type":"code","colab":{}},"source":["# Take a sample with replacement\n","X_train_sample = X_train.sample(frac=1.0, replace=True, random_state=42)\n","y_train_sample = y_train.loc[X_train_sample.index]\n","\n","# Build a \"weak\" Decision Tree classifier\n","clf = DecisionTreeClassifier(max_depth=4, max_features=2, random_state=500)\n","\n","# Fit the model to the training sample\n","clf.fit(X_train_sample, y_train_sample)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LrTAnqJq1wKn","colab_type":"text"},"source":["#### A first attempt at bagging\n","\n","You've seen what happens in a single iteration of a bagging ensemble. Now let's build a custom bagging model!\n","\n","Two functions have been prepared for you:\n","\n","\n","`def build_decision_tree(X_train, y_train, random_state=None):`\n","    # Takes a sample with replacement,\n","    # builds a \"weak\" decision tree,\n","    # and fits it to the train set\n","\n","\n","`def predict_voting(classifiers, X):`\n","    # Makes the individual predictions \n","    # and then combines them using \"Voting\""]},{"cell_type":"code","metadata":{"id":"IWNH1OIO181a","colab_type":"code","colab":{}},"source":["def build_decision_tree(X_train, y_train, random_state=None):\n","        # Take a sample with replacement\n","        X_train_sample = X_train.sample(frac=1.0, replace=True, random_state=random_state)\n","        y_train_sample = y_train.loc[X_train_sample.index]\n","\n","        # Build a \"weak\" Decision Tree classifier\n","        clf = DecisionTreeClassifier(max_depth=4, max_features=2, random_state=500)\n","\n","        # Fit the model on the training sample\n","        clf.fit(X_train_sample, y_train_sample)\n","        \n","        return clf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kmgmoSEd184i","colab_type":"code","colab":{}},"source":["def predict_voting(classifiers, X):\n","        # Make the individual predictions\n","        pred_list = [clf.predict(X) for clf in classifiers]\n","        \n","        # Combine the predictions using \"Voting\"\n","        pred_vote = []\n","        for i in range(X.shape[0]):\n","                individual_preds = np.array([pred[i] for pred in pred_list])\n","                combined_pred = stats.mode(individual_preds)[0][0]\n","                pred_vote.insert(i, combined_pred)\n","        \n","        return pred_vote\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nqwj-LwM1d22","colab_type":"code","colab":{}},"source":["# Build the list of individual models\n","clf_list = []\n","for i in range(21):\n","\tclf_list.append(build_decision_tree(X_train, y_train, random_state=i))\n","\n","# Predict on the test set\n","pred = predict_voting(clf_list, X_test)\n","\n","# Print the F1 score\n","print('F1 score: {:.3f}'.format(f1_score(y_test, pred)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"69V14NdG2TTm","colab_type":"text"},"source":["#### Bagging: the scikit-learn way\n","\n","Let's now apply scikit-learn's BaggingClassifier to the Pokémon dataset.\n","\n","You obtained an F1 score of around 0.56 with your custom bagging ensemble.\n","\n","Will `BaggingClassifier()` beat it? Time to find out!"]},{"cell_type":"code","metadata":{"id":"b2CPkBRD2Xh-","colab_type":"code","colab":{}},"source":["# Instantiate the base model\n","clf_dt = DecisionTreeClassifier(max_depth=4)\n","\n","# Build and train the Bagging classifier\n","clf_bag = BaggingClassifier(\n","  base_estimator=clf_dt,\n","  n_estimators=21,\n","  random_state=500)\n","clf_bag.fit(X_train, y_train)\n","\n","# Predict the labels of the test set\n","pred = clf_bag.predict(X_test)\n","\n","# Show the F1-score\n","print('F1-Score: {:.3f}'.format(f1_score(y_test, pred)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"22ZtAybJ2hTf","colab_type":"text"},"source":["#### Checking the out-of-bag score\n","\n","So far you've used the F1 score to measure performance. However, in this exercise you should use the accuracy score so that you can easily compare it to the out-of-bag score.\n","\n","The decision tree classifier from the previous exercise, clf_dt, is available in your workspace."]},{"cell_type":"code","metadata":{"id":"3Eb01FlM2XjJ","colab_type":"code","colab":{}},"source":["# Build and train the bagging classifier\n","clf_bag = BaggingClassifier(\n","  base_estimator=clf_dt,\n","  n_estimators=21,\n","  oob_score=True,\n","  random_state=500)\n","clf_bag.fit(X_train, y_train)\n","\n","# Print the out-of-bag score\n","print('OOB-Score: {:.3f}'.format(clf_bag.oob_score_))\n","\n","# Evaluate the performance on the test set to compare\n","pred = clf_bag.predict(X_test)\n","print('Accuracy: {:.3f}'.format(accuracy_score(y_test, pred)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ACGJyWjf2wHv","colab_type":"text"},"source":["### Bagging parameters: tips and tricks\n"]},{"cell_type":"markdown","metadata":{"id":"DZU5AFUV2yfp","colab_type":"text"},"source":["#### Exploring the UCI SECOM data\n","\n","It seems like this target is imbalanced, as more than 90% of the tests are negative. An individual model may be prone to overfitting, so it's a good idea to leverage an ensemble method here."]},{"cell_type":"markdown","metadata":{"id":"N2k31uPB25eS","colab_type":"text"},"source":["#### A more complex bagging model\n","\n","The preprocessed dataset is available in your workspace as uci_secom, and training and test sets have been created for you.\n","\n","As the target has a high class imbalance, use a \"balanced\" logistic regression as the base estimator here.\n","\n","- Instantiate a balanced logistic regression to use as the base classifier by specifying the parameter class_weight = 'balanced'.\n","- Build a bagging classifier using the logistic regression as the base estimator, including the out-of-bag score, and using the maximum number of features as 10.\n","- Print the out-of-bag score to compare to the accuracy."]},{"cell_type":"code","metadata":{"id":"GLKmeOgt1d5u","colab_type":"code","colab":{}},"source":["# Build a balanced logistic regression\n","clf_lr = LogisticRegression(class_weight='balanced')\n","\n","# Build and fit a bagging classifier\n","clf_bag = BaggingClassifier(base_estimator=clf_lr, max_features=10, oob_score=True, random_state=500)\n","clf_bag.fit(X_train, y_train)\n","\n","# Evaluate the accuracy on the test set and show the out-of-bag score\n","pred = clf_bag.predict(X_test)\n","print('Accuracy:  {:.2f}'.format(accuracy_score(y_test, pred)))\n","print('OOB-Score: {:.2f}'.format(clf_bag.oob_score_))\n","\n","# Print the confusion matrix\n","print(confusion_matrix(y_test, pred))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CUjo34023JF0","colab_type":"text"},"source":["#### Tuning bagging hyperparameters\n","\n","In this exercise, let's see if we can improve model performance by modifying the parameters of the bagging classifier.\n","\n","- Build a bagging classifier with 500 base estimators, 10 maximum features, and 0.65 (65%) maximum samples (max_samples). Sample without replacement.\n","- Use clf_bag to predict the labels of the test set, X_test."]},{"cell_type":"code","metadata":{"id":"7AjZ6L7m3SRM","colab_type":"code","colab":{}},"source":["# Build a balanced logistic regression\n","clf_base = LogisticRegression(class_weight='balanced', random_state=42)\n","\n","# Build and fit a bagging classifier with custom parameters\n","clf_bag = BaggingClassifier(base_estimator=clf_base, n_estimators=500, max_samples=0.65, max_features=10, bootstrap=False, random_state=500)\n","clf_bag.fit(X_train, y_train)\n","\n","# Calculate predictions and evaluate the accuracy on the test set\n","y_pred = clf_bag.predict(X_test)\n","print('Accuracy:  {:.2f}'.format(accuracy_score(y_test, y_pred)))\n","\n","# Print the classification report\n","print(classification_report(y_test, y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ILQNC4Vqr_3s","colab_type":"text"},"source":["## 3. Boosting\n","\n","Boosting is class of ensemble learning algorithms that includes award-winning models such as AdaBoost. In this chapter, you'll learn about this award-winning model, and use it to predict the revenue of award-winning movies! You'll also learn about gradient boosting algorithms such as CatBoost and XGBoost."]},{"cell_type":"markdown","metadata":{"id":"PgNmwiYR3bH8","colab_type":"text"},"source":["### The effectiveness of gradual learning\n"]},{"cell_type":"markdown","metadata":{"id":"wuWXXRMK3iKB","colab_type":"text"},"source":["#### Introducing the movie database\n","\n","The dataset is loaded and available to you as movies.\n","\n","Your main objective is to predict movie revenue - more specifically, log-revenue, which is the normalized version of the revenue feature.\n","\n","Use the .describe() method to explore this feature. "]},{"cell_type":"code","metadata":{"id":"5E1A-42z383R","colab_type":"code","colab":{}},"source":["movies.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_rprntQW322y","colab_type":"text"},"source":["#### Predicting movie revenue\n","Let's begin the challenge of predicting movie revenue by building a simple linear regression to predict the log-revenue of movies based on the 'budget' feature. The metric you will use here is the RMSE (root mean squared error). To calculate this using scikit-learn, you can use the mean_squared_error() function from the sklearn.metrics class and then take its square root using numpy.\n","\n","The movies dataset has been loaded for you and split into train and test sets."]},{"cell_type":"code","metadata":{"id":"knF_fiK53STb","colab_type":"code","colab":{}},"source":["# Build and fit linear regression model\n","reg_lm = LinearRegression(normalize=True)\n","reg_lm.fit(X_train, y_train)\n","\n","# Calculate the predictions on the test set\n","pred = reg_lm.predict(X_test)\n","\n","# Evaluate the performance using the RMSE\n","rmse = np.sqrt(mean_squared_error(y_test, pred))\n","print('RMSE: {:.3f}'.format(rmse))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jz3Kf_Nf4Kuc","colab_type":"text"},"source":["#### Boosting for predicted revenue\n","\n","You'll build another linear regression, but this time the target values are the errors from the base model, calculated as follows:\n","\n","`y_train_error = pred_train - y_train`\n","\n","`y_test_error = pred_test - y_test`"]},{"cell_type":"code","metadata":{"id":"rBZK9kMo3FZ6","colab_type":"code","colab":{}},"source":["# Fit a linear regression model to the previous errors\n","reg_error = LinearRegression(normalize=True)\n","reg_error.fit(X_train_pop, y_train_error)\n","\n","# Calculate the predicted errors on the test set\n","pred_error = reg_error.predict(X_test_pop)\n","\n","# Evaluate the updated performance\n","rmse_error = np.sqrt(mean_squared_error(y_test_error, pred_error))\n","print('RMSE: {:.3f}'.format(rmse_error))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uGc34Y2O4cz9","colab_type":"text"},"source":["### Adaptive boosting: award winning model\n"]},{"cell_type":"markdown","metadata":{"id":"MbicjbD74gKc","colab_type":"text"},"source":["#### Your first AdaBoost model\n","\n","In this exercise, you'll build your first AdaBoost model - an AdaBoostRegressor - in an attempt to improve performance even further."]},{"cell_type":"code","metadata":{"id":"9SnisbHN1d7s","colab_type":"code","colab":{}},"source":["# Instantiate a normalized linear regression model\n","reg_lm = LinearRegression(normalize=True)\n","\n","# Build and fit an AdaBoost regressor\n","reg_ada = AdaBoostRegressor(base_estimator=reg_lm, n_estimators=12, random_state=500)\n","reg_ada.fit(X_train, y_train)\n","\n","# Calculate the predictions on the test set\n","pred = reg_ada.predict(X_test)\n","\n","# Evaluate the performance using the RMSE\n","rmse = np.sqrt(mean_squared_error(y_test, pred))\n","print('RMSE: {:.3f}'.format(rmse))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TC1HkAtB4qeC","colab_type":"text"},"source":["#### Tree-based AdaBoost regression\n","\n","AdaBoost models are usually built with decision trees as the base estimators. \n","\n","We'll use twelve estimators as before to have a fair comparison. There's no need to instantiate the decision tree as it is the base estimator by default."]},{"cell_type":"code","metadata":{"id":"wGLSXuwn4n7D","colab_type":"code","colab":{}},"source":["# Build and fit a tree-based AdaBoost regressor\n","reg_ada = AdaBoostRegressor(n_estimators=12, random_state=500)\n","reg_ada.fit(X_train, y_train)\n","\n","# Calculate the predictions on the test set\n","pred = reg_ada.predict(X_test)\n","\n","# Evaluate the performance using the RMSE\n","rmse = np.sqrt(mean_squared_error(y_test, pred))\n","print('RMSE: {:.3f}'.format(rmse))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GNGuTk8d42Hu","colab_type":"text"},"source":["#### Making the most of AdaBoost\n","\n","As you have seen, for predicting movie revenue, AdaBoost gives the best results with decision trees as the base estimator.\n","\n","In this exercise, you'll specify some parameters to extract even more performance. In particular, you'll use a lower learning rate to have a smoother update of the hyperparameters. Therefore, the number of estimators should increase. "]},{"cell_type":"code","metadata":{"id":"Q33Cmytr4n9D","colab_type":"code","colab":{}},"source":["# Build and fit an AdaBoost regressor\n","reg_ada = AdaBoostRegressor(n_estimators=100, learning_rate=0.01, random_state=500)\n","reg_ada.fit(X_train, y_train)\n","\n","# Calculate the predictions on the test set\n","pred = reg_ada.predict(X_test)\n","\n","# Evaluate the performance using the RMSE\n","rmse = np.sqrt(mean_squared_error(y_test, pred))\n","print('RMSE: {:.3f}'.format(rmse))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2uTYR48A5Aze","colab_type":"text"},"source":["### Gradient boosting\n"]},{"cell_type":"markdown","metadata":{"id":"voEfyK5B8_VP","colab_type":"text"},"source":["#### Sentiment analysis with GBM\n","\n","Let's now use scikit-learn's GradientBoostingClassifier on the reviews dataset to predict the sentiment of a review given its text.\n","\n","We will not pass the raw text as input for the model. The following pre-processing has been done for you:\n","\n","1. Remove reviews with missing values.\n","2. Select data from the top 5 apps.\n","3. Select a random subsample of 500 reviews.\n","4. Remove \"stop words\" from the reviews.\n","5. Transform the reviews into a matrix, in which each feature represents the frequency of a word in a review."]},{"cell_type":"code","metadata":{"id":"6neoSTji9Bqd","colab_type":"code","colab":{}},"source":["# Build and fit a Gradient Boosting classifier\n","clf_gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=500)\n","clf_gbm.fit(X_train, y_train)\n","\n","# Calculate the predictions on the test set\n","pred = clf_gbm.predict(X_test)\n","\n","# Evaluate the performance based on the accuracy\n","acc = accuracy_score(y_test, pred)\n","print('Accuracy: {:.3f}'.format(acc))\n","\n","# Get and show the Confusion Matrix\n","cm = confusion_matrix(y_test, pred)\n","print(cm)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ZfAc0OV8_T4","colab_type":"text"},"source":["### Gradient boosting flavors\n"]},{"cell_type":"markdown","metadata":{"id":"31jk4wBA9aL1","colab_type":"text"},"source":["#### Movie revenue prediction with CatBoost\n","\n","Let's finish up this chapter on boosting by returning to the movies dataset! In this exercise, you'll build a CatBoostRegressor to predict the log-revenue."]},{"cell_type":"code","metadata":{"id":"zwz4Dynu9ZjJ","colab_type":"code","colab":{}},"source":["# Build and fit a CatBoost regressor\n","reg_cat = cb.CatBoostRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=500)\n","reg_cat.fit(X_train, y_train)\n","\n","# Calculate the predictions on the set set\n","pred = reg_cat.predict(X_test)\n","\n","# Evaluate the performance using the RMSE\n","rmse_cat = np.sqrt(mean_squared_error(y_test, pred))\n","print('RMSE (CatBoost): {:.3f}'.format(rmse_cat))\t"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sUWYdOr89m6Z","colab_type":"text"},"source":["#### Boosting contest: Light vs Extreme\n","\n","CatBoost is highly recommended when there are categorical features. In this case, all features are numeric, therefore one of the other approaches might perform better."]},{"cell_type":"code","metadata":{"id":"x0F2bTKr9Bsz","colab_type":"code","colab":{}},"source":["# Build and fit a XGBoost regressor\n","reg_xgb = xgb.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, random_state=500)\n","reg_xgb.fit(X_train, y_train)\n","\n","# Build and fit a LightGBM regressor\n","reg_lgb = lgb.LGBMRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, seed=500)\n","reg_lgb.fit(X_train, y_train)\n","\n","# Calculate the predictions and evaluate both regressors\n","pred_xgb = reg_xgb.predict(X_test)\n","rmse_xgb = np.sqrt(mean_squared_error(y_test, pred_xgb))\n","pred_lgb = reg_lgb.predict(X_test)\n","rmse_lgb = np.sqrt(mean_squared_error(y_test, pred_lgb))\n","\n","print('Extreme: {:.3f}, Light: {:.3f}'.format(rmse_xgb, rmse_lgb))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v-7-NHk5sDCt","colab_type":"text"},"source":["## 4. Stacking\n","\n","Get ready to see how things stack up! In this final chapter you'll learn about the stacking ensemble method. You'll learn how to implement it from scratch as well as using the `mlxtend` library! You'll apply stacking to predict the edibility of North American mushrooms, and revisit the ratings of Google apps with this more advanced approach."]},{"cell_type":"markdown","metadata":{"id":"qvDowdN19zDp","colab_type":"text"},"source":["### The intuition behind stacking\n"]},{"cell_type":"markdown","metadata":{"id":"CBbwEXuq9306","colab_type":"text"},"source":["#### Predicting mushroom edibility\n","\n","Now that you have explored the data, it's time to build a first model to predict mushroom edibility.\n","\n","The dataset is available to you as mushrooms. As both the features and the target are categorical, these have been transformed into \"dummy\" binary variables for you.\n","\n","Let's begin with Naive Bayes (using scikit-learn's GaussianNB) and see how this algorithm performs on this problem."]},{"cell_type":"code","metadata":{"id":"DulGFO9MsC9D","colab_type":"code","colab":{}},"source":["# Instantiate a Naive Bayes classifier\n","clf_nb = GaussianNB()\n","\n","# Fit the model to the training set\n","clf_nb.fit(X_train, y_train)\n","\n","# Calculate the predictions on the test set\n","pred = clf_nb.predict(X_test)\n","\n","# Evaluate the performance using the accuracy score\n","print(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, pred)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sh2dkewn-CSZ","colab_type":"text"},"source":["#### K-nearest neighbors for mushrooms\n","\n","The Gaussian Naive Bayes classifier did a really good job for being an initial model. Let's now build a new model to compare it against the Naive Bayes.\n","\n","In this case, the algorithm to use is a 5-nearest neighbors classifier. As the dummy features create a high-dimensional dataset, use the Ball Tree algorithm to make the model faster. "]},{"cell_type":"code","metadata":{"id":"8V8uesr6-FeX","colab_type":"code","colab":{}},"source":["# Instantiate a 5-nearest neighbors classifier\n","clf_knn = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n","\n","# Fit the model to the training set\n","clf_knn.fit(X_train, y_train)\n","\n","# Calculate the predictions on the test set\n","pred = clf_knn.predict(X_test)\n","\n","# Evaluate the performance using the accuracy score\n","print(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, pred)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iInQdsSd-I8c","colab_type":"text"},"source":["### Build your first stacked ensemble\n"]},{"cell_type":"markdown","metadata":{"id":"UlKxCuz6-I9_","colab_type":"text"},"source":["#### Applying stacking to predict app ratings\n","\n","In this exercise you'll start building your first Stacking ensemble from scratch. "]},{"cell_type":"code","metadata":{"id":"9zgwB8BAsHTF","colab_type":"code","colab":{}},"source":["# Build and fit a Decision Tree classifier\n","clf_dt = DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9, random_state=500)\n","clf_dt.fit(X_train, y_train)\n","\n","# Build and fit a 5-nearest neighbors classifier using the 'Ball-Tree' algorithm\n","clf_knn = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n","clf_knn.fit(X_train, y_train)\n","\n","# Evaluate the performance using the accuracy score\n","print('Decision Tree: {:0.4f}'.format(accuracy_score(y_test, clf_dt.predict(X_test))))\n","print('5-Nearest Neighbors: {:0.4f}'.format(accuracy_score(y_test, clf_knn.predict(X_test))))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x0ejGGEO-YL9","colab_type":"text"},"source":["#### Building the second-layer classifier\n","\n","Now you'll work on the next two steps of the process.\n","\n","**Step 3**: Append the predictions to the dataset The predictions with the first-layer estimators were already calculated and are available to you as: pred_dt and pred_knn. You'll use these to create a DataFrame and append them to the training features.\n","\n","**Step 4**: Build the second-layer meta estimator For this purpose you'll build the default DecisionTreeClassifier. This will take as input feature the concatenation of the original training features and the predictions from the first-layer estimators."]},{"cell_type":"code","metadata":{"id":"BMuRQ2STBBOv","colab_type":"code","colab":{}},"source":["# Create a Pandas DataFrame with the predictions\n","pred_df = pd.DataFrame({\n","\t'pred_dt': pred_dt,\n","    'pred_knn': pred_knn\n","}, index=X_train.index)\n","\n","# Concatenate X_train with the predictions DataFrame\n","X_train_2nd = pd.concat([X_train, pred_df], axis=1)\n","\n","# Build the second-layer meta estimator\n","clf_stack = DecisionTreeClassifier(random_state=500)\n","clf_stack.fit(X_train_2nd, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vv5UZpqkBDgh","colab_type":"text"},"source":["#### Stacked predictions for app ratings\n","\n","Once the second-layer estimator is built you are ready for step 5: use the stacked ensemble for predictions."]},{"cell_type":"code","metadata":{"id":"nTFJ2r5pBGYw","colab_type":"code","colab":{}},"source":["# Create a Pandas DataFrame with the predictions\n","pred_df = pd.DataFrame({\n","\t'pred_dt': pred_dt,\n","    'pred_knn': pred_knn\n","}, index=X_test.index)\n","\n","# Concatenate X_test with the predictions DataFrame\n","X_test_2nd = pd.concat([X_test, pred_df], axis=1)\n","\n","# Obtain the final predictions from the second-layer estimator\n","pred_stack = clf_stack.predict(X_test_2nd)\n","\n","# Evaluate the new performance on the test set\n","print('Accuracy: {:0.4f}'.format(accuracy_score(y_test, pred_stack)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"az9l11ETBJPI","colab_type":"text"},"source":["#### A first attempt with mlxtend\n","\n","You'll continue using the app ratings dataset. As you have already built a stacked ensemble model from scratch, you have a basis to compare with the model you'll now build with mlxtend."]},{"cell_type":"code","metadata":{"id":"zlYHziRJBNNk","colab_type":"code","colab":{}},"source":["# Instantiate the first-layer classifiers\n","clf_dt = DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9, random_state=500)\n","clf_knn = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n","\n","# Instantiate the second-layer meta classifier\n","clf_meta = DecisionTreeClassifier(random_state=500)\n","\n","# Build the Stacking classifier\n","clf_stack = StackingClassifier(classifiers=[clf_dt, clf_knn], meta_classifier=clf_meta, use_features_in_secondary=True)\n","clf_stack.fit(X_train, y_train)\n","\n","# Evaluate the performance of the Stacking classifier\n","pred_stack = clf_stack.predict(X_test)\n","print(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, pred_stack)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7gI09bwLBSLj","colab_type":"text"},"source":["#### Back to regression with stacking\n","\n","In Chapter 1, we treated the app ratings as a regression problem, predicting the rating on the interval from 1 to 5. "]},{"cell_type":"code","metadata":{"id":"9N4tvSfVBZN9","colab_type":"code","colab":{}},"source":["# Instantiate the 1st-layer regressors\n","reg_dt = DecisionTreeRegressor(min_samples_leaf=11, min_samples_split=33, random_state=500)\n","reg_lr = LinearRegression(normalize=True)\n","reg_ridge = Ridge(random_state=500)\n","\n","# Instantiate the 2nd-layer regressor\n","reg_meta = LinearRegression()\n","\n","# Build the Stacking regressor\n","reg_stack = StackingRegressor(regressors=[reg_dt, reg_lr, reg_ridge], meta_regressor=reg_meta)\n","reg_stack.fit(X_train, y_train)\n","\n","# Evaluate the performance on the test set using the MAE metric\n","pred = reg_stack.predict(X_test)\n","print('MAE: {:.3f}'.format(mean_absolute_error(y_test, pred)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aLv5WglaBeG0","colab_type":"text"},"source":["#### Mushrooms: a matter of life or death\n","\n","You'll try the stacking classifier to see if the score can be improved. \n","\n","As stacking uses a meta-estimator (second layer classifier) which attempts to correct predictions from the first layer, some of the misclassified instances could be corrected. This is a very important problem, as the edibility of a mushroom is a matter of life or death."]},{"cell_type":"code","metadata":{"id":"6oIakUJ_BZNE","colab_type":"code","colab":{}},"source":["# Create the first-layer models\n","clf_knn = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n","clf_dt = DecisionTreeClassifier(min_samples_leaf=5, min_samples_split=15, random_state=500)\n","clf_nb = GaussianNB()\n","\n","# Create the second-layer model (meta-model)\n","clf_lr = LogisticRegression()\n","\n","# Create and fit the stacked model\n","clf_stack = StackingClassifier(classifiers=[clf_knn, clf_dt, clf_nb], meta_classifier=clf_lr)\n","clf_stack.fit(X_train, y_train)\n","\n","# Evaluate the stacked model’s performance\n","print(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, clf_stack.predict(X_test))))"],"execution_count":0,"outputs":[]}]}