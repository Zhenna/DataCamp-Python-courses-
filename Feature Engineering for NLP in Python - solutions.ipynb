{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Feature Engineering for NLP in Python - solutions.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"_7zbAwdXsPeT","colab_type":"code","colab":{}},"source":["# https://www.datacamp.com/courses/feature-engineering-for-nlp-in-python"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZuDt0uDLGCN1","colab_type":"code","colab":{}},"source":["# try to convert input dataframe to series"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BvD329mkAPlq","colab_type":"code","colab":{}},"source":["# replace empty string with nan\n","lem_corpus.replace(r'\\s+( +\\.)|#',np.nan,regex=True).replace('',np.nan)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nkEQZC7Pefwu","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UC01Px8klEDo","colab_type":"code","outputId":"e018d807-8170-4d17-91fa-13e746f4f564","executionInfo":{"status":"ok","timestamp":1569202178590,"user_tz":-480,"elapsed":12141,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":326}},"source":["! pip3 install textatistic"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting textatistic\n","  Downloading https://files.pythonhosted.org/packages/73/f8/74dade1df8998ce9a42cba21b3cdf284f3f273e7e22fbcab27955d213e61/textatistic-0.0.1.tar.gz\n","Collecting pyhyphen>=2.0.5 (from textatistic)\n","  Downloading https://files.pythonhosted.org/packages/aa/e7/97ff5a575bd0744c67b28316fccbbcf180806bb89424df94fca9c1f6ea8c/PyHyphen-3.0.1.tar.gz\n","Collecting appdirs (from pyhyphen>=2.0.5->textatistic)\n","  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pyhyphen>=2.0.5->textatistic) (1.12.0)\n","Building wheels for collected packages: textatistic, pyhyphen\n","  Building wheel for textatistic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for textatistic: filename=textatistic-0.0.1-cp36-none-any.whl size=29057 sha256=c75ea774a89f6eec39d7814f06c6d6a211bae4919fbe6f54614dd78d7ed60ee6\n","  Stored in directory: /root/.cache/pip/wheels/1d/ec/34/69c3cae349149cd91552c4c470efcbd08bbd21ba30b12e08ab\n","  Building wheel for pyhyphen (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyhyphen: filename=PyHyphen-3.0.1-cp36-cp36m-linux_x86_64.whl size=57896 sha256=8ed6d24bb6a6c010aea1eafe63400df5b37f028b614c81d510e0b703e42ad031\n","  Stored in directory: /root/.cache/pip/wheels/85/46/93/46c556b5f054568b7470c86c4f76ea628a9a8bdf5a355b9c63\n","Successfully built textatistic pyhyphen\n","Installing collected packages: appdirs, pyhyphen, textatistic\n","Successfully installed appdirs-1.4.3 pyhyphen-3.0.1 textatistic-0.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Dzhc2VjLefsn","colab_type":"code","colab":{}},"source":["from textatistic import Textatistic\n","import spacy\n","import time\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics.pairwise import cosine_similarity, linear_kernel"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uPCcm9PAsTRT","colab_type":"text"},"source":["**Course Description**\n","\n","In this course, you will learn techniques that will allow you to extract useful information from text and process them into a format suitable for applying ML models. More specifically, you will learn about POS tagging, named entity recognition, readability scores, the n-gram and tf-idf models, and how to implement them using scikit-learn and spaCy. You will also learn to compute how similar two documents are to each other. In the process, you will predict the sentiment of movie reviews and build movie and Ted Talk recommenders. Following the course, you will be able to engineer critical features out of any text and solve some of the most challenging problems in data science!"]},{"cell_type":"markdown","metadata":{"id":"bpc6Z6V9sWAL","colab_type":"text"},"source":["## 1. Basic features and readability scores\n","\n","Learn to compute basic features such as number of words, number of characters, average word length and number of special characters (such as Twitter hashtags and mentions). You will also learn to compute readability scores and determine the amount of education required to comprehend a piece of text."]},{"cell_type":"markdown","metadata":{"id":"InD42gwmd65D","colab_type":"text"},"source":["#### Data format for ML algorithms\n","\n","This dataframe has numerical training features and the predictor variable is a class. Therefore, it is in a suitable format for applying a classification algorithm."]},{"cell_type":"markdown","metadata":{"id":"3ok82rvwd_Qp","colab_type":"text"},"source":["#### One-hot encoding\n","\n","In this exercise, your task is to convert df1 into a format that is suitable for machine learning."]},{"cell_type":"code","metadata":{"id":"142X1ieqsZKj","colab_type":"code","cellView":"form","colab":{}},"source":["#@title\n","d = '''    feature_1  feature_2  feature_3  feature_4 feature_5  label\n","0     29.0000          0          0   211.3375    female      1\n","1      0.9167          1          2   151.5500      male      1\n","2      2.0000          1          2   151.5500    female      0\n","3     30.0000          1          2   151.5500      male      0\n","4     25.0000          1          2   151.5500    female      0\n","5     48.0000          0          0    26.5500      male      1\n","6     63.0000          1          0    77.9583    female      1\n","7     39.0000          0          0     0.0000      male      0\n","8     53.0000          2          0    51.4792    female      1\n","9     71.0000          0          0    49.5042      male      0\n","10    47.0000          1          0   227.5250      male      0\n","11    18.0000          1          0   227.5250    female      1\n","12    24.0000          0          0    69.3000    female      1\n","13    26.0000          0          0    78.8500    female      1\n","14    80.0000          0          0    30.0000      male      1'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xQlXYdyqedWD","colab_type":"code","cellView":"both","outputId":"ab1eff3a-c84a-46b7-9341-6a916869c7dc","executionInfo":{"status":"ok","timestamp":1569147812648,"user_tz":-480,"elapsed":734,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":498}},"source":["df1 = pd.read_csv(pd.compat.StringIO(d), sep='\\s+')\n","df1"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>feature_1</th>\n","      <th>feature_2</th>\n","      <th>feature_3</th>\n","      <th>feature_4</th>\n","      <th>feature_5</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>29.0000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>211.3375</td>\n","      <td>female</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.9167</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>151.5500</td>\n","      <td>male</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0000</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>151.5500</td>\n","      <td>female</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>30.0000</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>151.5500</td>\n","      <td>male</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>25.0000</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>151.5500</td>\n","      <td>female</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>48.0000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>26.5500</td>\n","      <td>male</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>63.0000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>77.9583</td>\n","      <td>female</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>39.0000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0000</td>\n","      <td>male</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>53.0000</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>51.4792</td>\n","      <td>female</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>71.0000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>49.5042</td>\n","      <td>male</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>47.0000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>227.5250</td>\n","      <td>male</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>18.0000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>227.5250</td>\n","      <td>female</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>24.0000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>69.3000</td>\n","      <td>female</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>26.0000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>78.8500</td>\n","      <td>female</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>80.0000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>30.0000</td>\n","      <td>male</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    feature_1  feature_2  feature_3  feature_4 feature_5  label\n","0     29.0000          0          0   211.3375    female      1\n","1      0.9167          1          2   151.5500      male      1\n","2      2.0000          1          2   151.5500    female      0\n","3     30.0000          1          2   151.5500      male      0\n","4     25.0000          1          2   151.5500    female      0\n","5     48.0000          0          0    26.5500      male      1\n","6     63.0000          1          0    77.9583    female      1\n","7     39.0000          0          0     0.0000      male      0\n","8     53.0000          2          0    51.4792    female      1\n","9     71.0000          0          0    49.5042      male      0\n","10    47.0000          1          0   227.5250      male      0\n","11    18.0000          1          0   227.5250    female      1\n","12    24.0000          0          0    69.3000    female      1\n","13    26.0000          0          0    78.8500    female      1\n","14    80.0000          0          0    30.0000      male      1"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"FWIQtvu7gES6","colab_type":"text"},"source":["- Use the columns attribute to print the features of df1."]},{"cell_type":"code","metadata":{"id":"fUq6TehMgJ3L","colab_type":"code","outputId":"352675ac-5f14-4eb6-8321-8cd0795fb59f","executionInfo":{"status":"ok","timestamp":1569147888940,"user_tz":-480,"elapsed":713,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["# Print the features of df1\n","print(df1.columns)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Index(['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5',\n","       'label'],\n","      dtype='object')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DwKQbXAjgNay","colab_type":"text"},"source":["- Use the pd.get_dummies() function to perform one-hot encoding on feature 5 of df1."]},{"cell_type":"code","metadata":{"id":"tpYpaaqDgPip","colab_type":"code","outputId":"bfc1d3f8-dadb-4616-c2ce-0b6dff41d824","executionInfo":{"status":"ok","timestamp":1569147924439,"user_tz":-480,"elapsed":734,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["# Print the features of df1\n","print(df1.columns)\n","\n","# Perform one-hot encoding\n","df1 = pd.get_dummies(df1, columns=['feature_5'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Index(['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5',\n","       'label'],\n","      dtype='object')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c6He0NS8gajl","colab_type":"text"},"source":["Use the columns attribute again to print the new features of df1.\n","Print the first five rows of df1 using head()."]},{"cell_type":"code","metadata":{"id":"PL5IuazrgcSW","colab_type":"code","outputId":"f5d7d49b-439f-4db6-ff94-5c07e8b00461","executionInfo":{"status":"ok","timestamp":1569147980634,"user_tz":-480,"elapsed":721,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["# Print the new features of df1\n","print(df1.columns)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Index(['feature_1', 'feature_2', 'feature_3', 'feature_4', 'label',\n","       'feature_5_female', 'feature_5_male'],\n","      dtype='object')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pwz66jpvghVn","colab_type":"code","outputId":"a7dff269-5220-49f4-fd40-22e3c0ba02f8","executionInfo":{"status":"ok","timestamp":1569147985265,"user_tz":-480,"elapsed":739,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":175}},"source":["# Print first five rows of df1\n","print(df1.head())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   feature_1  feature_2  feature_3  ...  label  feature_5_female  feature_5_male\n","0    29.0000          0          0  ...      1                 1               0\n","1     0.9167          1          2  ...      1                 0               1\n","2     2.0000          1          2  ...      0                 1               0\n","3    30.0000          1          2  ...      0                 0               1\n","4    25.0000          1          2  ...      0                 1               0\n","\n","[5 rows x 7 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OvOfi5XOglRB","colab_type":"text"},"source":["### Basic feature extraction\n"]},{"cell_type":"markdown","metadata":{"id":"qgPYAcplgnzj","colab_type":"text"},"source":["#### Character count of Russian tweets\n","\n","Your task is to create a new feature 'char_count' in tweets which computes the number of characters for each tweet. Also, compute the average length of each tweet."]},{"cell_type":"code","metadata":{"id":"rit2WcMFiN45","colab_type":"code","outputId":"8898facd-3f92-48e2-c5ea-c34cce3ece5f","executionInfo":{"status":"ok","timestamp":1569148489257,"user_tz":-480,"elapsed":733,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"source":["tweets = pd.read_csv(\"russian_tweets.csv\", index_col=0)\n","print(\"dataframe dimension: \", tweets.shape)\n","tweets.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["dataframe dimension:  (1000, 1)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>127447</th>\n","      <td>LIVE STREAM VIDEO=&gt; Donald Trump Rallies in Co...</td>\n","    </tr>\n","    <tr>\n","      <th>123642</th>\n","      <td>Muslim Attacks NYPD Cops with Meat Cleaver. Me...</td>\n","    </tr>\n","    <tr>\n","      <th>226970</th>\n","      <td>.@vfpatlas well that's a swella word there (di...</td>\n","    </tr>\n","    <tr>\n","      <th>138339</th>\n","      <td>RT wehking_pamela: Bobby_Axelrod2k MMFlint don...</td>\n","    </tr>\n","    <tr>\n","      <th>161610</th>\n","      <td>Жители обстреливаемых районов Донецка проводят...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                  content\n","127447  LIVE STREAM VIDEO=> Donald Trump Rallies in Co...\n","123642  Muslim Attacks NYPD Cops with Meat Cleaver. Me...\n","226970  .@vfpatlas well that's a swella word there (di...\n","138339  RT wehking_pamela: Bobby_Axelrod2k MMFlint don...\n","161610  Жители обстреливаемых районов Донецка проводят..."]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"y79A89l-iPH5","colab_type":"code","outputId":"9fd43526-d743-49ff-e14c-d9451d981fda","executionInfo":{"status":"ok","timestamp":1569148633154,"user_tz":-480,"elapsed":725,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":198}},"source":["# match dataset\n","tweets.reset_index(inplace=True)\n","tweets.tail()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>995</th>\n","      <td>24329</td>\n","      <td>How To Inspire People With Your Music! https:/...</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>315060</td>\n","      <td>... https://t.co/AfWdTkKQlm</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>361901</td>\n","      <td>Trevor Noah: Until we start treating racism li...</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>119948</td>\n","      <td>SenSanders: RT SenJeffMerkley: We must act bol...</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>242678</td>\n","      <td>Police: Man arrested for February shooting at ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      index                                            content\n","995   24329  How To Inspire People With Your Music! https:/...\n","996  315060                        ... https://t.co/AfWdTkKQlm\n","997  361901  Trevor Noah: Until we start treating racism li...\n","998  119948  SenSanders: RT SenJeffMerkley: We must act bol...\n","999  242678  Police: Man arrested for February shooting at ..."]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"kNv4rfiwjDE_","colab_type":"text"},"source":["In this exercise, you have been given a dataframe tweets which contains some tweets associated with Russia's Internet Research Agency and compiled by FiveThirtyEight.\n","\n","- Create a new feature char_count by applying len to the 'content' feature of tweets.\n","- Print the average character count of the tweets by computing the mean of the 'char_count' feature."]},{"cell_type":"code","metadata":{"id":"_d4Vg-e0jHvo","colab_type":"code","outputId":"6afebd5e-a05b-4dda-bd9e-93c8d5c0e4d6","executionInfo":{"status":"ok","timestamp":1569148668925,"user_tz":-480,"elapsed":732,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Create a feature char_count\n","tweets['char_count'] = tweets['content'].apply(len)\n","\n","# Print the average character count\n","print(tweets['char_count'].mean())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["103.462\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kvp2-z5fjNTx","colab_type":"text"},"source":["**Observation:**\n","\n","Notice that the average character count of these tweets is approximately 104, which is much higher than the overall average tweet length of around 40 characters."]},{"cell_type":"markdown","metadata":{"id":"YN_FLntgjQHK","colab_type":"text"},"source":["#### Word count of TED talks\n","\n","ted is a dataframe that contains the transcripts of 500 TED talks. Your job is to compute a new feature word_count which contains the approximate number of words for each talk. Consequently, you also need to compute the average word count of the talks. The transcripts are available as the transcript feature in ted."]},{"cell_type":"code","metadata":{"id":"OxEy_SX8jYOT","colab_type":"code","outputId":"94d3bac4-0cf7-49ca-8653-ef408df8556c","executionInfo":{"status":"ok","timestamp":1569152083616,"user_tz":-480,"elapsed":715,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"source":["ted = pd.read_csv(\"ted.csv\")\n","del ted['url']\n","print(\"dataframe dimension: \", ted.shape)\n","ted.tail()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["dataframe dimension:  (500, 1)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>transcript</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>495</th>\n","      <td>Today I'm going to unpack for you three exampl...</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>Both myself and my brother belong to the under...</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>John Hockenberry: It's great to be here with y...</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>What you're doing, right now, at this very mom...</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>We've got a real problem with math education r...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            transcript\n","495  Today I'm going to unpack for you three exampl...\n","496  Both myself and my brother belong to the under...\n","497  John Hockenberry: It's great to be here with y...\n","498  What you're doing, right now, at this very mom...\n","499  We've got a real problem with math education r..."]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"qwRZ1lwujWhy","colab_type":"text"},"source":["In order to complete this task, you will need to define a function `count_words` that takes in a string as an argument and returns the number of words in the string. You will then need to apply this function to the transcript feature of ted to create the new feature `word_count` and compute its mean."]},{"cell_type":"code","metadata":{"id":"GE1XKC20iPF6","colab_type":"code","outputId":"13457828-2465-4da0-90bd-82ad6abb0efa","executionInfo":{"status":"ok","timestamp":1569148832741,"user_tz":-480,"elapsed":719,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Function that returns number of words in a string\n","def count_words(string):\n","\t# Split the string into words\n","    words = string.split()\n","    \n","    # Return the number of words\n","    return len(words)\n","\n","# Create a new feature word_count\n","ted['word_count'] = ted['transcript'].apply(count_words)\n","\n","# Print the average word count of the talks\n","print(ted['word_count'].mean())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1987.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f6j-DjYqjzEd","colab_type":"text"},"source":["#### Hashtags and mentions in Russian tweets\n","\n","In this exercise, you will compute the number of hashtags and mentions in each tweet by defining two functions count_hashtags() and count_mentions() respectively and applying them to the content feature of tweets."]},{"cell_type":"markdown","metadata":{"id":"EO-9s93LkIav","colab_type":"text"},"source":["- In the list comprehension, use startswith() to check if a particular word starts with '#'.\n","\n"]},{"cell_type":"code","metadata":{"id":"9GVM1AHBj5hA","colab_type":"code","colab":{}},"source":["# Function that returns number of hashtags in a string\n","def count_hashtags(string):\n","\t# Split the string into words\n","    words = string.split()\n","    \n","    # Create a list of words that are hashtags\n","    hashtags = [word for word in words if word.startswith('#')]\n","    \n","    # Return number of hashtags\n","    return(len(hashtags))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zYv1_GbAj5iS","colab_type":"code","outputId":"b77fddee-6af4-4317-c0a5-457509990292","executionInfo":{"status":"ok","timestamp":1569148896088,"user_tz":-480,"elapsed":737,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":281}},"source":["# Create a feature hashtag_count and display distribution\n","tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n","tweets['hashtag_count'].hist()\n","plt.title('Hashtag count distribution')\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"m7Ym-gd_kDcQ","colab_type":"text"},"source":["- In the list comprehension, use startswith() to check if a particular word starts with '@'."]},{"cell_type":"code","metadata":{"id":"97Bjd9KPj5ko","colab_type":"code","colab":{}},"source":["# Function that returns number of mentions in a string\n","def count_mentions(string):\n","\t# Split the string into words\n","    words = string.split()\n","    \n","    # Create a list of words that are mentions\n","    mentions = [word for word in words if word.startswith('@')]\n","    \n","    # Return number of mentions\n","    return(len(mentions))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tDzDNKyTkQFJ","colab_type":"code","outputId":"0f4aa14b-4634-4779-c8f2-21096be368b9","executionInfo":{"status":"ok","timestamp":1569148962982,"user_tz":-480,"elapsed":740,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":281}},"source":["# Create a feature mention_count and display distribution\n","tweets['mention_count'] = tweets['content'].apply(count_mentions)\n","tweets['mention_count'].hist()\n","plt.title('Mention count distribution')\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA7NENtvtRKs6AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"c_MfM_OnkSVp","colab_type":"text"},"source":["### Readability tests\n"]},{"cell_type":"markdown","metadata":{"id":"fa3TMx45kUOa","colab_type":"text"},"source":["#### Readability of 'The Myth of Sisyphus'\n","\n","In this exercise, you will compute the Flesch reading ease score for Albert Camus' famous essay The Myth of Sisyphus. "]},{"cell_type":"code","metadata":{"id":"_OfAR37iklWk","colab_type":"code","outputId":"0b4378b4-4012-41c0-b7f1-94138f134dcb","executionInfo":{"status":"ok","timestamp":1569149076493,"user_tz":-480,"elapsed":776,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["sisyphus_essay = '\\nThe gods had condemned Sisyphus to ceaselessly rolling a rock to the top of a mountain, whence the stone would fall back of its own weight. They had thought with some reason that there is no more dreadful punishment than futile and hopeless labor. If one believes Homer, Sisyphus was the wisest and most prudent of mortals. According to another tradition, however, he was disposed to practice the profession of highwayman. I see no contradiction in this. Opinions differ as to the reasons why he became the futile laborer of the underworld. To begin with, he is accused of a certain levity in regard to the gods. He stole their secrets. Egina, the daughter of Esopus, was carried off by Jupiter. The father was shocked by that disappearance and complained to Sisyphus. He, who knew of the abduction, offered to tell about it on condition that Esopus would give water to the citadel of Corinth. To the celestial thunderbolts he preferred the benediction of water. He was punished for this in the underworld. Homer tells us also that Sisyphus had put Death in chains. Pluto could not endure the sight of his deserted, silent empire. He dispatched the god of war, who liberated Death from the hands of her conqueror. It is said that Sisyphus, being near to death, rashly wanted to test his wife\\'s love. He ordered her to cast his unburied body into the middle of the public square. Sisyphus woke up in the underworld. And there, annoyed by an obedience so contrary to human love, he obtained from Pluto permission to return to earth in order to chastise his wife. But when he had seen again the face of this world, enjoyed water and sun, warm stones and the sea, he no longer wanted to go back to the infernal darkness. Recalls, signs of anger, warnings were of no avail. Many years more he lived facing the curve of the gulf, the sparkling sea, and the smiles of earth. A decree of the gods was necessary. Mercury came and seized the impudent man by the collar and, snatching him from his joys, lead him forcibly back to the underworld, where his rock was ready for him. You have already grasped that Sisyphus is the absurd hero. He is, as much through his passions as through his torture. His scorn of the gods, his hatred of death, and his passion for life won him that unspeakable penalty in which the whole being is exerted toward accomplishing nothing. This is the price that must be paid for the passions of this earth. Nothing is told us about Sisyphus in the underworld. Myths are made for the imagination to breathe life into them. As for this myth, one sees merely the whole effort of a body straining to raise the huge stone, to roll it, and push it up a slope a hundred times over; one sees the face screwed up, the cheek tight against the stone, the shoulder bracing the clay-covered mass, the foot wedging it, the fresh start with arms outstretched, the wholly human security of two earth-clotted hands. At the very end of his long effort measured by skyless space and time without depth, the purpose is achieved. Then Sisyphus watches the stone rush down in a few moments toward tlower world whence he will have to push it up again toward the summit. He goes back down to the plain. It is during that return, that pause, that Sisyphus interests me. A face that toils so close to stones is already stone itself! I see that man going back down with a heavy yet measured step toward the torment of which he will never know the end. That hour like a breathing-space which returns as surely as his suffering, that is the hour of consciousness. At each of those moments when he leaves the heights and gradually sinks toward the lairs of the gods, he is superior to his fate. He is stronger than his rock. If this myth is tragic, that is because its hero is conscious. Where would his torture be, indeed, if at every step the hope of succeeding upheld him? The workman of today works everyday in his life at the same tasks, and his fate is no less absurd. But it is tragic only at the rare moments when it becomes conscious. Sisyphus, proletarian of the gods, powerless and rebellious, knows the whole extent of his wretched condition: it is what he thinks of during his descent. The lucidity that was to constitute his torture at the same time crowns his victory. There is no fate that can not be surmounted by scorn. If the descent is thus sometimes performed in sorrow, it can also take place in joy. This word is not too much. Again I fancy Sisyphus returning toward his rock, and the sorrow was in the beginning. When the images of earth cling too tightly to memory, when the call of happiness becomes too insistent, it happens that melancholy arises in man\\'s heart: this is the rock\\'s victory, this is the rock itself. The boundless grief is too heavy to bear. These are our nights of Gethsemane. But crushing truths perish from being acknowledged. Thus, Edipus at the outset obeys fate without knowing it. But from the moment he knows, his tragedy begins. Yet at the same moment, blind and desperate, he realizes that the only bond linking him to the world is the cool hand of a girl. Then a tremendous remark rings out: \"Despite so many ordeals, my advanced age and the nobility of my soul make me conclude that all is well.\" Sophocles\\' Edipus, like Dostoevsky\\'s Kirilov, thus gives the recipe for the absurd victory. Ancient wisdom confirms modern heroism. One does not discover the absurd without being tempted to write a manual of happiness. \"What!---by such narrow ways--?\" There is but one world, however. Happiness and the absurd are two sons of the same earth. They are inseparable. It would be a mistake to say that happiness necessarily springs from the absurd. Discovery. It happens as well that the felling of the absurd springs from happiness. \"I conclude that all is well,\" says Edipus, and that remark is sacred. It echoes in the wild and limited universe of man. It teaches that all is not, has not been, exhausted. It drives out of this world a god who had come into it with dissatisfaction and a preference for futile suffering. It makes of fate a human matter, which must be settled among men. All Sisyphus\\' silent joy is contained therein. His fate belongs to him. His rock is a thing. Likewise, the absurd man, when he contemplates his torment, silences all the idols. In the universe suddenly restored to its silence, the myriad wondering little voices of the earth rise up. Unconscious, secret calls, invitations from all the faces, they are the necessary reverse and price of victory. There is no sun without shadow, and it is essential to know the night. The absurd man says yes and his efforts will henceforth be unceasing. If there is a personal fate, there is no higher destiny, or at least there is, but one which he concludes is inevitable and despicable. For the rest, he knows himself to be the master of his days. At that subtle moment when man glances backward over his life, Sisyphus returning toward his rock, in that slight pivoting he contemplates that series of unrelated actions which become his fate, created by him, combined under his memory\\'s eye and soon sealed by his death. Thus, convinced of the wholly human origin of all that is human, a blind man eager to see who knows that the night has no end, he is still on the go. The rock is still rolling. I leave Sisyphus at the foot of the mountain! One always finds one\\'s burden again. But Sisyphus teaches the higher fidelity that negates the gods and raises rocks. He too concludes that all is well. This universe henceforth without a master seems to him neither sterile nor futile. Each atom of that stone, each mineral flake of that night filled mountain, in itself forms a world. The struggle itself toward the heights is enough to fill a man\\'s heart. One must imagine Sisyphus happy.\\n'\n","\n","print(\"sisyphus_essay loaded...\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["sisyphus_essay loaded...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ei-sZtjdkltD","colab_type":"text"},"source":["The entire essay is in the form of a string and is available as sisyphus_essay.\n","\n","- Import the Textatistic class from textatistic.\n","- Compute the `readability_scores` dictionary for `sisyphus_essay` using Textatistic.\n","- Print the Flesch reading ease score from the `readability_scores` dictionary."]},{"cell_type":"code","metadata":{"id":"WHmyuTHOkuE_","colab_type":"code","colab":{}},"source":["# Import Textatistic\n","from textatistic import Textatistic\n","\n","# Compute the readability scores \n","readability_scores = Textatistic(sisyphus_essay).scores"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o-nxQ2C9lONW","colab_type":"code","outputId":"001a68f6-18aa-4710-b5af-b086ae2824d2","executionInfo":{"status":"ok","timestamp":1569149220164,"user_tz":-480,"elapsed":764,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["readability_scores"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'dalechall_score': 7.487433762517882,\n"," 'flesch_score': 81.67466335836913,\n"," 'fleschkincaid_score': 5.485083154506439,\n"," 'gunningfog_score': 7.913698140200286,\n"," 'smog_score': 8.110721755262034}"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"HAacoD9mkuH3","colab_type":"code","outputId":"0a4e840d-65d1-49a3-9128-9400268baede","executionInfo":{"status":"ok","timestamp":1569149223356,"user_tz":-480,"elapsed":740,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Print the flesch reading ease score\n","flesch = readability_scores['flesch_score']\n","print(\"The Flesch Reading Ease is %.2f\" % (flesch))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The Flesch Reading Ease is 81.67\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eQdoyZxYp_V5","colab_type":"text"},"source":["#### Readability of various publications\n","\n","In this exercise, you have been given excerpts of articles from four publications. Your task is to compute the readability of these excerpts using the Gunning fog index and consequently, determine the relative difficulty of reading these publications."]},{"cell_type":"code","metadata":{"id":"oafC_qPXqVcF","colab_type":"code","outputId":"7a1ab6c1-fcaf-44f4-81b8-5a166ed4e2ad","executionInfo":{"status":"ok","timestamp":1569150588506,"user_tz":-480,"elapsed":1643,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["forbes = '\\nThe idea is to create more transparency about companies and individuals that are breaking the law or are non-compliant with official obligations and incentivize the right behaviors with the overall goal of improving governance and market order. The Chinese Communist Party intends the social credit score system to “allow the trustworthy to roam freely under heaven while making it hard for the discredited to take a single step.” Even though the system is still under development it currently plays out in real life in myriad ways for private citizens, businesses and government officials. Generally, higher credit scores give people a variety of advantages. Individuals are often given perks such as discounted energy bills and access or better visibility on dating websites. Often, those with higher social credit scores are able to forgo deposits on rental properties, bicycles, and umbrellas. They can even get better travel deals. In addition, Chinese hospitals are currently experimenting with social credit scores. A social credit score above 650 at one hospital allows an individual to see a doctor without lining up to pay.\\n'\n","print(\"forbes loaded\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["forbes loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EYOkHmOBqd1Z","colab_type":"code","outputId":"b55749e9-c919-4476-dcbf-494b0d50ca72","executionInfo":{"status":"ok","timestamp":1569150622342,"user_tz":-480,"elapsed":2374,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["harvard_law = '\\nIn his important new book, The Schoolhouse Gate: Public Education, the Supreme Court, and the Battle for the American Mind, Professor Justin Driver reminds us that private controversies that arise within the confines of public schools are part of a broader historical arc — one that tracks a range of cultural and intellectual flashpoints in U.S. history. Moreover, Driver explains, these tensions are reflected in constitutional law, and indeed in the history and jurisprudence of the Supreme Court. As such, debates that arise in the context of public education are not simply about the conflict between academic freedom, public safety, and student rights. They mirror our persistent struggle to reconcile our interest in fostering a pluralistic society, rooted in the ideal of individual autonomy, with our desire to cultivate a sense of national unity and shared identity (or, put differently, our effort to reconcile our desire to forge common norms of citizenship with our fear of state indoctrination and overencroachment). In this regard, these debates reflect the unique role that both the school and the courts have played in defining and enforcing the boundaries of American citizenship. \\n'\n","print(\"harvard_law loaded\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["harvard_law loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zj49b9chqlUf","colab_type":"code","outputId":"2b066894-3f43-437b-d40c-8b1df8beca81","executionInfo":{"status":"ok","timestamp":1569150651535,"user_tz":-480,"elapsed":726,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["r_digest = '\\nThis week 30 passengers were reportedly injured when a Turkish Airlines flight landing at John F. Kennedy International Airport encountered turbulent conditions. Injuries included bruises, bloody noses, and broken bones. In mid-February, a Delta Airlines flight made an emergency landing to assist three passengers in getting to the nearest hospital after some sudden and unexpected turbulence. Doctors treated 15 passengers after a flight from Miami to Buenos Aires last October for everything from severe bruising to nosebleeds after the plane caught some rough winds over Brazil. In 2016, 23 passengers were injured on a United Airlines flight after severe turbulence threw people into the cabin ceiling. The list goes on. Turbulence has been become increasingly common, with painful outcomes for those on board. And more costly to the airlines, too. Forbes estimates that the cost of turbulence has risen to over $500 million each year in damages and delays. And there are no signs the increase in turbulence will be stopping anytime soon.\\n'\n","print(\"r_digest loaded\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["r_digest loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FaDXTa89qd9R","colab_type":"code","outputId":"ce629202-12a2-4d82-8fd4-2288c141a5a1","executionInfo":{"status":"ok","timestamp":1569150674113,"user_tz":-480,"elapsed":744,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["time_kids = '\\nThat, of course, is easier said than done. The more you eat salty foods, the more you develop a taste for them. The key to changing your diet is to start small. “Small changes in sodium in foods are not usually noticed,” Quader says. Eventually, she adds, the effort will reset a kid’s taste buds so the salt cravings stop. Bridget Murphy is a dietitian at New York University’s Langone Medical Center. She suggests kids try adding spices to their food instead of salt. Eating fruits and veggies and cutting back on packaged foods will also help. Need a little inspiration? Murphy offers this tip: Focus on the immediate effects of a diet that is high in sodium. High blood pressure can make it difficult to be active. “Do you want to be able to think clearly and perform well in school?” she asks. “If you’re an athlete, do you want to run faster?” If you answered yes to these questions, then it’s time to shake the salt habit.\\n'\n","print(\"time_kids loaded\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["time_kids loaded\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y_GTWEB2qJOM","colab_type":"text"},"source":["The excerpts are available as the following strings:\n","\n","- `forbes` - An excerpt from an article from Forbes magazine on the Chinese social credit score system.\n","- `harvard_law` - An excerpt from a book review published in Harvard Law Review.\n","- `r_digest` - An excerpt from a Reader's Digest article on flight turbulence.\n","- `time_kids` - An excerpt from an article on the ill effects of salt consumption published in TIME for Kids."]},{"cell_type":"code","metadata":{"id":"bzcMxChlkuKv","colab_type":"code","outputId":"aeb26fa9-66e3-4ca1-a11d-bdb0d805aceb","executionInfo":{"status":"ok","timestamp":1569150694272,"user_tz":-480,"elapsed":766,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["# Import Textatistic\n","from textatistic import Textatistic\n","\n","# List of excerpts\n","excerpts = [forbes, harvard_law, r_digest, time_kids]\n","\n","# Loop through excerpts and compute gunning fog index\n","gunning_fog_scores = []\n","for excerpt in excerpts:\n","  readability_scores = Textatistic(excerpt).scores\n","  gunning_fog = readability_scores['gunningfog_score']\n","  gunning_fog_scores.append(gunning_fog)\n","  \n","# Print the gunning fog indices\n","print(gunning_fog_scores)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[14.436002482929858, 20.735401069518716, 11.085587583148559, 5.926785009861934]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nWkV0jtcsZPl","colab_type":"text"},"source":["## 2. Text preprocessing, POS tagging and NER\n","\n","In this chapter, you will learn about tokenization and lemmatization. You will then learn how to perform text cleaning, part-of-speech tagging, and named entity recognition using the spaCy library. Upon mastering these concepts, you will proceed to make the Gettysburg address machine-friendly, analyze noun usage in fake news, and identify people mentioned in a TechCrunch article."]},{"cell_type":"markdown","metadata":{"id":"6zfhFFj-q-XZ","colab_type":"text"},"source":["###Tokenization and Lemmatization\n"]},{"cell_type":"markdown","metadata":{"id":"pjRZ64xNrAQi","colab_type":"text"},"source":["####Tokenizing the Gettysburg Address\n","\n","In this exercise, you will be tokenizing one of the most famous speeches of all time: the Gettysburg Address delivered by American President Abraham Lincoln during the American Civil War."]},{"cell_type":"code","metadata":{"id":"IzSuyvB_scCF","colab_type":"code","colab":{}},"source":["gettysburg = \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we're engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We're met on a great battlefield of that war. We've come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It's altogether fitting and proper that we should do this. But, in a larger sense, we can't dedicate - we can not consecrate - we can not hallow - this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It's rather for us to be here dedicated to the great task remaining before us - that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion - that we here highly resolve that these dead shall not have died in vain - that this nation, under God, shall have a new birth of freedom - and that government of the people, by the people, for the people, shall not perish from the earth.\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mn3ESi7lrK20","colab_type":"text"},"source":["The entire speech is available as a string named gettysburg.\n","\n","- Load the en_core_web_sm model using spacy.load().\n","- Create a Doc object doc for the gettysburg string.\n","- Using list comprehension, loop over doc to generate the token texts."]},{"cell_type":"code","metadata":{"id":"aUOR7rPBrOWX","colab_type":"code","outputId":"744360e9-ecc5-4443-d146-02f565460ceb","executionInfo":{"status":"ok","timestamp":1569150821255,"user_tz":-480,"elapsed":5737,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["import spacy\n","\n","# Load the en_core_web_sm model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Create a Doc object\n","doc = nlp(gettysburg)\n","\n","# Generate the tokens\n","tokens = [token.text for token in doc]\n","print(tokens)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceived', 'in', 'Liberty', ',', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', 'all', 'men', 'are', 'created', 'equal', '.', 'Now', 'we', \"'re\", 'engaged', 'in', 'a', 'great', 'civil', 'war', ',', 'testing', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceived', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.', 'We', \"'re\", 'met', 'on', 'a', 'great', 'battlefield', 'of', 'that', 'war', '.', 'We', \"'ve\", 'come', 'to', 'dedicate', 'a', 'portion', 'of', 'that', 'field', ',', 'as', 'a', 'final', 'resting', 'place', 'for', 'those', 'who', 'here', 'gave', 'their', 'lives', 'that', 'that', 'nation', 'might', 'live', '.', 'It', \"'s\", 'altogether', 'fitting', 'and', 'proper', 'that', 'we', 'should', 'do', 'this', '.', 'But', ',', 'in', 'a', 'larger', 'sense', ',', 'we', 'ca', \"n't\", 'dedicate', '-', 'we', 'can', 'not', 'consecrate', '-', 'we', 'can', 'not', 'hallow', '-', 'this', 'ground', '.', 'The', 'brave', 'men', ',', 'living', 'and', 'dead', ',', 'who', 'struggled', 'here', ',', 'have', 'consecrated', 'it', ',', 'far', 'above', 'our', 'poor', 'power', 'to', 'add', 'or', 'detract', '.', 'The', 'world', 'will', 'little', 'note', ',', 'nor', 'long', 'remember', 'what', 'we', 'say', 'here', ',', 'but', 'it', 'can', 'never', 'forget', 'what', 'they', 'did', 'here', '.', 'It', 'is', 'for', 'us', 'the', 'living', ',', 'rather', ',', 'to', 'be', 'dedicated', 'here', 'to', 'the', 'unfinished', 'work', 'which', 'they', 'who', 'fought', 'here', 'have', 'thus', 'far', 'so', 'nobly', 'advanced', '.', 'It', \"'s\", 'rather', 'for', 'us', 'to', 'be', 'here', 'dedicated', 'to', 'the', 'great', 'task', 'remaining', 'before', 'us', '-', 'that', 'from', 'these', 'honored', 'dead', 'we', 'take', 'increased', 'devotion', 'to', 'that', 'cause', 'for', 'which', 'they', 'gave', 'the', 'last', 'full', 'measure', 'of', 'devotion', '-', 'that', 'we', 'here', 'highly', 'resolve', 'that', 'these', 'dead', 'shall', 'not', 'have', 'died', 'in', 'vain', '-', 'that', 'this', 'nation', ',', 'under', 'God', ',', 'shall', 'have', 'a', 'new', 'birth', 'of', 'freedom', '-', 'and', 'that', 'government', 'of', 'the', 'people', ',', 'by', 'the', 'people', ',', 'for', 'the', 'people', ',', 'shall', 'not', 'perish', 'from', 'the', 'earth', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CS-BCb3HtNAV","colab_type":"text"},"source":["#### Lemmatizing the Gettysburg address\n","\n","In this exercise, we will perform lemmatization on the same gettysburg address from before.\n","\n","However, this time, we will also take a look at the speech, before and after lemmatization, and try to adjudge the kind of changes that take place to make the piece more machine friendly."]},{"cell_type":"code","metadata":{"id":"e6jXjgvjrOX9","colab_type":"code","outputId":"b6ca696a-49a4-434a-9c4e-8a032a23c8fd","executionInfo":{"status":"ok","timestamp":1569151347032,"user_tz":-480,"elapsed":723,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["# Print the gettysburg address\n","print(gettysburg)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we're engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We're met on a great battlefield of that war. We've come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It's altogether fitting and proper that we should do this. But, in a larger sense, we can't dedicate - we can not consecrate - we can not hallow - this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It's rather for us to be here dedicated to the great task remaining before us - that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion - that we here highly resolve that these dead shall not have died in vain - that this nation, under God, shall have a new birth of freedom - and that government of the people, by the people, for the people, shall not perish from the earth.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D0aGWci_tYCU","colab_type":"text"},"source":["- Loop over doc and extract the lemma for each token of gettysburg."]},{"cell_type":"code","metadata":{"id":"l7S-3G5drOZ2","colab_type":"code","outputId":"e71ef9f5-0991-48fe-e21c-95c23ef23e7c","executionInfo":{"status":"ok","timestamp":1569151379659,"user_tz":-480,"elapsed":970,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["import spacy\n","\n","# Load the en_core_web_sm model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Create a Doc object\n","doc = nlp(gettysburg)\n","\n","doc"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we're engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We're met on a great battlefield of that war. We've come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It's altogether fitting and proper that we should do this. But, in a larger sense, we can't dedicate - we can not consecrate - we can not hallow - this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It's rather for us to be here dedicated to the great task remaining before us - that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion - that we here highly resolve that these dead shall not have died in vain - that this nation, under God, shall have a new birth of freedom - and that government of the people, by the people, for the people, shall not perish from the earth."]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"AGsG6x29tdVv","colab_type":"code","colab":{}},"source":["# Generate lemmas\n","lemmas = [token.lemma_ for token in doc]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XMvL3U5-tinq","colab_type":"text"},"source":["- Convert lemmas into a string using join."]},{"cell_type":"code","metadata":{"id":"OS3DW5p8to2M","colab_type":"code","outputId":"dd5c5e2e-01c7-43f6-af7e-262f0f0a1cea","executionInfo":{"status":"ok","timestamp":1569151423539,"user_tz":-480,"elapsed":717,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["# Convert lemmas into a string\n","print(' '.join(lemmas))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["four score and seven year ago -PRON- father bring forth on this continent , a new nation , conceive in Liberty , and dedicate to the proposition that all man be create equal . now -PRON- be engage in a great civil war , test whether that nation , or any nation so conceive and so dedicated , can long endure . -PRON- be meet on a great battlefield of that war . -PRON- have come to dedicate a portion of that field , as a final resting place for those who here give -PRON- life that that nation may live . -PRON- be altogether fitting and proper that -PRON- should do this . but , in a large sense , -PRON- can not dedicate - -PRON- can not consecrate - -PRON- can not hallow - this ground . the brave man , live and dead , who struggle here , have consecrate -PRON- , far above -PRON- poor power to add or detract . the world will little note , nor long remember what -PRON- say here , but -PRON- can never forget what -PRON- do here . -PRON- be for -PRON- the living , rather , to be dedicate here to the unfinished work which -PRON- who fight here have thus far so nobly advanced . -PRON- be rather for -PRON- to be here dedicated to the great task remain before -PRON- - that from these honor dead -PRON- take increase devotion to that cause for which -PRON- give the last full measure of devotion - that -PRON- here highly resolve that these dead shall not have die in vain - that this nation , under God , shall have a new birth of freedom - and that government of the people , by the people , for the people , shall not perish from the earth .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jmdKMZpSts8g","colab_type":"text"},"source":["###Text cleaning\n"]},{"cell_type":"markdown","metadata":{"id":"2bLcIKf2tuyg","colab_type":"text"},"source":["####Cleaning a blog post\n","\n","In this exercise, you have been given an excerpt from a blog post. Your task is to clean this text into a more machine friendly format. \n","\n","- This will involve converting to lowercase, lemmatization and removing stopwords, punctuations and non-alphabetic characters."]},{"cell_type":"code","metadata":{"id":"OnNpS8Uzt4CX","colab_type":"code","colab":{}},"source":["blog = '\\nTwenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a steady rise in populist and far-right parties that have capitalized on Europe’s Immigration Crisis to raise nationalist and anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the Bundestag, thus upsetting Germany’s political order for the first time since the Second World War, the success of the Five Star Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland and Austria.\\n'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"usR-DfEut4EM","colab_type":"code","cellView":"form","colab":{}},"source":["#@title\n","stopwords =  ['fifteen',\n"," 'noone',\n"," 'whereupon',\n"," 'could',\n"," 'ten',\n"," 'all',\n"," 'please',\n"," 'indeed',\n"," 'whole',\n"," 'beside',\n"," 'therein',\n"," 'using',\n"," 'but',\n"," 'very',\n"," 'already',\n"," 'about',\n"," 'no',\n"," 'regarding',\n"," 'afterwards',\n"," 'front',\n"," 'go',\n"," 'in',\n"," 'make',\n"," 'three',\n"," 'here',\n"," 'what',\n"," 'without',\n"," 'yourselves',\n"," 'which',\n"," 'nothing',\n"," 'am',\n"," 'between',\n"," 'along',\n"," 'herein',\n"," 'sometimes',\n"," 'did',\n"," 'as',\n"," 'within',\n"," 'elsewhere',\n"," 'was',\n"," 'forty',\n"," 'becoming',\n"," 'how',\n"," 'will',\n"," 'other',\n"," 'bottom',\n"," 'these',\n"," 'amount',\n"," 'across',\n"," 'the',\n"," 'than',\n"," 'first',\n"," 'namely',\n"," 'may',\n"," 'none',\n"," 'anyway',\n"," 'again',\n"," 'eleven',\n"," 'his',\n"," 'meanwhile',\n"," 'name',\n"," 're',\n"," 'from',\n"," 'some',\n"," 'thru',\n"," 'upon',\n"," 'whither',\n"," 'he',\n"," 'such',\n"," 'down',\n"," 'my',\n"," 'often',\n"," 'whether',\n"," 'made',\n"," 'while',\n"," 'empty',\n"," 'two',\n"," 'latter',\n"," 'whatever',\n"," 'cannot',\n"," 'less',\n"," 'many',\n"," 'you',\n"," 'ours',\n"," 'done',\n"," 'thus',\n"," 'since',\n"," 'everything',\n"," 'for',\n"," 'more',\n"," 'unless',\n"," 'former',\n"," 'anyone',\n"," 'per',\n"," 'seeming',\n"," 'hereafter',\n"," 'on',\n"," 'yours',\n"," 'always',\n"," 'due',\n"," 'last',\n"," 'alone',\n"," 'one',\n"," 'something',\n"," 'twenty',\n"," 'until',\n"," 'latterly',\n"," 'seems',\n"," 'were',\n"," 'where',\n"," 'eight',\n"," 'ourselves',\n"," 'further',\n"," 'themselves',\n"," 'therefore',\n"," 'they',\n"," 'whenever',\n"," 'after',\n"," 'among',\n"," 'when',\n"," 'at',\n"," 'through',\n"," 'put',\n"," 'thereby',\n"," 'then',\n"," 'should',\n"," 'formerly',\n"," 'third',\n"," 'who',\n"," 'this',\n"," 'neither',\n"," 'others',\n"," 'twelve',\n"," 'also',\n"," 'else',\n"," 'seemed',\n"," 'has',\n"," 'ever',\n"," 'someone',\n"," 'its',\n"," 'that',\n"," 'does',\n"," 'sixty',\n"," 'why',\n"," 'do',\n"," 'whereas',\n"," 'are',\n"," 'either',\n"," 'hereupon',\n"," 'rather',\n"," 'because',\n"," 'might',\n"," 'those',\n"," 'via',\n"," 'hence',\n"," 'itself',\n"," 'show',\n"," 'perhaps',\n"," 'various',\n"," 'during',\n"," 'otherwise',\n"," 'thereafter',\n"," 'yourself',\n"," 'become',\n"," 'now',\n"," 'same',\n"," 'enough',\n"," 'been',\n"," 'take',\n"," 'their',\n"," 'seem',\n"," 'there',\n"," 'next',\n"," 'above',\n"," 'mostly',\n"," 'once',\n"," 'a',\n"," 'top',\n"," 'almost',\n"," 'six',\n"," 'every',\n"," 'nobody',\n"," 'any',\n"," 'say',\n"," 'each',\n"," 'them',\n"," 'must',\n"," 'she',\n"," 'throughout',\n"," 'whence',\n"," 'hundred',\n"," 'not',\n"," 'however',\n"," 'together',\n"," 'several',\n"," 'myself',\n"," 'i',\n"," 'anything',\n"," 'somehow',\n"," 'or',\n"," 'used',\n"," 'keep',\n"," 'much',\n"," 'thereupon',\n"," 'ca',\n"," 'just',\n"," 'behind',\n"," 'can',\n"," 'becomes',\n"," 'me',\n"," 'had',\n"," 'only',\n"," 'back',\n"," 'four',\n"," 'somewhere',\n"," 'if',\n"," 'by',\n"," 'whereafter',\n"," 'everywhere',\n"," 'beforehand',\n"," 'well',\n"," 'doing',\n"," 'everyone',\n"," 'nor',\n"," 'five',\n"," 'wherein',\n"," 'so',\n"," 'amongst',\n"," 'though',\n"," 'still',\n"," 'move',\n"," 'except',\n"," 'see',\n"," 'us',\n"," 'your',\n"," 'against',\n"," 'although',\n"," 'is',\n"," 'became',\n"," 'call',\n"," 'have',\n"," 'most',\n"," 'wherever',\n"," 'few',\n"," 'out',\n"," 'whom',\n"," 'yet',\n"," 'be',\n"," 'own',\n"," 'off',\n"," 'quite',\n"," 'with',\n"," 'and',\n"," 'side',\n"," 'whoever',\n"," 'would',\n"," 'both',\n"," 'fifty',\n"," 'before',\n"," 'full',\n"," 'get',\n"," 'sometime',\n"," 'beyond',\n"," 'part',\n"," 'least',\n"," 'besides',\n"," 'around',\n"," 'even',\n"," 'whose',\n"," 'hereby',\n"," 'up',\n"," 'being',\n"," 'we',\n"," 'an',\n"," 'him',\n"," 'below',\n"," 'moreover',\n"," 'really',\n"," 'it',\n"," 'of',\n"," 'our',\n"," 'nowhere',\n"," 'whereby',\n"," 'too',\n"," 'her',\n"," 'toward',\n"," 'anyhow',\n"," 'give',\n"," 'never',\n"," 'another',\n"," 'anywhere',\n"," 'mine',\n"," 'herself',\n"," 'over',\n"," 'himself',\n"," 'to',\n"," 'onto',\n"," 'into',\n"," 'thence',\n"," 'towards',\n"," 'hers',\n"," 'nevertheless',\n"," 'serious',\n"," 'under',\n"," 'nine']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SMSbPageuD3B","colab_type":"text"},"source":["The excerpt is available as a string blog and has been printed to the console. The list of stopwords are available as stopwords.\n","\n","- Using list comprehension, loop through doc to extract the lemma_ of each token.\n","- Remove stopwords and non-alphabetic tokens using stopwords and isalpha()."]},{"cell_type":"code","metadata":{"id":"xk3Cu9QVuKAf","colab_type":"code","outputId":"7fc71f54-226b-4d4c-c78c-8a75603335cf","executionInfo":{"status":"ok","timestamp":1569151569739,"user_tz":-480,"elapsed":969,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["# Load model and create Doc object\n","nlp = spacy.load('en_core_web_sm')\n","doc = nlp(blog)\n","\n","# Generate lemmatized tokens\n","lemmas = [token.lemma_ for token in doc]\n","\n","# Remove stopwords and non-alphabetic tokens\n","a_lemmas = [lemma for lemma in lemmas \n","            if lemma.isalpha() and lemma not in stopwords]\n","\n","# Print string after text cleaning\n","print(' '.join(a_lemmas))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["century politic witness alarming rise populism Europe warning sign come UK Brexit Referendum vote swinging way Leave follow stupendous victory billionaire Donald Trump President United States November Europe steady rise populist far right party capitalize Europe Immigration Crisis raise nationalist anti europe sentiment instance include Alternative Germany AfD win seat enter Bundestag upsetting Germany political order time Second World War success Five Star Movement Italy surge popularity neo nazism neo fascism country Hungary Czech Republic Poland Austria\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DOW3gCLAuOFB","colab_type":"text"},"source":["**Tips:**\n","\n","\n","It is always advisable to use your custom functions in place of isalpha() for more nuanced cases."]},{"cell_type":"markdown","metadata":{"id":"CQ0kMOezuZl6","colab_type":"text"},"source":["####Cleaning TED talks in a dataframe\n","\n","In this exercise, we will revisit the TED Talks from the first chapter. \n","\n","Your task is to clean these talks using techniques discussed earlier by writing a function preprocess and applying it to the transcript feature of the dataframe."]},{"cell_type":"code","metadata":{"id":"C4bHLHOduWBW","colab_type":"code","outputId":"e101ec9e-aec6-4ccf-fcc0-bf835e94c04e","executionInfo":{"status":"ok","timestamp":1569152089665,"user_tz":-480,"elapsed":712,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["ted.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(500, 1)"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"GIvR45_ZvcCN","colab_type":"code","outputId":"338f4a6a-d29c-4775-8deb-68ccb27a5276","executionInfo":{"status":"ok","timestamp":1569152090913,"user_tz":-480,"elapsed":792,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":198}},"source":["ted = ted[:20]\n","ted.tail()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>transcript</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>15</th>\n","      <td>I have given the slide show that I gave here t...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>I'd like to take you to another world. And I'd...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>I am failing as a woman, I am failing as a fem...</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>There have been many revolutions over the last...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Today, a baffled lady observed the shell where...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           transcript\n","15  I have given the slide show that I gave here t...\n","16  I'd like to take you to another world. And I'd...\n","17  I am failing as a woman, I am failing as a fem...\n","18  There have been many revolutions over the last...\n","19  Today, a baffled lady observed the shell where..."]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"FCsgniWtvWda","colab_type":"text"},"source":["You have been a given a dataframe ted consisting of 5 TED Talks. \n","\n","- Generate the Doc object for text. Ignore the disable argument for now.\n","- Generate lemmas using list comprehension using the lemma_ attribute.\n","- Remove non-alphabetic characters using isalpha() in the if condition."]},{"cell_type":"code","metadata":{"id":"gh63KgO5vcEU","colab_type":"code","colab":{}},"source":["# Function to preprocess text\n","def preprocess(text):\n","  \t# Create Doc object\n","    doc = nlp(text, disable=['ner', 'parser'])\n","    # Generate lemmas\n","    lemmas = [token.lemma_ for token in doc]\n","    # Remove stopwords and non-alphabetic characters\n","    a_lemmas = [lemma for lemma in lemmas \n","            if lemma.isalpha() and lemma not in stopwords]\n","    \n","    return ' '.join(a_lemmas)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jsCW-VOOuWDV","colab_type":"code","outputId":"8fd10314-8d73-4889-e554-03f0a75f0493","executionInfo":{"status":"ok","timestamp":1569152155643,"user_tz":-480,"elapsed":2385,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":380}},"source":["# Apply preprocess to ted['transcript']\n","ted['transcript'] = ted['transcript'].apply(preprocess)\n","print(ted['transcript'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0     talk new lecture TED illusion create TED try r...\n","1     representation brain brain break left half log...\n","2     great honor today share Digital Universe creat...\n","3     passion music technology thing combination thi...\n","4     use want computer new program programming requ...\n","5     neuroscientist mixed background physics medici...\n","6     Pat Mitchell day January begin like work love ...\n","7     Taylor Wilson year old nuclear physicist littl...\n","8     grow Northern Ireland right north end absolute...\n","9     publish article New York Times Modern Love col...\n","10    Joseph Member Parliament Kenya picture Maasai ...\n","11    hi talk little bit music machine life specific...\n","12    hi let ask audience question lie child raise h...\n","13    historical record allow know ancient Greeks dr...\n","14    good morning little boy experience change life...\n","15    slide year ago time short slide morning time w...\n","16    like world like share year old love story poor...\n","17    fail woman fail feminist passionate opinion ge...\n","18    revolution century significant longevity revol...\n","19    today baffled lady observe shell soul dwellsAn...\n","Name: transcript, dtype: object\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N0hefBOvwg2c","colab_type":"text"},"source":["### Part-of-speech tagging\n"]},{"cell_type":"markdown","metadata":{"id":"7V9SnZ83wjBM","colab_type":"text"},"source":["#### POS tagging in Lord of the Flies\n","\n","In this exercise, you will perform part-of-speech tagging on a famous passage from one of the most well-known novels of all time, Lord of the Flies, authored by William Golding."]},{"cell_type":"code","metadata":{"id":"nHGJqUQLuKCq","colab_type":"code","colab":{}},"source":["lotf = 'He found himself understanding the wearisomeness of this life, where every path was an improvisation and a considerable part of one’s waking life was spent watching one’s feet.'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-U8Xxs3UwxW5","colab_type":"text"},"source":["The passage is available as lotf and has already been printed to the console.\n","\n","- Load the `en_core_web_sm` model.\n","- Create a doc object for lotf using nlp().\n","- Using the text and pos_ attributes, generate tokens and their corresponding POS tags."]},{"cell_type":"code","metadata":{"id":"jQ1Tk62UuCmK","colab_type":"code","outputId":"cf210659-fbd5-42db-85af-6248e5616938","executionInfo":{"status":"ok","timestamp":1569152276316,"user_tz":-480,"elapsed":710,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["# Load the en_core_web_sm model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Create a Doc object\n","doc = nlp(lotf)\n","\n","# Generate tokens and pos tags\n","pos = [(token.text, token.pos_) for token in doc]\n","print(pos)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[('He', 'PRON'), ('found', 'VERB'), ('himself', 'PRON'), ('understanding', 'VERB'), ('the', 'DET'), ('wearisomeness', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('life', 'NOUN'), (',', 'PUNCT'), ('where', 'ADV'), ('every', 'DET'), ('path', 'NOUN'), ('was', 'VERB'), ('an', 'DET'), ('improvisation', 'NOUN'), ('and', 'CCONJ'), ('a', 'DET'), ('considerable', 'ADJ'), ('part', 'NOUN'), ('of', 'ADP'), ('one', 'NUM'), ('’s', 'PROPN'), ('waking', 'VERB'), ('life', 'NOUN'), ('was', 'VERB'), ('spent', 'VERB'), ('watching', 'VERB'), ('one', 'NUM'), ('’s', 'PROPN'), ('feet', 'NOUN'), ('.', 'PUNCT')]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nQnz8_S-w8KG","colab_type":"text"},"source":["#### Counting nouns in a piece of text\n","In this exercise, we will write two functions, nouns() and proper_nouns() that will count the number of other nouns and proper nouns in a piece of text respectively.\n","\n"]},{"cell_type":"code","metadata":{"id":"0_mn6hf7xDXK","colab_type":"code","colab":{}},"source":["nlp = spacy.load('en_core_web_sm')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-UQE1lAxDZc","colab_type":"code","colab":{}},"source":["# Returns number of proper nouns\n","def proper_nouns(text, model=nlp):\n","  \t# Create doc object\n","    doc = model(text)\n","    # Generate list of POS tags\n","    pos = [token.pos_ for token in doc]\n","    \n","    # Return number of proper nouns\n","    return pos.count('PROPN')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t47d-oxAt4GQ","colab_type":"code","outputId":"1a20bab2-155e-47ab-d6e7-2fa5b3176661","executionInfo":{"status":"ok","timestamp":1569152389659,"user_tz":-480,"elapsed":728,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Uh_Yo0dDxVTc","colab_type":"code","colab":{}},"source":["nlp = spacy.load('en_core_web_sm')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMklneNnxVYP","colab_type":"code","colab":{}},"source":["# Returns number of other nouns\n","def nouns(text, model=nlp):\n","  \t# Create doc object\n","    doc = model(text)\n","    # Generate list of POS tags\n","    pos = [token.pos_ for token in doc]\n","    \n","    # Return number of other nouns\n","    return pos.count('NOUN')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8V5WBsBxfrT","colab_type":"code","outputId":"61bd54b5-a5db-445c-d5cb-a966646bfb50","executionInfo":{"status":"ok","timestamp":1569152435071,"user_tz":-480,"elapsed":739,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp)) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NJuQNkssxGIN","colab_type":"text"},"source":["These functions will take in a piece of text and generate a list containing the POS tags for each word. It will then return the number of proper nouns/other nouns that the text contains. We will use these functions in the next exercise to generate interesting insights about fake news."]},{"cell_type":"markdown","metadata":{"id":"EwrQM3VtxivP","colab_type":"text"},"source":["#### Noun usage in fake news\n","\n","In this exercise, you have been given a dataframe headlines that contains news headlines that are either fake or real. Your task is to generate two new features num_propn and num_noun that represent the number of proper nouns and other nouns contained in the title feature of headlines.\n","\n","Next, we will compute the mean number of proper nouns and other nouns used in fake and real news headlines and compare the values. If there is a remarkable difference, then there is a good chance that using the num_propn and num_noun features in fake news detectors will improve its performance.\n","\n","To accomplish this task, the functions proper_nouns and nouns that you had built in the previous exercise have already been made available to you."]},{"cell_type":"code","metadata":{"id":"bta-bMpMxniP","colab_type":"code","outputId":"2849a8dd-7362-4ba0-a6d3-e28be4207ad8","executionInfo":{"status":"ok","timestamp":1569152513002,"user_tz":-480,"elapsed":723,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"source":["headlines = pd.read_csv(\"fakenews.csv\")\n","print(\"headlines shape: \", headlines.shape)\n","headlines.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["headlines shape:  (100, 3)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>title</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>You Can Smell Hillary’s Fear</td>\n","      <td>FAKE</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n","      <td>FAKE</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Kerry to go to Paris in gesture of sympathy</td>\n","      <td>REAL</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n","      <td>FAKE</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>The Battle of New York: Why This Primary Matters</td>\n","      <td>REAL</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                                              title label\n","0           0                       You Can Smell Hillary’s Fear  FAKE\n","1           1  Watch The Exact Moment Paul Ryan Committed Pol...  FAKE\n","2           2        Kerry to go to Paris in gesture of sympathy  REAL\n","3           3  Bernie supporters on Twitter erupt in anger ag...  FAKE\n","4           4   The Battle of New York: Why This Primary Matters  REAL"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"markdown","metadata":{"id":"She-PIrhx4j3","colab_type":"text"},"source":["- Create a new feature num_propn by applying proper_nouns to headlines['title'].\n","- Create a new feature num_noun by applying nouns to headlines['title'].\n","- Filter headlines to compute the mean number of proper nouns in fake news using the mean method.\n","- Filter headlines to compute the mean number of other nouns in real news using the mean method."]},{"cell_type":"code","metadata":{"id":"PnMpzL05x8ai","colab_type":"code","colab":{}},"source":["headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n","headlines['num_noun'] = headlines['title'].apply(nouns)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"27S4p9VJyKZG","colab_type":"code","outputId":"7c04f86d-f911-431a-dac9-803823988de0","executionInfo":{"status":"ok","timestamp":1569152615562,"user_tz":-480,"elapsed":722,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":302}},"source":["headlines.tail()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>title</th>\n","      <th>label</th>\n","      <th>num_propn</th>\n","      <th>num_noun</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>95</th>\n","      <td>95</td>\n","      <td>The Mandela Effect was made by one overlooked ...</td>\n","      <td>FAKE</td>\n","      <td>2</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>96</td>\n","      <td>CNN: One voter can make a difference by voting...</td>\n","      <td>FAKE</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>97</td>\n","      <td>Give Social Security recipients a CEO-style raise</td>\n","      <td>REAL</td>\n","      <td>2</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>98</td>\n","      <td>Fireworks erupt between Trump and Bush, Rubio ...</td>\n","      <td>REAL</td>\n","      <td>5</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>99</td>\n","      <td>Obama, sounding like his critics, admits no 'c...</td>\n","      <td>REAL</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Unnamed: 0  ... num_noun\n","95          95  ...        5\n","96          96  ...        2\n","97          97  ...        4\n","98          98  ...        3\n","99          99  ...        2\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"id":"N5Mu5jWkx8dl","colab_type":"code","colab":{}},"source":["# Compute mean of proper nouns\n","real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n","fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TOUU3Cb4yPhY","colab_type":"code","outputId":"e23d5bde-0e40-4014-af42-57c6b42ad4bc","executionInfo":{"status":"ok","timestamp":1569152633007,"user_tz":-480,"elapsed":718,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["real_propn"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.6491228070175437"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"code","metadata":{"id":"QkMLCWULyGtM","colab_type":"code","colab":{}},"source":["# Compute mean of other nouns\n","real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n","fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uRZ3hRv3yTSe","colab_type":"code","outputId":"980261ea-7e76-4b5f-fdfc-eb2204843be2","executionInfo":{"status":"ok","timestamp":1569152645870,"user_tz":-480,"elapsed":717,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["real_noun"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.3684210526315788"]},"metadata":{"tags":[]},"execution_count":75}]},{"cell_type":"code","metadata":{"id":"Rev6lQ_EyGrB","colab_type":"code","outputId":"c20ff1df-ea2e-4fff-d3aa-12ad7e52423b","executionInfo":{"status":"ok","timestamp":1569152649156,"user_tz":-480,"elapsed":891,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["# Print results\n","print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))\n","print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mean no. of proper nouns in real and fake headlines are 2.65 and 4.79 respectively\n","Mean no. of other nouns in real and fake headlines are 2.37 and 1.67 respectively\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5JVq2ZWX2UT6","colab_type":"text"},"source":["###Named entity recognition\n"]},{"cell_type":"markdown","metadata":{"id":"LlfSBBSP2Y4f","colab_type":"text"},"source":["#### Named entities in a sentence\n","In this exercise, we will identify and classify the labels of various named entities in a body of text using one of spaCy's statistical models. We will also verify the veracity of these labels.\n","\n","- Use `spacy.load()` to load the `en_core_web_sm model`.\n","- Create a Doc instance doc using text and nlp.\n","- Loop over doc.ents to print all the named entities and their corresponding labels."]},{"cell_type":"code","metadata":{"id":"2an2avgtxnkr","colab_type":"code","outputId":"62eb2ccb-e6b8-4460-e944-af581281a032","executionInfo":{"status":"ok","timestamp":1569153789357,"user_tz":-480,"elapsed":1002,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# Load the required model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Create a Doc instance \n","text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n","doc = nlp(text)\n","\n","# Print all named entities and their labels\n","for ent in doc.ents:\n","    print(ent.text, ent.label_)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Sundar Pichai PERSON\n","Google ORG\n","Mountain View GPE\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-OLllke52sy1","colab_type":"text"},"source":["#### Identifying people mentioned in a news article\n","\n","In this exercise, you have been given an excerpt from a news article published in TechCrunch. Your task is to write a function find_people that identifies the names of people that have been mentioned in a particular piece of text. You will then use find_people to identify the people of interest in the article."]},{"cell_type":"code","metadata":{"id":"kXC3Rox32wfL","colab_type":"code","colab":{}},"source":["tc = \"\\nIt’s' been a busy day for Facebook  exec op-eds. Earlier this morning, Sheryl Sandberg broke the site’s silence around the Christchurch massacre, and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the sorts of data Facebook traffics in. He’s hoping to get out in front of heavy-handed regulation and get a seat at the table shaping it.\\n\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kIRLAdF32wkR","colab_type":"text"},"source":["The article is available as the string tc and has been printed to the console. The required spacy model has also been already loaded as nlp.\n","\n","- Create a Doc object for text.\n","- Using list comprehension, loop through doc.ents and create a list of named entities whose label is PERSON.\n","- Using find_persons(), print the people mentioned in tc."]},{"cell_type":"code","metadata":{"id":"ptXuZr_g22fX","colab_type":"code","colab":{}},"source":["def find_persons(text):\n","  # Create Doc object\n","  doc = nlp(text)\n","  \n","  # Identify the persons\n","  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n","  \n","  # Return persons\n","  return persons\n","\n","print(find_persons(tc))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zxM7pfdVseCE","colab_type":"text"},"source":["## 3. N-Gram models\n","\n","Learn about n-gram modeling and use it to perform sentiment analysis on movie reviews."]},{"cell_type":"markdown","metadata":{"id":"TjZVlWQB2-zm","colab_type":"text"},"source":["### Building a bag of words model\n"]},{"cell_type":"markdown","metadata":{"id":"p-KKY4EX3DS8","colab_type":"text"},"source":["####BoW model for movie taglines\n","\n","In this exercise, you have been provided with a corpus of more than 7000 movie tag lines. Your job is to generate the bag of words representation bow_matrix for these taglines. \n","\n","For this exercise, we will ignore the text preprocessing step and generate bow_matrix directly."]},{"cell_type":"code","metadata":{"id":"t7gJntDS22hu","colab_type":"code","outputId":"4239813a-fa73-4344-d243-ba349f0d548b","executionInfo":{"status":"ok","timestamp":1569202201050,"user_tz":-480,"elapsed":848,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["movies = pd.read_csv(\"movie_overviews.csv\")\n","movies.tail()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>overview</th>\n","      <th>tagline</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9094</th>\n","      <td>159550</td>\n","      <td>The Last Brickmaker in America</td>\n","      <td>A man must cope with the loss of his wife and ...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>9095</th>\n","      <td>392572</td>\n","      <td>Rustom</td>\n","      <td>Rustom Pavri, an honourable officer of the Ind...</td>\n","      <td>Decorated Officer. Devoted Family Man. Defendi...</td>\n","    </tr>\n","    <tr>\n","      <th>9096</th>\n","      <td>402672</td>\n","      <td>Mohenjo Daro</td>\n","      <td>Village lad Sarman is drawn to big, bad Mohenj...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>9097</th>\n","      <td>315011</td>\n","      <td>Shin Godzilla</td>\n","      <td>From the mind behind Evangelion comes a hit la...</td>\n","      <td>A god incarnate. A city doomed.</td>\n","    </tr>\n","    <tr>\n","      <th>9098</th>\n","      <td>391698</td>\n","      <td>The Beatles: Eight Days a Week - The Touring Y...</td>\n","      <td>The band stormed Europe in 1963, and, in 1964,...</td>\n","      <td>The band you know. The story you don't.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          id  ...                                            tagline\n","9094  159550  ...                                                NaN\n","9095  392572  ...  Decorated Officer. Devoted Family Man. Defendi...\n","9096  402672  ...                                                NaN\n","9097  315011  ...                    A god incarnate. A city doomed.\n","9098  391698  ...            The band you know. The story you don't.\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"ntvekqKFshq7","colab_type":"code","outputId":"a02d347d-e4bb-48a0-ee7d-191f43b1fa26","executionInfo":{"status":"ok","timestamp":1569155756624,"user_tz":-480,"elapsed":715,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["# deleted rows with missing strings\n","# corpus = movies.replace(np.nan, '', regex=True)\n","corpus = movies.dropna(subset = ['tagline'])\n","corpus.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>overview</th>\n","      <th>tagline</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>8844</td>\n","      <td>Jumanji</td>\n","      <td>When siblings Judy and Peter discover an encha...</td>\n","      <td>Roll the dice and unleash the excitement!</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>15602</td>\n","      <td>Grumpier Old Men</td>\n","      <td>A family wedding reignites the ancient feud be...</td>\n","      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>31357</td>\n","      <td>Waiting to Exhale</td>\n","      <td>Cheated on, mistreated and stepped on, the wom...</td>\n","      <td>Friends are the people who let you be yourself...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11862</td>\n","      <td>Father of the Bride Part II</td>\n","      <td>Just when George Banks has recovered from his ...</td>\n","      <td>Just When His World Is Back To Normal... He's ...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>949</td>\n","      <td>Heat</td>\n","      <td>Obsessive master thief, Neil McCauley leads a ...</td>\n","      <td>A Los Angeles Crime Saga</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id  ...                                            tagline\n","1   8844  ...          Roll the dice and unleash the excitement!\n","2  15602  ...  Still Yelling. Still Fighting. Still Ready for...\n","3  31357  ...  Friends are the people who let you be yourself...\n","4  11862  ...  Just When His World Is Back To Normal... He's ...\n","5    949  ...                           A Los Angeles Crime Saga\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":123}]},{"cell_type":"code","metadata":{"id":"gjJUfdLs4_Bo","colab_type":"code","outputId":"9556819a-ee02-4bf2-a6c2-fe19fef06f67","executionInfo":{"status":"ok","timestamp":1569155757517,"user_tz":-480,"elapsed":639,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["corpus = corpus['tagline']\n","corpus.tail(15)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9076                                  Who You Gonna Call?\n","9077    Saving the world takes a little Hart and a big...\n","9078                                    You know his name\n","9079          How far will a family go to bury the truth?\n","9080                                       Human. Nature.\n","9081                                   Keep America great\n","9082          They needed hot dates. They got hot messes.\n","9084                           Life  can be a real mother\n","9086                       Are you a watcher or a player?\n","9090                          Blood always follows money.\n","9091                        Kingsglaive: Final Fantasy XV\n","9093    What happens in Vegas, stays in Vegas. Unless ...\n","9095    Decorated Officer. Devoted Family Man. Defendi...\n","9097                      A god incarnate. A city doomed.\n","9098              The band you know. The story you don't.\n","Name: tagline, dtype: object"]},"metadata":{"tags":[]},"execution_count":124}]},{"cell_type":"markdown","metadata":{"id":"9TRPwbTw4NDR","colab_type":"text"},"source":["- Import the CountVectorizer class from sklearn.\n","- Instantiate a CountVectorizer object. Name it vectorizer.\n","- Using fit_transform(), generate bow_matrix for corpus."]},{"cell_type":"code","metadata":{"id":"gqjJAP854QXG","colab_type":"code","outputId":"1a70927d-2865-4163-84aa-0d79c9b9a263","executionInfo":{"status":"ok","timestamp":1569155767340,"user_tz":-480,"elapsed":711,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Import CountVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Create CountVectorizer object\n","vectorizer = CountVectorizer()\n","\n","# Generate matrix of word vectors\n","bow_matrix = vectorizer.fit_transform(corpus) # input is series, not dataframe\n","\n","# Print the shape of bow_matrix\n","print(bow_matrix.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(7033, 6614)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r_nexzHN6zbT","colab_type":"text"},"source":["####Analyzing dimensionality and preprocessing\n","In this exercise, you have been provided with a lem_corpus which contains the pre-processed versions of the movie taglines from the previous exercise. In other words, the taglines have been lowercased and lemmatized, and stopwords have been removed.\n","\n","Your job is to generate the bag of words representation bow_lem_matrix for these lemmatized taglines and compare its shape with that of bow_matrix obtained in the previous exercise. The first five lemmatized taglines in lem_corpus have been printed to the console for you to examine."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"437kAJ9Q7M4Z","colab":{}},"source":["# Function to preprocess text\n","def preprocess(text):\n","  \t# Create Doc object\n","    doc = nlp(text, disable=['ner', 'parser'])\n","    # Generate lemmas\n","    lemmas = [token.lemma_ for token in doc]\n","    # Remove stopwords and non-alphabetic characters\n","    a_lemmas = [lemma for lemma in lemmas \n","            if lemma.isalpha() and lemma not in stopwords]\n","    \n","    return ' '.join(a_lemmas)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"a62ca9e0-8f3e-47b0-dd94-48f3d6f3e8d8","executionInfo":{"status":"ok","timestamp":1569156203117,"user_tz":-480,"elapsed":18994,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"id":"VsCmozjf7M4c","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Apply preprocess to ted['transcript']\n","lem_corpus = corpus.apply(preprocess)\n","print(lem_corpus)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1                            roll dice unleash excitement\n","2                                   yell fight Ready love\n","3                            friend people let let forget\n","4                              world normal surprise life\n","5                                  Los Angeles Crime Saga\n","6                 cordially invite surprising merger year\n","7                                       Original bad Boys\n","8                                         terror overtime\n","9                                   limit fear substitute\n","10                          powerful man world thing want\n","12                                Part Dog Part Wolf Hero\n","13      Triumphant Victory Bitter Defeat change World ...\n","14      course Set turn prepare weapon Summon Courage ...\n","15                                           stay forever\n","16                                  lose heart come sense\n","17      outrageous guest scandalous request lone bellh...\n","18                          new animal new adventure hair\n","19                                                Get way\n","20                                mob tough like business\n","21      man copy notorious killer history time woman s...\n","22        shadow life business death man find reason live\n","23                          extraordinary encounter human\n","24                                               love way\n","25                               Envy greed jealousy love\n","26                                       woman girl leave\n","28                                          happily dream\n","29                     Shanghai violence problem solution\n","30                                 break rule change life\n","31                                         future history\n","32                                    little pig long way\n","                              ...                        \n","9058                                     story Snow White\n","9059                                         new neighbor\n","9060                                       silence killer\n","9061                                    conspiracy theory\n","9062              unforgettable journey probably remember\n","9063                                        lay vengeance\n","9064                                celebrate day connect\n","9065                                         private dick\n","9067                                         deep shallow\n","9068                    expect controversial relationship\n","9069                                         fight future\n","9070                                        Have Anything\n","9071                          look live inside television\n","9072                                          raise shell\n","9074              true story case file Ed Lorraine Warren\n","9076                                                     \n","9077                   save world little Hart big johnson\n","9078                                                 know\n","9079                                far family bury truth\n","9080                                         human nature\n","9081                                        America great\n","9082                               need hot date hot mess\n","9084                                     life real mother\n","9086                                       watcher player\n","9090                                   blood follow money\n","9091                         kingsglaive final Fantasy xv\n","9093                       happen Vegas stay Vegas happen\n","9095    Decorated Officer Devoted Family Man defend ho...\n","9097                              god incarnate city doom\n","9098                                      band know story\n","Name: tagline, Length: 7033, dtype: object\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CkR5Wzli_IJC","colab_type":"code","outputId":"079e559f-c259-4e79-f5c5-59113cdee807","executionInfo":{"status":"ok","timestamp":1569156012480,"user_tz":-480,"elapsed":717,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["lem_corpus[9076]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["''"]},"metadata":{"tags":[]},"execution_count":134}]},{"cell_type":"code","metadata":{"id":"e4hsSuAq_t0l","colab_type":"code","outputId":"41e3db60-1131-4118-ae5e-9b37d90f07aa","executionInfo":{"status":"ok","timestamp":1569156750970,"user_tz":-480,"elapsed":731,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":669}},"source":["# replace empty string with nan\n","temp = pd.DataFrame(lem_corpus.replace(r'\\s+( +\\.)|#',np.nan,regex=True).replace('',np.nan))\n","temp.tail(20)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tagline</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9069</th>\n","      <td>fight future</td>\n","    </tr>\n","    <tr>\n","      <th>9070</th>\n","      <td>Have Anything</td>\n","    </tr>\n","    <tr>\n","      <th>9071</th>\n","      <td>look live inside television</td>\n","    </tr>\n","    <tr>\n","      <th>9072</th>\n","      <td>raise shell</td>\n","    </tr>\n","    <tr>\n","      <th>9074</th>\n","      <td>true story case file Ed Lorraine Warren</td>\n","    </tr>\n","    <tr>\n","      <th>9076</th>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>9077</th>\n","      <td>save world little Hart big johnson</td>\n","    </tr>\n","    <tr>\n","      <th>9078</th>\n","      <td>know</td>\n","    </tr>\n","    <tr>\n","      <th>9079</th>\n","      <td>far family bury truth</td>\n","    </tr>\n","    <tr>\n","      <th>9080</th>\n","      <td>human nature</td>\n","    </tr>\n","    <tr>\n","      <th>9081</th>\n","      <td>America great</td>\n","    </tr>\n","    <tr>\n","      <th>9082</th>\n","      <td>need hot date hot mess</td>\n","    </tr>\n","    <tr>\n","      <th>9084</th>\n","      <td>life real mother</td>\n","    </tr>\n","    <tr>\n","      <th>9086</th>\n","      <td>watcher player</td>\n","    </tr>\n","    <tr>\n","      <th>9090</th>\n","      <td>blood follow money</td>\n","    </tr>\n","    <tr>\n","      <th>9091</th>\n","      <td>kingsglaive final Fantasy xv</td>\n","    </tr>\n","    <tr>\n","      <th>9093</th>\n","      <td>happen Vegas stay Vegas happen</td>\n","    </tr>\n","    <tr>\n","      <th>9095</th>\n","      <td>Decorated Officer Devoted Family Man defend ho...</td>\n","    </tr>\n","    <tr>\n","      <th>9097</th>\n","      <td>god incarnate city doom</td>\n","    </tr>\n","    <tr>\n","      <th>9098</th>\n","      <td>band know story</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                tagline\n","9069                                       fight future\n","9070                                      Have Anything\n","9071                        look live inside television\n","9072                                        raise shell\n","9074            true story case file Ed Lorraine Warren\n","9076                                                NaN\n","9077                 save world little Hart big johnson\n","9078                                               know\n","9079                              far family bury truth\n","9080                                       human nature\n","9081                                      America great\n","9082                             need hot date hot mess\n","9084                                   life real mother\n","9086                                     watcher player\n","9090                                 blood follow money\n","9091                       kingsglaive final Fantasy xv\n","9093                     happen Vegas stay Vegas happen\n","9095  Decorated Officer Devoted Family Man defend ho...\n","9097                            god incarnate city doom\n","9098                                    band know story"]},"metadata":{"tags":[]},"execution_count":158}]},{"cell_type":"code","metadata":{"id":"-dYdEpv68hd-","colab_type":"code","outputId":"bc7c14be-e923-45a0-e3a8-982c9b601577","executionInfo":{"status":"ok","timestamp":1569156822232,"user_tz":-480,"elapsed":728,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# replace empty string with nan\n","# lem_corpus.replace(r'^\\s*$', np.nan, regex=True)\n","lem_corpus = temp.dropna()\n","lem_corpus = lem_corpus['tagline']\n","lem_corpus"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1                            roll dice unleash excitement\n","2                                   yell fight Ready love\n","3                            friend people let let forget\n","4                              world normal surprise life\n","5                                  Los Angeles Crime Saga\n","6                 cordially invite surprising merger year\n","7                                       Original bad Boys\n","8                                         terror overtime\n","9                                   limit fear substitute\n","10                          powerful man world thing want\n","12                                Part Dog Part Wolf Hero\n","13      Triumphant Victory Bitter Defeat change World ...\n","14      course Set turn prepare weapon Summon Courage ...\n","15                                           stay forever\n","16                                  lose heart come sense\n","17      outrageous guest scandalous request lone bellh...\n","18                          new animal new adventure hair\n","19                                                Get way\n","20                                mob tough like business\n","21      man copy notorious killer history time woman s...\n","22        shadow life business death man find reason live\n","23                          extraordinary encounter human\n","24                                               love way\n","25                               Envy greed jealousy love\n","26                                       woman girl leave\n","28                                          happily dream\n","29                     Shanghai violence problem solution\n","30                                 break rule change life\n","31                                         future history\n","32                                    little pig long way\n","                              ...                        \n","9057         Boy meet girl girl unimpresse boy start band\n","9058                                     story Snow White\n","9059                                         new neighbor\n","9060                                       silence killer\n","9061                                    conspiracy theory\n","9062              unforgettable journey probably remember\n","9063                                        lay vengeance\n","9064                                celebrate day connect\n","9065                                         private dick\n","9067                                         deep shallow\n","9068                    expect controversial relationship\n","9069                                         fight future\n","9070                                        Have Anything\n","9071                          look live inside television\n","9072                                          raise shell\n","9074              true story case file Ed Lorraine Warren\n","9077                   save world little Hart big johnson\n","9078                                                 know\n","9079                                far family bury truth\n","9080                                         human nature\n","9081                                        America great\n","9082                               need hot date hot mess\n","9084                                     life real mother\n","9086                                       watcher player\n","9090                                   blood follow money\n","9091                         kingsglaive final Fantasy xv\n","9093                       happen Vegas stay Vegas happen\n","9095    Decorated Officer Devoted Family Man defend ho...\n","9097                              god incarnate city doom\n","9098                                      band know story\n","Name: tagline, Length: 6970, dtype: object"]},"metadata":{"tags":[]},"execution_count":161}]},{"cell_type":"markdown","metadata":{"id":"tAJqfGbOCVyq","colab_type":"text"},"source":["**Observation:**\n","\n","The reduced number of dimensions on account of text preprocessing usually leads to better performance when conducting machine learning and it is a good idea to consider it. "]},{"cell_type":"markdown","metadata":{"id":"QHNVoCVS8IFI","colab_type":"text"},"source":["- Import the CountVectorizer class from sklearn.\n","- Instantiate a CountVectorizer object. Name it vectorizer.\n","- Using fit_transform(), generate bow_lem_matrix for lem_corpus."]},{"cell_type":"code","metadata":{"id":"wpLdClWD4QZL","colab_type":"code","outputId":"e13ce751-b037-42df-fd8a-457fd257ee52","executionInfo":{"status":"ok","timestamp":1569156868249,"user_tz":-480,"elapsed":897,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Import CountVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Create CountVectorizer object\n","vectorizer = CountVectorizer()\n","\n","# Generate matrix of word vectors\n","bow_lem_matrix = vectorizer.fit_transform(lem_corpus)\n","\n","# Print the shape of bow_lem_matrix\n","print(bow_lem_matrix.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(6970, 5284)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a-IWcFGqCcfp","colab_type":"text"},"source":["#### Mapping feature indices with feature names\n","\n","In the lesson video, we had seen that CountVectorizer doesn't necessarily index the vocabulary in alphabetical order. In this exercise, we will learn to map each feature index to its corresponding feature name from the vocabulary."]},{"cell_type":"code","metadata":{"id":"dfZChK5N4QbB","colab_type":"code","colab":{}},"source":["corpus = ['The lion is the king of the jungle',\n"," 'Lions have lifespans of a decade',\n"," 'The lion is an endangered species']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mlo25RJeClRD","colab_type":"text"},"source":["We will use the same three sentences on lions from the video. The sentences are available in a list named corpus and has already been printed to the console.\n","\n","- Instantiate a CountVectorizer object. Name it vectorizer.\n","- Using fit_transform(), generate bow_matrix for corpus.\n","- Using the get_feature_names() method, map the column names to the corresponding word in the vocabulary."]},{"cell_type":"code","metadata":{"id":"Fj8syP-UCoZI","colab_type":"code","outputId":"0de026bf-6c94-4c41-ba06-e2b6d6efbcb5","executionInfo":{"status":"ok","timestamp":1569156967935,"user_tz":-480,"elapsed":715,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# Create CountVectorizer object\n","vectorizer = CountVectorizer()\n","\n","# Generate matrix of word vectors\n","bow_matrix = vectorizer.fit_transform(corpus)\n","\n","# Convert bow_matrix into a DataFrame\n","bow_df = pd.DataFrame(bow_matrix.toarray())\n","\n","# Map the column names to vocabulary \n","bow_df.columns = vectorizer.get_feature_names()\n","\n","# Print bow_df\n","print(bow_df)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   an  decade  endangered  have  is  ...  lion  lions  of  species  the\n","0   0       0           0     0   1  ...     1      0   1        0    3\n","1   0       1           0     1   0  ...     0      1   1        0    0\n","2   1       0           1     0   1  ...     1      0   0        1    1\n","\n","[3 rows x 13 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iVZOo6DnDddj","colab_type":"text"},"source":["###Building a BoW Naive Bayes classifier\n"]},{"cell_type":"markdown","metadata":{"id":"JUp9YkPeDgwt","colab_type":"text"},"source":["####BoW vectors for movie reviews\n","\n","Your task is to preprocess the reviews and generate BoW vectors for these two sets using CountVectorizer.\n","\n","Once we have generated the BoW vector matrices X_train_bow and X_test_bow, we will be in a very good position to apply a machine learning model to it and conduct sentiment analysis."]},{"cell_type":"code","metadata":{"id":"1X2XA0MfCobW","colab_type":"code","outputId":"99577843-c00d-4ef4-9b52-01b4657818a8","executionInfo":{"status":"ok","timestamp":1569202211146,"user_tz":-480,"elapsed":865,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["movies_clean = pd.read_csv(\"movie_reviews_clean.csv\")\n","movies_clean.tail()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>995</th>\n","      <td>this movie turned out to be pretty much what i...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>from time to time it s very advisable for the ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>ed wood is eclipsed and becomes orson welles t...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>well here we have yet another role reversal mo...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>there is a lot of obvious hype associated with...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                review  sentiment\n","995  this movie turned out to be pretty much what i...          1\n","996  from time to time it s very advisable for the ...          0\n","997  ed wood is eclipsed and becomes orson welles t...          0\n","998  well here we have yet another role reversal mo...          0\n","999  there is a lot of obvious hype associated with...          0"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"cbIRz8HGElDe","colab_type":"code","cellView":"form","outputId":"b0af56fc-131f-407d-9a5d-506e7c55c664","executionInfo":{"status":"ok","timestamp":1569157517984,"user_tz":-480,"elapsed":740,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@title\n","train_ind = [446,\n"," 937,\n"," 316,\n"," 990,\n"," 793,\n"," 649,\n"," 182,\n"," 50,\n"," 858,\n"," 536,\n"," 783,\n"," 255,\n"," 950,\n"," 800,\n"," 309,\n"," 701,\n"," 606,\n"," 779,\n"," 463,\n"," 46,\n"," 314,\n"," 440,\n"," 877,\n"," 799,\n"," 696,\n"," 413,\n"," 177,\n"," 34,\n"," 984,\n"," 436,\n"," 917,\n"," 247,\n"," 206,\n"," 650,\n"," 556,\n"," 396,\n"," 265,\n"," 728,\n"," 507,\n"," 654,\n"," 377,\n"," 618,\n"," 69,\n"," 768,\n"," 55,\n"," 875,\n"," 644,\n"," 861,\n"," 157,\n"," 305,\n"," 386,\n"," 886,\n"," 325,\n"," 501,\n"," 626,\n"," 45,\n"," 306,\n"," 315,\n"," 156,\n"," 718,\n"," 302,\n"," 92,\n"," 939,\n"," 415,\n"," 126,\n"," 37,\n"," 167,\n"," 303,\n"," 831,\n"," 272,\n"," 983,\n"," 705,\n"," 525,\n"," 888,\n"," 695,\n"," 782,\n"," 991,\n"," 208,\n"," 239,\n"," 148,\n"," 532,\n"," 954,\n"," 245,\n"," 104,\n"," 922,\n"," 787,\n"," 575,\n"," 483,\n"," 613,\n"," 435,\n"," 175,\n"," 837,\n"," 513,\n"," 867,\n"," 418,\n"," 161,\n"," 136,\n"," 774,\n"," 56,\n"," 252,\n"," 804,\n"," 573,\n"," 683,\n"," 111,\n"," 93,\n"," 18,\n"," 400,\n"," 975,\n"," 930,\n"," 907,\n"," 672,\n"," 23,\n"," 997,\n"," 153,\n"," 74,\n"," 127,\n"," 771,\n"," 365,\n"," 380,\n"," 487,\n"," 250,\n"," 209,\n"," 609,\n"," 125,\n"," 391,\n"," 839,\n"," 669,\n"," 466,\n"," 83,\n"," 928,\n"," 447,\n"," 262,\n"," 591,\n"," 732,\n"," 251,\n"," 294,\n"," 84,\n"," 191,\n"," 467,\n"," 506,\n"," 852,\n"," 892,\n"," 278,\n"," 757,\n"," 586,\n"," 453,\n"," 913,\n"," 376,\n"," 389,\n"," 717,\n"," 269,\n"," 270,\n"," 583,\n"," 703,\n"," 497,\n"," 166,\n"," 574,\n"," 184,\n"," 485,\n"," 681,\n"," 233,\n"," 651,\n"," 444,\n"," 420,\n"," 498,\n"," 256,\n"," 464,\n"," 194,\n"," 637,\n"," 851,\n"," 117,\n"," 271,\n"," 368,\n"," 648,\n"," 959,\n"," 300,\n"," 369,\n"," 815,\n"," 677,\n"," 225,\n"," 179,\n"," 670,\n"," 30,\n"," 691,\n"," 739,\n"," 988,\n"," 164,\n"," 356,\n"," 346,\n"," 135,\n"," 457,\n"," 3,\n"," 622,\n"," 168,\n"," 122,\n"," 89,\n"," 518,\n"," 145,\n"," 841,\n"," 656,\n"," 547,\n"," 500,\n"," 840,\n"," 870,\n"," 658,\n"," 577,\n"," 462,\n"," 713,\n"," 964,\n"," 333,\n"," 144,\n"," 992,\n"," 570,\n"," 521,\n"," 657,\n"," 165,\n"," 374,\n"," 347,\n"," 235,\n"," 826,\n"," 121,\n"," 866,\n"," 338,\n"," 332,\n"," 216,\n"," 910,\n"," 70,\n"," 994,\n"," 388,\n"," 763,\n"," 557,\n"," 40,\n"," 936,\n"," 602,\n"," 830,\n"," 146,\n"," 971,\n"," 149,\n"," 571,\n"," 738,\n"," 0,\n"," 986,\n"," 679,\n"," 829,\n"," 564,\n"," 4,\n"," 643,\n"," 100,\n"," 496,\n"," 808]\n","print(\"length of train_ind: \", len(train_ind))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["length of train_ind:  250\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2hQmW3xME5vO","colab_type":"code","cellView":"form","outputId":"d3f4c984-b75c-4a3f-ad1e-02f132856765","executionInfo":{"status":"ok","timestamp":1569157588356,"user_tz":-480,"elapsed":3149,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@title\n","test_ind = [88,\n"," 810,\n"," 747,\n"," 824,\n"," 675,\n"," 742,\n"," 419,\n"," 57,\n"," 908,\n"," 236,\n"," 698,\n"," 692,\n"," 619,\n"," 355,\n"," 539,\n"," 285,\n"," 638,\n"," 629,\n"," 186,\n"," 756,\n"," 665,\n"," 720,\n"," 478,\n"," 282,\n"," 448,\n"," 54,\n"," 777,\n"," 834,\n"," 17,\n"," 220,\n"," 195,\n"," 576,\n"," 764,\n"," 811,\n"," 849,\n"," 970,\n"," 678,\n"," 694,\n"," 9,\n"," 709,\n"," 871,\n"," 26,\n"," 845,\n"," 428,\n"," 115,\n"," 985,\n"," 932,\n"," 982,\n"," 795,\n"," 684,\n"," 621,\n"," 87,\n"," 327,\n"," 759,\n"," 35,\n"," 67,\n"," 716,\n"," 211,\n"," 524,\n"," 526,\n"," 723,\n"," 19,\n"," 509,\n"," 96,\n"," 284,\n"," 192,\n"," 494,\n"," 520,\n"," 143,\n"," 989,\n"," 226,\n"," 94,\n"," 277,\n"," 102,\n"," 578,\n"," 475,\n"," 503,\n"," 993,\n"," 15,\n"," 874,\n"," 12,\n"," 685,\n"," 906,\n"," 996,\n"," 199,\n"," 519,\n"," 47,\n"," 722,\n"," 482,\n"," 196,\n"," 449,\n"," 620,\n"," 132,\n"," 560,\n"," 98,\n"," 319,\n"," 835,\n"," 789,\n"," 178,\n"," 688,\n"," 134,\n"," 155,\n"," 210,\n"," 224,\n"," 86,\n"," 652,\n"," 78,\n"," 973,\n"," 484,\n"," 687,\n"," 185,\n"," 39,\n"," 99,\n"," 283,\n"," 726,\n"," 289,\n"," 180,\n"," 403,\n"," 745,\n"," 116,\n"," 274,\n"," 408,\n"," 318,\n"," 776,\n"," 909,\n"," 181,\n"," 373,\n"," 43,\n"," 280,\n"," 350,\n"," 109,\n"," 154,\n"," 531,\n"," 600,\n"," 838,\n"," 599,\n"," 297,\n"," 95,\n"," 966,\n"," 607,\n"," 704,\n"," 290,\n"," 382,\n"," 707,\n"," 259,\n"," 16,\n"," 330,\n"," 212,\n"," 734,\n"," 633,\n"," 632,\n"," 72,\n"," 598,\n"," 173,\n"," 755,\n"," 151,\n"," 582,\n"," 240,\n"," 473,\n"," 227,\n"," 202,\n"," 207,\n"," 814,\n"," 169,\n"," 890,\n"," 612,\n"," 363,\n"," 803,\n"," 510,\n"," 10,\n"," 378,\n"," 406,\n"," 843,\n"," 567,\n"," 304,\n"," 307,\n"," 772,\n"," 562,\n"," 594,\n"," 292,\n"," 120,\n"," 348,\n"," 854,\n"," 20,\n"," 213,\n"," 943,\n"," 859,\n"," 437,\n"," 287,\n"," 349,\n"," 914,\n"," 584,\n"," 147,\n"," 322,\n"," 342,\n"," 129,\n"," 603,\n"," 6,\n"," 421,\n"," 492,\n"," 735,\n"," 454,\n"," 281,\n"," 796,\n"," 942,\n"," 894,\n"," 781,\n"," 445,\n"," 201,\n"," 405,\n"," 593,\n"," 680,\n"," 395,\n"," 465,\n"," 301,\n"," 641,\n"," 891,\n"," 724,\n"," 398,\n"," 85,\n"," 945,\n"," 897,\n"," 535,\n"," 118,\n"," 951,\n"," 38,\n"," 275,\n"," 987,\n"," 130,\n"," 452,\n"," 174,\n"," 152,\n"," 545,\n"," 569,\n"," 480,\n"," 880,\n"," 64,\n"," 360,\n"," 217,\n"," 234,\n"," 375,\n"," 860,\n"," 5,\n"," 844,\n"," 423,\n"," 474,\n"," 204,\n"," 627,\n"," 980,\n"," 254,\n"," 676,\n"," 761,\n"," 76,\n"," 261,\n"," 33,\n"," 490,\n"," 655,\n"," 958,\n"," 775,\n"," 551,\n"," 710,\n"," 955,\n"," 409,\n"," 13,\n"," 949,\n"," 552,\n"," 11,\n"," 931,\n"," 370,\n"," 719,\n"," 105,\n"," 960,\n"," 114,\n"," 108,\n"," 352,\n"," 471,\n"," 876,\n"," 721,\n"," 390,\n"," 160,\n"," 889,\n"," 22,\n"," 821,\n"," 351,\n"," 298,\n"," 459,\n"," 957,\n"," 42,\n"," 686,\n"," 29,\n"," 767,\n"," 399,\n"," 141,\n"," 7,\n"," 75,\n"," 572,\n"," 364,\n"," 855,\n"," 52,\n"," 512,\n"," 647,\n"," 792,\n"," 832,\n"," 770,\n"," 555,\n"," 273,\n"," 522,\n"," 608,\n"," 491,\n"," 916,\n"," 999,\n"," 339,\n"," 311,\n"," 898,\n"," 528,\n"," 82,\n"," 737,\n"," 868,\n"," 956,\n"," 671,\n"," 80,\n"," 53,\n"," 28,\n"," 150,\n"," 902,\n"," 176,\n"," 340,\n"," 27,\n"," 809,\n"," 736,\n"," 183,\n"," 353,\n"," 468,\n"," 730,\n"," 544,\n"," 36,\n"," 921,\n"," 883,\n"," 802,\n"," 801,\n"," 750,\n"," 253,\n"," 581,\n"," 604,\n"," 425,\n"," 341,\n"," 223,\n"," 667,\n"," 690,\n"," 548,\n"," 697,\n"," 172,\n"," 538,\n"," 812,\n"," 729,\n"," 358,\n"," 766,\n"," 785,\n"," 953,\n"," 123,\n"," 865,\n"," 896,\n"," 326,\n"," 631,\n"," 2,\n"," 798,\n"," 97,\n"," 708,\n"," 935,\n"," 646,\n"," 477,\n"," 439,\n"," 170,\n"," 727,\n"," 469,\n"," 291,\n"," 559,\n"," 972,\n"," 187,\n"," 645,\n"," 534,\n"," 856,\n"," 455,\n"," 630,\n"," 864,\n"," 640,\n"," 689,\n"," 279,\n"," 642,\n"," 887,\n"," 171,\n"," 397,\n"," 899,\n"," 71,\n"," 661,\n"," 426,\n"," 489,\n"," 81,\n"," 624,\n"," 131,\n"," 944,\n"," 923,\n"," 310,\n"," 882,\n"," 816,\n"," 106,\n"," 229,\n"," 610,\n"," 460,\n"," 456,\n"," 412,\n"," 266,\n"," 276,\n"," 674,\n"," 246,\n"," 299,\n"," 103,\n"," 818,\n"," 765,\n"," 31,\n"," 410,\n"," 407,\n"," 673,\n"," 529,\n"," 422,\n"," 563,\n"," 110,\n"," 200,\n"," 639,\n"," 119,\n"," 702,\n"," 666,\n"," 615,\n"," 313,\n"," 662,\n"," 73,\n"," 616,\n"," 974,\n"," 189,\n"," 625,\n"," 190,\n"," 249,\n"," 244,\n"," 664,\n"," 706,\n"," 443,\n"," 142,\n"," 904,\n"," 427,\n"," 296,\n"," 417,\n"," 933,\n"," 762,\n"," 929,\n"," 540,\n"," 361,\n"," 404,\n"," 743,\n"," 238,\n"," 733,\n"," 438,\n"," 354,\n"," 230,\n"," 797,\n"," 91,\n"," 335,\n"," 321,\n"," 961,\n"," 601,\n"," 900,\n"," 288,\n"," 402,\n"," 924,\n"," 51,\n"," 863,\n"," 925,\n"," 320,\n"," 514,\n"," 740,\n"," 222,\n"," 794,\n"," 754,\n"," 218,\n"," 260,\n"," 24,\n"," 965,\n"," 822,\n"," 393,\n"," 895,\n"," 873,\n"," 434,\n"," 488,\n"," 343,\n"," 357,\n"," 752,\n"," 493,\n"," 711,\n"," 268,\n"," 533,\n"," 773,\n"," 976,\n"," 946,\n"," 546,\n"," 554,\n"," 869,\n"," 243,\n"," 203,\n"," 905,\n"," 411,\n"," 846,\n"," 885,\n"," 317,\n"," 366,\n"," 693,\n"," 328,\n"," 495,\n"," 502,\n"," 345,\n"," 392,\n"," 228,\n"," 700,\n"," 967,\n"," 124,\n"," 65,\n"," 163,\n"," 836,\n"," 850,\n"," 214,\n"," 336,\n"," 901,\n"," 872,\n"," 48,\n"," 634,\n"," 62,\n"," 486,\n"," 383,\n"," 590,\n"," 653,\n"," 819,\n"," 788,\n"," 461,\n"," 344,\n"," 682,\n"," 857,\n"," 511,\n"," 432,\n"," 561,\n"," 853,\n"," 911,\n"," 416,\n"," 248,\n"," 527,\n"," 401,\n"," 879,\n"," 585,\n"," 523,\n"," 359,\n"," 139,\n"," 481,\n"," 331,\n"," 912,\n"," 384,\n"," 938,\n"," 614,\n"," 806,\n"," 828,\n"," 138,\n"," 881,\n"," 969,\n"," 429,\n"," 699,\n"," 107,\n"," 566,\n"," 414,\n"," 162,\n"," 617,\n"," 893,\n"," 263,\n"," 915,\n"," 441,\n"," 424,\n"," 515,\n"," 499,\n"," 215,\n"," 628,\n"," 884,\n"," 323,\n"," 14,\n"," 842,\n"," 68,\n"," 862,\n"," 188,\n"," 286,\n"," 504,\n"," 505,\n"," 769,\n"," 193,\n"," 553,\n"," 587,\n"," 580,\n"," 550,\n"," 101,\n"," 940,\n"," 232,\n"," 61,\n"," 430,\n"," 159,\n"," 517,\n"," 219,\n"," 817,\n"," 549,\n"," 565,\n"," 21,\n"," 748,\n"," 79,\n"," 113,\n"," 833,\n"," 472,\n"," 660,\n"," 367,\n"," 128,\n"," 1,\n"," 623,\n"," 813,\n"," 979,\n"," 543,\n"," 668,\n"," 611,\n"," 329,\n"," 605,\n"," 58,\n"," 760,\n"," 362,\n"," 820,\n"," 918,\n"," 807,\n"," 90,\n"," 237,\n"," 49,\n"," 758,\n"," 8,\n"," 258,\n"," 878,\n"," 805,\n"," 442,\n"," 825,\n"," 948,\n"," 267,\n"," 60,\n"," 63,\n"," 394,\n"," 746,\n"," 140,\n"," 25,\n"," 41,\n"," 242,\n"," 257,\n"," 962,\n"," 372,\n"," 920,\n"," 579,\n"," 205,\n"," 978,\n"," 597,\n"," 659,\n"," 371,\n"," 934,\n"," 66,\n"," 381,\n"," 952,\n"," 568,\n"," 744,\n"," 847,\n"," 790,\n"," 981,\n"," 450,\n"," 470,\n"," 663,\n"," 387,\n"," 530,\n"," 264,\n"," 137,\n"," 589,\n"," 596,\n"," 32,\n"," 749,\n"," 112,\n"," 848,\n"," 158,\n"," 479,\n"," 431,\n"," 995,\n"," 595,\n"," 77,\n"," 433,\n"," 731,\n"," 712,\n"," 295,\n"," 919,\n"," 231,\n"," 926,\n"," 542,\n"," 592,\n"," 714,\n"," 59,\n"," 44,\n"," 778,\n"," 458,\n"," 516,\n"," 635,\n"," 780,\n"," 823,\n"," 312,\n"," 927,\n"," 903,\n"," 379,\n"," 786,\n"," 537,\n"," 133,\n"," 293,\n"," 741,\n"," 558,\n"," 508,\n"," 947,\n"," 784,\n"," 198,\n"," 636,\n"," 725,\n"," 941,\n"," 241,\n"," 977,\n"," 588,\n"," 476,\n"," 385,\n"," 751,\n"," 324,\n"," 541,\n"," 334,\n"," 998,\n"," 308,\n"," 337,\n"," 715,\n"," 753,\n"," 968,\n"," 221,\n"," 791,\n"," 963,\n"," 197,\n"," 451,\n"," 827]\n","\n","print(\"length of test_ind\", len(test_ind))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["length of test_ind 750\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r8vQeAbWE5xz","colab_type":"code","colab":{}},"source":["y = movies_clean['sentiment']\n","X = movies_clean['review']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uGnGfviBFcVl","colab_type":"code","colab":{}},"source":["X_train = X.iloc[train_ind]\n","X_test = X.iloc[test_ind]\n","y_train = y.iloc[train_ind]\n","y_test = y.iloc[test_ind]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EE0qXuyqFr1Y","colab_type":"code","outputId":"87617c89-5827-4272-d319-097fe1f15d6c","executionInfo":{"status":"ok","timestamp":1569157862016,"user_tz":-480,"elapsed":644,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# sanity check\n","X_train.tail()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4      it ll soon be 10 yrs since this movie was rele...\n","643     tip have it read to you heres how 1 copy and ...\n","100    killjoy 2 is the same as killjoy 1 bad acting ...\n","496    ok this movie was the worst display i have see...\n","808    heather graham is not just a pretty face she i...\n","Name: review, dtype: object"]},"metadata":{"tags":[]},"execution_count":179}]},{"cell_type":"code","metadata":{"id":"ZJ9QbY89FxCl","colab_type":"code","outputId":"b0759d0b-3e24-4086-c56f-7a4d2e43cf05","executionInfo":{"status":"ok","timestamp":1569157864709,"user_tz":-480,"elapsed":734,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["y_test.tail()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["791    1\n","963    0\n","197    0\n","451    1\n","827    0\n","Name: sentiment, dtype: int64"]},"metadata":{"tags":[]},"execution_count":180}]},{"cell_type":"markdown","metadata":{"id":"RdFHWCx2F5gy","colab_type":"text"},"source":["In this exercise, you have been given two pandas Series, X_train and X_test, which consist of movie reviews. They represent the training and the test review data respectively. "]},{"cell_type":"code","metadata":{"id":"xGlm9OgIF6pS","colab_type":"code","outputId":"78654d6f-51a9-4ddd-ef34-6ef4e0d0cbed","executionInfo":{"status":"ok","timestamp":1569157868059,"user_tz":-480,"elapsed":700,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Import CountVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Create a CountVectorizer object\n","vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n","\n","# Fit and transform X_train\n","X_train_bow = vectorizer.fit_transform(X_train)\n","\n","# Transform X_test\n","X_test_bow = vectorizer.transform(X_test)\n","\n","# Print shape of X_train_bow and X_test_bow\n","print(X_train_bow.shape)\n","print(X_test_bow.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(250, 8158)\n","(750, 8158)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cgnp7FZ3HeKi","colab_type":"text"},"source":["####Predicting the sentiment of a movie review\n","\n","In this exercise, we will use this model to train a Naive Bayes classifier that can detect the sentiment of a movie review and compute its accuracy. Note that since this is a binary classification problem, the model is only capable of classifying a review as either positive (1) or negative (0). It is incapable of detecting neutral reviews."]},{"cell_type":"code","metadata":{"id":"HVedLMSMHs6A","colab_type":"code","colab":{}},"source":["from sklearn.naive_bayes import MultinomialNB"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rDOizRX6Hnoa","colab_type":"code","outputId":"1dd5f362-40d5-41d9-c41c-8df0bd78e734","executionInfo":{"status":"ok","timestamp":1569158256976,"user_tz":-480,"elapsed":713,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Create a MultinomialNB object\n","clf = MultinomialNB()\n","\n","# Fit the classifier\n","clf.fit(X_train_bow, y_train)\n","\n","# Measure the accuracy\n","accuracy = clf.score(X_test_bow, y_test)\n","print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n","\n","# Predict the sentiment of a negative review\n","review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n","prediction = clf.predict(vectorizer.transform([review]))[0]\n","print(\"The sentiment predicted by the classifier is %i\" % (prediction))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The accuracy of the classifier on the test set is 0.732\n","The sentiment predicted by the classifier is 0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kcVMzg4CH2PY","colab_type":"text"},"source":["###Building n-gram models\n"]},{"cell_type":"markdown","metadata":{"id":"fe5f-UDwH5do","colab_type":"text"},"source":["####n-gram models for movie tag lines\n","In this exercise, we have been provided with a corpus of more than 9000 movie tag lines. Our job is to generate n-gram models up to n equal to 1, n equal to 2 and n equal to 3 for this data and discover the number of features for each model."]},{"cell_type":"code","metadata":{"id":"Z4d3ZWOkIRce","colab_type":"code","outputId":"45a88a25-e991-4be8-9a61-39af48223c33","executionInfo":{"status":"ok","timestamp":1569158502810,"user_tz":-480,"elapsed":729,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["movies = pd.read_csv(\"movie_overviews.csv\")\n","movies.tail()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>overview</th>\n","      <th>tagline</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9094</th>\n","      <td>159550</td>\n","      <td>The Last Brickmaker in America</td>\n","      <td>A man must cope with the loss of his wife and ...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>9095</th>\n","      <td>392572</td>\n","      <td>Rustom</td>\n","      <td>Rustom Pavri, an honourable officer of the Ind...</td>\n","      <td>Decorated Officer. Devoted Family Man. Defendi...</td>\n","    </tr>\n","    <tr>\n","      <th>9096</th>\n","      <td>402672</td>\n","      <td>Mohenjo Daro</td>\n","      <td>Village lad Sarman is drawn to big, bad Mohenj...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>9097</th>\n","      <td>315011</td>\n","      <td>Shin Godzilla</td>\n","      <td>From the mind behind Evangelion comes a hit la...</td>\n","      <td>A god incarnate. A city doomed.</td>\n","    </tr>\n","    <tr>\n","      <th>9098</th>\n","      <td>391698</td>\n","      <td>The Beatles: Eight Days a Week - The Touring Y...</td>\n","      <td>The band stormed Europe in 1963, and, in 1964,...</td>\n","      <td>The band you know. The story you don't.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          id  ...                                            tagline\n","9094  159550  ...                                                NaN\n","9095  392572  ...  Decorated Officer. Devoted Family Man. Defendi...\n","9096  402672  ...                                                NaN\n","9097  315011  ...                    A god incarnate. A city doomed.\n","9098  391698  ...            The band you know. The story you don't.\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":195}]},{"cell_type":"code","metadata":{"id":"-_Xr4obCID92","colab_type":"code","outputId":"9744ec72-37a6-4898-cfae-5d54c278c032","executionInfo":{"status":"ok","timestamp":1569158509972,"user_tz":-480,"elapsed":729,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["df = movies.replace(np.nan, '', regex=True)\n","df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>overview</th>\n","      <th>tagline</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>862</td>\n","      <td>Toy Story</td>\n","      <td>Led by Woody, Andy's toys live happily in his ...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>8844</td>\n","      <td>Jumanji</td>\n","      <td>When siblings Judy and Peter discover an encha...</td>\n","      <td>Roll the dice and unleash the excitement!</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>15602</td>\n","      <td>Grumpier Old Men</td>\n","      <td>A family wedding reignites the ancient feud be...</td>\n","      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>31357</td>\n","      <td>Waiting to Exhale</td>\n","      <td>Cheated on, mistreated and stepped on, the wom...</td>\n","      <td>Friends are the people who let you be yourself...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11862</td>\n","      <td>Father of the Bride Part II</td>\n","      <td>Just when George Banks has recovered from his ...</td>\n","      <td>Just When His World Is Back To Normal... He's ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id  ...                                            tagline\n","0    862  ...                                                   \n","1   8844  ...          Roll the dice and unleash the excitement!\n","2  15602  ...  Still Yelling. Still Fighting. Still Ready for...\n","3  31357  ...  Friends are the people who let you be yourself...\n","4  11862  ...  Just When His World Is Back To Normal... He's ...\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":196}]},{"cell_type":"code","metadata":{"id":"yGSUzUVIIV3T","colab_type":"code","outputId":"8199ef6a-388f-45a9-beb1-d84b60fffc8d","executionInfo":{"status":"ok","timestamp":1569158523740,"user_tz":-480,"elapsed":707,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["corpus = df['tagline']\n","corpus"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0                                                        \n","1               Roll the dice and unleash the excitement!\n","2       Still Yelling. Still Fighting. Still Ready for...\n","3       Friends are the people who let you be yourself...\n","4       Just When His World Is Back To Normal... He's ...\n","5                                A Los Angeles Crime Saga\n","6       You are cordially invited to the most surprisi...\n","7                                  The Original Bad Boys.\n","8                              Terror goes into overtime.\n","9                    No limits. No fears. No substitutes.\n","10      Why can't the most powerful man in the world h...\n","11                                                       \n","12                         Part Dog. Part Wolf. All Hero.\n","13      Triumphant in Victory, Bitter in Defeat. He Ch...\n","14      The Course Has Been Set. There Is No Turning B...\n","15                       No one stays at the top forever.\n","16               Lose your heart and come to your senses.\n","17      Twelve outrageous guests. Four scandalous requ...\n","18                New animals. New adventures. Same hair.\n","19                            Get on, or GET OUT THE WAY!\n","20      The mob is tough, but it’s nothing like show b...\n","21      One man is copying the most notorious killers ...\n","22      In the shadows of life, In the business of dea...\n","23      An extraordinary encounter with another human ...\n","24                         I Love You... The Way You Are.\n","25                        Envy, greed, jealousy and love.\n","26      In every woman there is the girl she left behind.\n","27                                                       \n","28              Where happily ever after is just a dream.\n","29      In 1930's Shanghai violence was not the proble...\n","                              ...                        \n","9069                          You can't fight the future.\n","9070                        You Haven't Seen Anything Yet\n","9071           Look what's living inside your television!\n","9072                                    Raise some shell.\n","9073                                                     \n","9074    The next true story from the case files of Ed ...\n","9075                                                     \n","9076                                  Who You Gonna Call?\n","9077    Saving the world takes a little Hart and a big...\n","9078                                    You know his name\n","9079          How far will a family go to bury the truth?\n","9080                                       Human. Nature.\n","9081                                   Keep America great\n","9082          They needed hot dates. They got hot messes.\n","9083                                                     \n","9084                           Life  can be a real mother\n","9085                                                     \n","9086                       Are you a watcher or a player?\n","9087                                                     \n","9088                                                     \n","9089                                                     \n","9090                          Blood always follows money.\n","9091                        Kingsglaive: Final Fantasy XV\n","9092                                                     \n","9093    What happens in Vegas, stays in Vegas. Unless ...\n","9094                                                     \n","9095    Decorated Officer. Devoted Family Man. Defendi...\n","9096                                                     \n","9097                      A god incarnate. A city doomed.\n","9098              The band you know. The story you don't.\n","Name: tagline, Length: 9099, dtype: object"]},"metadata":{"tags":[]},"execution_count":197}]},{"cell_type":"markdown","metadata":{"id":"X4d77aE6IyfY","colab_type":"text"},"source":["In this exercise, we have been provided with a corpus of more than 9000 movie tag lines.\n","\n","- Generate an n-gram model with n-grams up to n=1. Name it ng1\n","- Generate an n-gram model with n-grams up to n=2. Name it ng2\n","- Generate an n-Gram Model with n-grams up to n=3. Name it ng3\n","- Print the number of features for each model."]},{"cell_type":"code","metadata":{"id":"amOMgHhVH8Kv","colab_type":"code","outputId":"b1287729-d6fe-4dd2-cf8c-b35f51b8fe0b","executionInfo":{"status":"ok","timestamp":1569158545076,"user_tz":-480,"elapsed":1472,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Generate n-grams upto n=1\n","vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n","ng1 = vectorizer_ng1.fit_transform(corpus)\n","\n","# Generate n-grams upto n=2\n","vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n","ng2 = vectorizer_ng2.fit_transform(corpus)\n","\n","# Generate n-grams upto n=3\n","vectorizer_ng3 = CountVectorizer(ngram_range=(1,3))\n","ng3 = vectorizer_ng3.fit_transform(corpus)\n","\n","# Print the number of features for each model\n","print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["ng1, ng2 and ng3 have 6614, 37100 and 76881 features respectively\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CGLNdmbOI7k9","colab_type":"text"},"source":["####Higher order n-grams for sentiment analysis\n","\n","Similar to a previous exercise, we are going to build a classifier that can detect if the review of a particular movie is positive or negative. However, this time, we will use n-grams up to n=2 for the task."]},{"cell_type":"code","metadata":{"id":"vXmWcBVcJbfh","colab_type":"code","cellView":"form","colab":{}},"source":["#@title\n","ind_train = [575,\n"," 964,\n"," 446,\n"," 54,\n"," 33,\n"," 683,\n"," 685,\n"," 95,\n"," 625,\n"," 407,\n"," 917,\n"," 247,\n"," 779,\n"," 875,\n"," 453,\n"," 720,\n"," 34,\n"," 557,\n"," 733,\n"," 916,\n"," 821,\n"," 308,\n"," 81,\n"," 677,\n"," 960,\n"," 262,\n"," 332,\n"," 391,\n"," 841,\n"," 142,\n"," 166,\n"," 713,\n"," 50,\n"," 224,\n"," 45,\n"," 104,\n"," 479,\n"," 516,\n"," 120,\n"," 346,\n"," 132,\n"," 812,\n"," 177,\n"," 798,\n"," 368,\n"," 804,\n"," 515,\n"," 233,\n"," 4,\n"," 640,\n"," 670,\n"," 355,\n"," 616,\n"," 100,\n"," 268,\n"," 793,\n"," 362,\n"," 943,\n"," 861,\n"," 780,\n"," 395,\n"," 536,\n"," 939,\n"," 155,\n"," 256,\n"," 167,\n"," 735,\n"," 122,\n"," 882,\n"," 204,\n"," 396,\n"," 763,\n"," 337,\n"," 223,\n"," 808,\n"," 629,\n"," 975,\n"," 990,\n"," 111,\n"," 478,\n"," 932,\n"," 154,\n"," 695,\n"," 84,\n"," 357,\n"," 570,\n"," 457,\n"," 800,\n"," 959,\n"," 696,\n"," 182,\n"," 892,\n"," 125,\n"," 513,\n"," 907,\n"," 60,\n"," 311,\n"," 877,\n"," 854,\n"," 864,\n"," 926,\n"," 514,\n"," 778,\n"," 321,\n"," 983,\n"," 740,\n"," 68,\n"," 29,\n"," 701,\n"," 886,\n"," 25,\n"," 618,\n"," 301,\n"," 467,\n"," 971,\n"," 348,\n"," 558,\n"," 498,\n"," 351,\n"," 930,\n"," 601,\n"," 74,\n"," 381,\n"," 718,\n"," 18,\n"," 463,\n"," 816,\n"," 418,\n"," 37,\n"," 954,\n"," 609,\n"," 997,\n"," 915,\n"," 271,\n"," 532,\n"," 884,\n"," 242,\n"," 380,\n"," 117,\n"," 377,\n"," 782,\n"," 40,\n"," 838,\n"," 764,\n"," 42,\n"," 552,\n"," 799,\n"," 815,\n"," 240,\n"," 156,\n"," 669,\n"," 286,\n"," 316,\n"," 984,\n"," 564,\n"," 215,\n"," 135,\n"," 144,\n"," 5,\n"," 294,\n"," 245,\n"," 646,\n"," 436,\n"," 429,\n"," 936,\n"," 315,\n"," 728,\n"," 483,\n"," 655,\n"," 888,\n"," 270,\n"," 57,\n"," 251,\n"," 830,\n"," 317,\n"," 962,\n"," 110,\n"," 239,\n"," 300,\n"," 573,\n"," 162,\n"," 466,\n"," 174,\n"," 153,\n"," 175,\n"," 172,\n"," 672,\n"," 989,\n"," 755,\n"," 858,\n"," 52,\n"," 649,\n"," 22,\n"," 501,\n"," 296,\n"," 591,\n"," 768,\n"," 379,\n"," 190,\n"," 313,\n"," 151,\n"," 451,\n"," 976,\n"," 338,\n"," 161,\n"," 89,\n"," 131,\n"," 512,\n"," 839,\n"," 705,\n"," 94,\n"," 130,\n"," 742,\n"," 806,\n"," 977,\n"," 973,\n"," 79,\n"," 831,\n"," 691,\n"," 743,\n"," 988,\n"," 136,\n"," 499,\n"," 303,\n"," 165,\n"," 774,\n"," 739,\n"," 285,\n"," 911,\n"," 803,\n"," 121,\n"," 654,\n"," 386,\n"," 214,\n"," 314,\n"," 645,\n"," 309,\n"," 928,\n"," 783,\n"," 447,\n"," 897,\n"," 72,\n"," 46,\n"," 440,\n"," 206,\n"," 250,\n"," 257,\n"," 327,\n"," 211,\n"," 369,\n"," 948,\n"," 567,\n"," 55,\n"," 163,\n"," 492,\n"," 213,\n"," 565,\n"," 234,\n"," 716,\n"," 873,\n"," 69,\n"," 643,\n"," 197,\n"," 459,\n"," 681,\n"," 523,\n"," 534,\n"," 9,\n"," 422,\n"," 656,\n"," 703,\n"," 445,\n"," 325,\n"," 93,\n"," 586,\n"," 347,\n"," 159,\n"," 840,\n"," 730,\n"," 225,\n"," 862,\n"," 650,\n"," 762,\n"," 390,\n"," 604,\n"," 751,\n"," 208,\n"," 237,\n"," 596,\n"," 752,\n"," 602,\n"," 434,\n"," 992,\n"," 389,\n"," 400,\n"," 644,\n"," 384,\n"," 157,\n"," 32,\n"," 760,\n"," 65,\n"," 41,\n"," 518,\n"," 176,\n"," 587,\n"," 545,\n"," 415,\n"," 554,\n"," 419,\n"," 272,\n"," 150,\n"," 265,\n"," 851,\n"," 794,\n"," 614,\n"," 866,\n"," 686,\n"," 209,\n"," 148,\n"," 168,\n"," 950,\n"," 126,\n"," 843,\n"," 969,\n"," 947,\n"," 101,\n"," 80,\n"," 196,\n"," 255,\n"," 749,\n"," 731,\n"," 689,\n"," 139,\n"," 474,\n"," 991,\n"," 83,\n"," 353,\n"," 746,\n"," 198,\n"," 595,\n"," 507,\n"," 878,\n"," 30,\n"," 468,\n"," 59,\n"," 506,\n"," 732,\n"," 56,\n"," 435,\n"," 909,\n"," 937,\n"," 217,\n"," 410,\n"," 982,\n"," 852,\n"," 14,\n"," 44,\n"," 279,\n"," 195,\n"," 128,\n"," 254,\n"," 889,\n"," 568,\n"," 723,\n"," 890,\n"," 70,\n"," 825,\n"," 464,\n"," 785,\n"," 409,\n"," 413,\n"," 648,\n"," 891,\n"," 725,\n"," 787,\n"," 191,\n"," 520,\n"," 365,\n"," 539,\n"," 184,\n"," 985,\n"," 694,\n"," 227,\n"," 252,\n"," 6,\n"," 278,\n"," 757,\n"," 392,\n"," 795,\n"," 913,\n"," 563,\n"," 193,\n"," 444,\n"," 370,\n"," 476,\n"," 420,\n"," 116,\n"," 529,\n"," 98,\n"," 963,\n"," 657,\n"," 574,\n"," 376,\n"," 485,\n"," 293,\n"," 577,\n"," 179,\n"," 651,\n"," 867,\n"," 953,\n"," 560,\n"," 432,\n"," 328,\n"," 194,\n"," 684,\n"," 637,\n"," 231,\n"," 547,\n"," 714,\n"," 145,\n"," 31,\n"," 556,\n"," 216,\n"," 887,\n"," 635,\n"," 487,\n"," 388,\n"," 525,\n"," 583,\n"," 582,\n"," 462,\n"," 149,\n"," 333,\n"," 717,\n"," 421,\n"," 164,\n"," 21,\n"," 183,\n"," 837,\n"," 857,\n"," 3,\n"," 622,\n"," 823,\n"," 414,\n"," 393,\n"," 771,\n"," 497,\n"," 578,\n"," 704,\n"," 482,\n"," 127,\n"," 500,\n"," 613,\n"," 870,\n"," 658,\n"," 765,\n"," 880,\n"," 814,\n"," 925,\n"," 603,\n"," 986,\n"," 302,\n"," 23,\n"," 521,\n"," 269,\n"," 922,\n"," 374,\n"," 666,\n"," 26,\n"," 826,\n"," 112,\n"," 331,\n"," 53,\n"," 709,\n"," 626,\n"," 910,\n"," 387,\n"," 727,\n"," 226,\n"," 994,\n"," 306,\n"," 92,\n"," 43,\n"," 972,\n"," 235,\n"," 146,\n"," 606,\n"," 442,\n"," 571,\n"," 738,\n"," 0,\n"," 551,\n"," 679,\n"," 829,\n"," 283,\n"," 241,\n"," 356,\n"," 305,\n"," 496,\n"," 667]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RAChhotVJiR5","colab_type":"code","cellView":"form","colab":{}},"source":["#@title\n","ind_test = [103,\n"," 872,\n"," 846,\n"," 76,\n"," 610,\n"," 608,\n"," 359,\n"," 192,\n"," 24,\n"," 807,\n"," 543,\n"," 966,\n"," 495,\n"," 344,\n"," 385,\n"," 900,\n"," 871,\n"," 598,\n"," 260,\n"," 542,\n"," 51,\n"," 941,\n"," 71,\n"," 405,\n"," 599,\n"," 178,\n"," 115,\n"," 698,\n"," 509,\n"," 273,\n"," 403,\n"," 581,\n"," 449,\n"," 946,\n"," 38,\n"," 597,\n"," 284,\n"," 956,\n"," 818,\n"," 850,\n"," 107,\n"," 736,\n"," 585,\n"," 664,\n"," 281,\n"," 220,\n"," 140,\n"," 753,\n"," 123,\n"," 863,\n"," 531,\n"," 559,\n"," 253,\n"," 134,\n"," 364,\n"," 49,\n"," 933,\n"," 860,\n"," 366,\n"," 668,\n"," 899,\n"," 266,\n"," 642,\n"," 906,\n"," 805,\n"," 634,\n"," 291,\n"," 171,\n"," 904,\n"," 770,\n"," 417,\n"," 661,\n"," 777,\n"," 856,\n"," 203,\n"," 1,\n"," 538,\n"," 299,\n"," 811,\n"," 277,\n"," 827,\n"," 448,\n"," 647,\n"," 186,\n"," 707,\n"," 290,\n"," 978,\n"," 576,\n"," 201,\n"," 949,\n"," 99,\n"," 339,\n"," 489,\n"," 631,\n"," 958,\n"," 526,\n"," 221,\n"," 675,\n"," 494,\n"," 834,\n"," 540,\n"," 820,\n"," 229,\n"," 938,\n"," 39,\n"," 828,\n"," 470,\n"," 519,\n"," 456,\n"," 772,\n"," 970,\n"," 524,\n"," 719,\n"," 319,\n"," 412,\n"," 480,\n"," 699,\n"," 832,\n"," 775,\n"," 61,\n"," 537,\n"," 980,\n"," 690,\n"," 402,\n"," 207,\n"," 918,\n"," 129,\n"," 590,\n"," 993,\n"," 263,\n"," 955,\n"," 20,\n"," 605,\n"," 28,\n"," 441,\n"," 607,\n"," 998,\n"," 680,\n"," 287,\n"," 967,\n"," 781,\n"," 310,\n"," 64,\n"," 579,\n"," 724,\n"," 158,\n"," 600,\n"," 137,\n"," 833,\n"," 143,\n"," 594,\n"," 935,\n"," 205,\n"," 382,\n"," 372,\n"," 855,\n"," 312,\n"," 358,\n"," 341,\n"," 118,\n"," 510,\n"," 324,\n"," 354,\n"," 692,\n"," 934,\n"," 555,\n"," 160,\n"," 267,\n"," 276,\n"," 715,\n"," 589,\n"," 848,\n"," 533,\n"," 48,\n"," 619,\n"," 173,\n"," 180,\n"," 859,\n"," 371,\n"," 503,\n"," 894,\n"," 641,\n"," 455,\n"," 968,\n"," 653,\n"," 706,\n"," 232,\n"," 802,\n"," 636,\n"," 974,\n"," 796,\n"," 292,\n"," 885,\n"," 522,\n"," 329,\n"," 919,\n"," 202,\n"," 481,\n"," 623,\n"," 326,\n"," 810,\n"," 304,\n"," 36,\n"," 90,\n"," 940,\n"," 633,\n"," 363,\n"," 931,\n"," 945,\n"," 264,\n"," 274,\n"," 756,\n"," 460,\n"," 842,\n"," 307,\n"," 334,\n"," 530,\n"," 404,\n"," 96,\n"," 394,\n"," 569,\n"," 674,\n"," 951,\n"," 345,\n"," 893,\n"," 335,\n"," 788,\n"," 230,\n"," 397,\n"," 66,\n"," 995,\n"," 169,\n"," 275,\n"," 471,\n"," 73,\n"," 138,\n"," 621,\n"," 874,\n"," 430,\n"," 7,\n"," 562,\n"," 693,\n"," 511,\n"," 729,\n"," 996,\n"," 535,\n"," 711,\n"," 212,\n"," 105,\n"," 726,\n"," 712,\n"," 822,\n"," 486,\n"," 741,\n"," 548,\n"," 375,\n"," 399,\n"," 662,\n"," 847,\n"," 12,\n"," 721,\n"," 813,\n"," 320,\n"," 817,\n"," 106,\n"," 849,\n"," 490,\n"," 754,\n"," 238,\n"," 433,\n"," 473,\n"," 88,\n"," 912,\n"," 792,\n"," 924,\n"," 999,\n"," 987,\n"," 957,\n"," 63,\n"," 896,\n"," 243,\n"," 17,\n"," 249,\n"," 929,\n"," 458,\n"," 318,\n"," 406,\n"," 895,\n"," 734,\n"," 452,\n"," 678,\n"," 383,\n"," 747,\n"," 124,\n"," 295,\n"," 248,\n"," 181,\n"," 378,\n"," 687,\n"," 35,\n"," 78,\n"," 141,\n"," 77,\n"," 517,\n"," 620,\n"," 881,\n"," 790,\n"," 759,\n"," 27,\n"," 297,\n"," 508,\n"," 981,\n"," 187,\n"," 330,\n"," 745,\n"," 228,\n"," 769,\n"," 114,\n"," 398,\n"," 109,\n"," 550,\n"," 504,\n"," 218,\n"," 869,\n"," 505,\n"," 541,\n"," 750,\n"," 921,\n"," 352,\n"," 97,\n"," 425,\n"," 454,\n"," 700,\n"," 102,\n"," 15,\n"," 784,\n"," 426,\n"," 927,\n"," 133,\n"," 340,\n"," 11,\n"," 761,\n"," 360,\n"," 588,\n"," 200,\n"," 170,\n"," 336,\n"," 737,\n"," 898,\n"," 544,\n"," 427,\n"," 416,\n"," 612,\n"," 592,\n"," 282,\n"," 58,\n"," 322,\n"," 845,\n"," 665,\n"," 615,\n"," 546,\n"," 484,\n"," 411,\n"," 908,\n"," 87,\n"," 853,\n"," 10,\n"," 222,\n"," 786,\n"," 465,\n"," 905,\n"," 189,\n"," 323,\n"," 431,\n"," 766,\n"," 528,\n"," 572,\n"," 722,\n"," 258,\n"," 75,\n"," 652,\n"," 673,\n"," 942,\n"," 108,\n"," 246,\n"," 439,\n"," 298,\n"," 776,\n"," 493,\n"," 628,\n"," 349,\n"," 13,\n"," 611,\n"," 367,\n"," 219,\n"," 477,\n"," 244,\n"," 638,\n"," 261,\n"," 469,\n"," 475,\n"," 373,\n"," 443,\n"," 47,\n"," 350,\n"," 639,\n"," 819,\n"," 660,\n"," 682,\n"," 914,\n"," 236,\n"," 632,\n"," 488,\n"," 965,\n"," 408,\n"," 835,\n"," 343,\n"," 472,\n"," 584,\n"," 361,\n"," 82,\n"," 617,\n"," 789,\n"," 624,\n"," 836,\n"," 708,\n"," 824,\n"," 549,\n"," 85,\n"," 627,\n"," 663,\n"," 152,\n"," 188,\n"," 91,\n"," 566,\n"," 702,\n"," 920,\n"," 280,\n"," 876,\n"," 902,\n"," 710,\n"," 19,\n"," 944,\n"," 423,\n"," 952,\n"," 438,\n"," 259,\n"," 671,\n"," 791,\n"," 903,\n"," 491,\n"," 879,\n"," 901,\n"," 67,\n"," 450,\n"," 593,\n"," 809,\n"," 676,\n"," 113,\n"," 185,\n"," 147,\n"," 2,\n"," 199,\n"," 580,\n"," 659,\n"," 979,\n"," 758,\n"," 748,\n"," 86,\n"," 16,\n"," 424,\n"," 773,\n"," 527,\n"," 561,\n"," 553,\n"," 865,\n"," 461,\n"," 797,\n"," 697,\n"," 923,\n"," 801,\n"," 744,\n"," 868,\n"," 62,\n"," 502,\n"," 342,\n"," 401,\n"," 437,\n"," 883,\n"," 119,\n"," 8,\n"," 428,\n"," 210,\n"," 288,\n"," 767,\n"," 289,\n"," 630,\n"," 688,\n"," 844,\n"," 961]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8J_bX-QtJbm8","colab":{}},"source":["y = movies_clean['sentiment']\n","X = movies_clean['review']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bTHOHXFRJbm_","colab":{}},"source":["X_train = X.iloc[ind_train]\n","X_test = X.iloc[ind_test]\n","y_train = y.iloc[ind_train]\n","y_test = y.iloc[ind_test]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-Yoz0T1KbUA","colab_type":"code","outputId":"c44c9085-ef11-4f3a-f590-305aea454ca2","executionInfo":{"status":"ok","timestamp":1569202222469,"user_tz":-480,"elapsed":904,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y_test.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(500,)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"rEVCVvWgJG6z","colab_type":"code","colab":{}},"source":["vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aBBNvYHxJRC7","colab_type":"code","colab":{}},"source":["X_train_ng = vectorizer_ng2.fit_transform(X_train)\n","X_test_ng = vectorizer_ng2.fit_transform(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DaH4FGhnKH_f","colab_type":"code","outputId":"d50769bf-d4cc-489e-e3ff-8bec36a84ac6","executionInfo":{"status":"ok","timestamp":1569202234913,"user_tz":-480,"elapsed":857,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["X_train_ng"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<500x79059 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 169907 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"sO0qboOzKLiH","colab_type":"code","outputId":"eddcb4d2-a438-4c39-82f0-6faa0e334cdf","executionInfo":{"status":"ok","timestamp":1569202237183,"user_tz":-480,"elapsed":861,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["X_test_ng"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<500x76123 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 165573 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"8IKhX-mcKOqa","colab_type":"text"},"source":["The n-gram training reviews are available as X_train_ng. The corresponding test reviews are available as X_test_ng. Finally, use y_train and y_test to access the training and test sentiment classes respectively."]},{"cell_type":"code","metadata":{"id":"BnRrsREpKPmu","colab_type":"code","outputId":"59e1079c-596a-48ce-ca2f-b7435ffc8c2f","executionInfo":{"status":"ok","timestamp":1569202243209,"user_tz":-480,"elapsed":1152,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Define an instance of MultinomialNB \n","clf_ng = MultinomialNB()\n","\n","# Fit the classifier\n","clf_ng.fit(X_train_ng, y_train)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"X17mYpdwKTII","colab_type":"code","outputId":"6a2d3b68-a8f0-46b1-9601-23295a94968c","executionInfo":{"status":"error","timestamp":1569202246154,"user_tz":-480,"elapsed":864,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":324}},"source":["# Measure the accuracy\n","accuracy = clf_ng.score(X_test_ng, y_test)\n","print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-9f6a9344c384>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_ng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_ng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The accuracy of the classifier on the test set is %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \"\"\"\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \"\"\"\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0m\u001b[1;32m    738\u001b[0m                 self.class_log_prior_)\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \"\"\"\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"toarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: dimension mismatch"]}]},{"cell_type":"code","metadata":{"id":"d2H7oAiDKSya","colab_type":"code","colab":{}},"source":["# Predict the sentiment of a negative review\n","review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n","prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n","print(\"The sentiment predicted by the classifier is %i\" % (prediction))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HOq2msDoKzFN","colab_type":"text"},"source":["####Comparing performance of n-gram models\n","You now know how to conduct sentiment analysis by converting text into various n-gram representations and feeding them to a classifier. In this exercise, we will conduct sentiment analysis for the same movie reviews from before using two n-gram models: unigrams and n-grams upto n equal to 3.\n","\n","We will then compare the performance using three criteria: accuracy of the model on the test set, time taken to execute the program and the number of features created when generating the n-gram representation."]},{"cell_type":"code","metadata":{"id":"r93j4pJDv6ge","colab_type":"code","outputId":"71594ec0-5011-43ca-e603-91329eb351a5","executionInfo":{"status":"ok","timestamp":1569202371965,"user_tz":-480,"elapsed":860,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["df = movies_clean\n","df.tail()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>995</th>\n","      <td>this movie turned out to be pretty much what i...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>from time to time it s very advisable for the ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>ed wood is eclipsed and becomes orson welles t...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>well here we have yet another role reversal mo...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>there is a lot of obvious hype associated with...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                review  sentiment\n","995  this movie turned out to be pretty much what i...          1\n","996  from time to time it s very advisable for the ...          0\n","997  ed wood is eclipsed and becomes orson welles t...          0\n","998  well here we have yet another role reversal mo...          0\n","999  there is a lot of obvious hype associated with...          0"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"i2e1mfjYvvBh","colab_type":"text"},"source":["1. Initialize a CountVectorizer object such that it generates unigrams."]},{"cell_type":"code","metadata":{"id":"FO3mK2dawSFA","colab_type":"code","colab":{}},"source":["import time\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qTRcL45iK2N8","colab_type":"code","outputId":"20931778-999b-44b8-d5c6-8806a788b333","executionInfo":{"status":"ok","timestamp":1569202529983,"user_tz":-480,"elapsed":868,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["start_time = time.time()\n","# Splitting the data into training and test sets\n","train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n","\n","# Generating ngrams\n","vectorizer = CountVectorizer()\n","train_X = vectorizer.fit_transform(train_X)\n","test_X = vectorizer.transform(test_X)\n","\n","# Fit classifier\n","clf = MultinomialNB()\n","clf.fit(train_X, train_y)\n","\n","# Print accuracy, time and number of dimensions\n","print(\"The program took %.3f seconds to complete. \\n The accuracy on the test set is %.2f.\\n The ngram representation had %i features.\" \n","      % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The program took 0.242 seconds to complete. \n"," The accuracy on the test set is 0.75.\n"," The ngram representation had 12347 features.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xKi-3-lRv1XQ","colab_type":"text"},"source":["2. Initialize a CountVectorizer object such that it generates ngrams upto n=3."]},{"cell_type":"code","metadata":{"id":"SyyKri7pwG6W","colab_type":"code","outputId":"10695594-4ab5-4a41-d8e4-3f0ccca5e90b","executionInfo":{"status":"ok","timestamp":1569202550332,"user_tz":-480,"elapsed":2085,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["start_time = time.time()\n","# Splitting the data into training and test sets\n","train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n","\n","# Generating ngrams\n","vectorizer = CountVectorizer(ngram_range=(1,3))\n","train_X = vectorizer.fit_transform(train_X)\n","test_X = vectorizer.transform(test_X)\n","\n","# Fit classifier\n","clf = MultinomialNB()\n","clf.fit(train_X, train_y)\n","\n","# Print accuracy, time and number of dimensions\n","print(\"The program took %.3f seconds to complete.\\n The accuracy on the test set is %.2f.\\n The ngram representation had %i features.\" \n","      % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The program took 1.170 seconds to complete.\n"," The accuracy on the test set is 0.77.\n"," The ngram representation had 178240 features.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jecZGubNshwN","colab_type":"text"},"source":["## 4. TF-IDF and similarity scores\n","\n","Learn how to compute tf-idf weights and the cosine similarity score between two vectors. You will use these concepts to build a movie and a TED Talk recommender. Finally, you will also learn about word embeddings and using word vector representations, you will compute similarities between various Pink Floyd songs."]},{"cell_type":"markdown","metadata":{"id":"78oEmmMfwy-1","colab_type":"text"},"source":["###Building tf-idf document vectors\n"]},{"cell_type":"markdown","metadata":{"id":"0mktKHgFw4Ya","colab_type":"text"},"source":["####tf-idf vectors for TED talks\n","\n","Your task is to generate the tf-idf vectors for these talks.\n","\n","In a later lesson, we will use these vectors to generate recommendations of similar talks based on the transcript."]},{"cell_type":"code","metadata":{"id":"qjn2n41zwG8d","colab_type":"code","outputId":"c6826dc3-618b-4690-b45d-63f8cf464ae2","executionInfo":{"status":"ok","timestamp":1569202770957,"user_tz":-480,"elapsed":874,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["ted = pd.read_csv(\"ted.csv\")\n","print(\"dataframe dimension: \", ted.shape)\n","ted.tail()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["dataframe dimension:  (500, 2)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>transcript</th>\n","      <th>url</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>495</th>\n","      <td>Today I'm going to unpack for you three exampl...</td>\n","      <td>https://www.ted.com/talks/john_hodgman_design_...</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>Both myself and my brother belong to the under...</td>\n","      <td>https://www.ted.com/talks/sheikha_al_mayassa_g...</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>John Hockenberry: It's great to be here with y...</td>\n","      <td>https://www.ted.com/talks/tom_shannon_the_pain...</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>What you're doing, right now, at this very mom...</td>\n","      <td>https://www.ted.com/talks/nilofer_merchant_got...</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>We've got a real problem with math education r...</td>\n","      <td>https://www.ted.com/talks/conrad_wolfram_teach...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            transcript                                                url\n","495  Today I'm going to unpack for you three exampl...  https://www.ted.com/talks/john_hodgman_design_...\n","496  Both myself and my brother belong to the under...  https://www.ted.com/talks/sheikha_al_mayassa_g...\n","497  John Hockenberry: It's great to be here with y...  https://www.ted.com/talks/tom_shannon_the_pain...\n","498  What you're doing, right now, at this very mom...  https://www.ted.com/talks/nilofer_merchant_got...\n","499  We've got a real problem with math education r...  https://www.ted.com/talks/conrad_wolfram_teach..."]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"HG-dFsc6xfih","colab_type":"code","outputId":"a85b3a30-3a4d-4fbd-8797-e5251e1ef0e6","executionInfo":{"status":"ok","timestamp":1569202793786,"user_tz":-480,"elapsed":875,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# transform to series\n","ted = ted['transcript']\n","ted.tail()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["495    Today I'm going to unpack for you three exampl...\n","496    Both myself and my brother belong to the under...\n","497    John Hockenberry: It's great to be here with y...\n","498    What you're doing, right now, at this very mom...\n","499    We've got a real problem with math education r...\n","Name: transcript, dtype: object"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"MR-b36AExSGU","colab_type":"text"},"source":["In this exercise, you have been given a corpus ted which contains the transcripts of 500 TED Talks. \n","\n","- Import TfidfVectorizer from sklearn.\n","- Create a TfidfVectorizer object. Name it vectorizer.\n","- Generate tfidf_matrix for ted using the fit_transform() method."]},{"cell_type":"code","metadata":{"id":"iqEZY4hVslKk","colab_type":"code","outputId":"535e6463-3095-4fb2-9e48-91bc62d82e01","executionInfo":{"status":"ok","timestamp":1569202808511,"user_tz":-480,"elapsed":1527,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Import TfidfVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Create TfidfVectorizer object\n","vectorizer = TfidfVectorizer()\n","\n","# Generate matrix of word vectors\n","tfidf_matrix = vectorizer.fit_transform(ted)\n","\n","# Print the shape of tfidf_matrix\n","print(tfidf_matrix.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(500, 29158)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FvYJgKN1xxZU","colab_type":"text"},"source":["###Cosine similarity\n"]},{"cell_type":"markdown","metadata":{"id":"dR_0--K1xz-A","colab_type":"text"},"source":["####Computing dot product\n","\n","In this exercise, we will learn to compute the dot product between two vectors, A = (1, 3) and B = (-2, 2), using the numpy library. \n","\n","More specifically, we will use the np.dot() function to compute the dot product of two numpy arrays."]},{"cell_type":"code","metadata":{"id":"foNaTJFMyybF","colab_type":"code","outputId":"4547d737-a394-4584-dea3-098bff8918bb","executionInfo":{"status":"ok","timestamp":1569203109438,"user_tz":-480,"elapsed":865,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Initialize numpy vectors\n","A = np.array([1,3])\n","B = np.array([-2, 2])\n","\n","# Compute dot product\n","dot_prod = np.dot(A, B)\n","\n","# Print dot product\n","print(dot_prod)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-mm6GbEDy1Sm","colab_type":"text"},"source":["####Cosine similarity matrix of a corpus\n","\n","In this exercise, you have been given a corpus, which is a list containing five sentences. The corpus is printed in the console. You have to compute the cosine similarity matrix which contains the pairwise cosine similarity score for every pair of sentences (vectorized using tf-idf).\n","\n","Remember, the value corresponding to the ith row and jth column of a similarity matrix denotes the similarity score for the ith and jth vector."]},{"cell_type":"code","metadata":{"id":"iWZ0hKeLy5q_","colab_type":"code","colab":{}},"source":["corpus = ['The sun is the largest celestial body in the solar system',\n"," 'The solar system consists of the sun and eight revolving planets',\n"," 'Ra was the Egyptian Sun God',\n"," 'The Pyramids were the pinnacle of Egyptian architecture',\n"," 'The quick brown fox jumps over the lazy dog']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SvpLMUery-TQ","colab_type":"text"},"source":["- Initialize an instance of `TfidfVectorizer`. Name it tfidf_vectorizer.\n","- Using `fit_transform()`, generate the tf-idf vectors for corpus. Name it `tfidf_matrix`.\n","- Use `cosine_similarity()` and pass `tfidf_matrix` to compute the cosine similarity matrix `cosine_sim`."]},{"cell_type":"code","metadata":{"id":"c4WV2OiNzdWi","colab_type":"code","colab":{}},"source":["from sklearn.metrics.pairwise import cosine_similarity"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JWVmrvojzKUR","colab_type":"code","outputId":"a644c0d8-9486-4eaa-fbc6-acb7735f0cf4","executionInfo":{"status":"ok","timestamp":1569203290888,"user_tz":-480,"elapsed":833,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["# Initialize an instance of tf-idf Vectorizer\n","tfidf_vectorizer = TfidfVectorizer()\n","\n","# Generate the tf-idf vectors for the corpus\n","tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n","\n","# Compute and print the cosine similarity matrix\n","cosine_sim = cosine_similarity(tfidf_matrix,tfidf_matrix)\n","print(cosine_sim)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n"," [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n"," [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n"," [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n"," [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TLmWW552zpq8","colab_type":"text"},"source":["###Building a plot line based recommender\n"]},{"cell_type":"markdown","metadata":{"id":"UbUKESXHzsTW","colab_type":"text"},"source":["####Comparing linear_kernel and cosine_similarity\n","\n","In this exercise, you have been given tfidf_matrix which contains the tf-idf vectors of a thousand documents. Your task is to generate the cosine similarity matrix for these vectors first using cosine_similarity and then, using linear_kernel.\n","\n","1. Compute the cosine similarity matrix for tfidf_matrix using cosine_similarity."]},{"cell_type":"code","metadata":{"id":"dxNkX2g8zKW_","colab_type":"code","outputId":"ec53983a-e4a1-49fc-c2cb-4d154ae3b4e6","executionInfo":{"status":"ok","timestamp":1569203402249,"user_tz":-480,"elapsed":867,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# Record start time\n","start = time.time()\n","\n","# Compute cosine similarity matrix\n","cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n","\n","# Print cosine similarity matrix\n","print(cosine_sim)\n","\n","# Print time taken\n","print(\"Time taken: %s seconds\" %(time.time() - start))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n"," [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n"," [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n"," [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n"," [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n","Time taken: 0.004064321517944336 seconds\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NMKR-O04z3Gm","colab_type":"text"},"source":["2. Compute the cosine similarity matrix for tfidf_matrix using linear_kernel."]},{"cell_type":"code","metadata":{"id":"9OxR7L3f0Pk9","colab_type":"code","colab":{}},"source":["from sklearn.metrics.pairwise import linear_kernel"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wDeyV47uz42f","colab_type":"code","outputId":"03787f61-fcb3-413f-97f7-11eb92fc247d","executionInfo":{"status":"ok","timestamp":1569203492648,"user_tz":-480,"elapsed":586,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# Record start time\n","start = time.time()\n","\n","# Compute cosine similarity matrix\n","cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n","\n","# Print cosine similarity matrix\n","print(cosine_sim)\n","\n","# Print time taken\n","print(\"Time taken: %s seconds\" %(time.time() - start))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n"," [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n"," [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n"," [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n"," [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n","Time taken: 0.0032014846801757812 seconds\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DGvKM_Qf0Ii9","colab_type":"text"},"source":["**Observation:**\n","\n","Notice how both linear_kernel and cosine_similarity produced the same result. However, linear_kernel took a smaller amount of time to execute. "]},{"cell_type":"markdown","metadata":{"id":"rRJAe03B0VuB","colab_type":"text"},"source":["#### Plot recommendation engine\n","\n","In this exercise, we will build a recommendation engine that suggests movies based on similarity of plot lines. \n","\n","- You have been given a `get_recommendations()` function that takes in the title of a movie, a similarity matrix and an indices series as its arguments and outputs a list of most similar movies. \n","- `indices` has already been provided to you.\n","\n","- You have also been given a `movie_plots` Series that contains the plot lines of several movies. \n","\n","**Your task** is to generate a cosine similarity matrix for the tf-idf vectors of these plots.\n","\n","Consequently, we will check the potency of our engine by generating recommendations for one of my favorite movies, The Dark Knight Rises."]},{"cell_type":"code","metadata":{"id":"J_0AWeqf0r1V","colab_type":"code","colab":{}},"source":["def get_recommendations(title, cosine_sim, indices):\n","    # Get the index of the movie that matches the title\n","    idx = indices[title]\n","    # Get the pairwsie similarity scores\n","    sim_scores = list(enumerate(cosine_sim[idx]))\n","    # Sort the movies based on the similarity scores\n","    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n","    # Get the scores for 10 most similar movies\n","    sim_scores = sim_scores[1:11]\n","    # Get the movie indices\n","    movie_indices = [i[0] for i in sim_scores]\n","    # Return the top 10 most similar movies\n","    return metadata['title'].iloc[movie_indices]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FJyIvIrC0r3b","colab_type":"code","colab":{}},"source":["# movie_plots not available"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wX2S9VoX2WZY","colab_type":"text"},"source":["- Initialize a TfidfVectorizer with English stop_words. Name it tfidf.\n","- Construct tfidf_matrix by fitting and transforming the movie plot data using fit_transform().\n","- Generate the cosine similarity matrix cosine_sim using tfidf_matrix. Don't use cosine_similarity()!\n","- Use get_recommendations() to generate recommendations for 'The Dark Knight Rises'."]},{"cell_type":"code","metadata":{"id":"8BPoiAkY2dwZ","colab_type":"code","colab":{}},"source":["# Initialize the TfidfVectorizer \n","tfidf = TfidfVectorizer(stop_words='english')\n","\n","# Construct the TF-IDF matrix\n","tfidf_matrix = tfidf.fit_transform(movie_plots)\n","\n","# Generate the cosine similarity matrix\n","cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n"," \n","# Generate recommendations\n","print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"unQIUmrv2gCR","colab_type":"text"},"source":["#### The recommender function\n","In this exercise, we will build a recommender function `get_recommendations()`, as discussed in the lesson and the previous exercise. As we know, it takes in a title, a cosine similarity matrix, and a movie title and index mapping as arguments and outputs a list of 10 titles most similar to the original title (excluding the title itself).\n","\n","You have been given a dataset `metadata` that consists of the movie titles and overviews. The head of this dataset has been printed to console."]},{"cell_type":"code","metadata":{"id":"gggIvHy52pdw","colab_type":"code","colab":{}},"source":["# metadata not available"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"twsiSzQ_5rf6","colab_type":"text"},"source":["- Get index of the movie that matches the title by using the title key of indices.\n","- Extract the ten most similar movies from sim_scores and store it back in sim_scores."]},{"cell_type":"code","metadata":{"id":"WsbtjzaT5vty","colab_type":"code","colab":{}},"source":["# Generate mapping between titles and index\n","indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n","\n","def get_recommendations(title, cosine_sim, indices):\n","    # Get index of movie that matches title\n","    idx = indices[title]\n","    # Sort the movies based on the similarity scores\n","    sim_scores = list(enumerate(cosine_sim[idx]))\n","    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n","    # Get the scores for 10 most similar movies\n","    sim_scores = sim_scores[1:11]\n","    # Get the movie indices\n","    movie_indices = [i[0] for i in sim_scores]\n","    # Return the top 10 most similar movies\n","    return metadata['title'].iloc[movie_indices]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-C_eiS8W50T_","colab_type":"text"},"source":["####TED talk recommender\n","\n","In this exercise, we will build a recommendation system that suggests TED Talks based on their transcripts.\n","\n","Consequently, we will generate recommendations for a talk titled '5 ways to kill your dreams' by Brazilian entrepreneur Bel Pesce."]},{"cell_type":"code","metadata":{"id":"anZDcL0K55Q7","colab_type":"code","outputId":"1aec10cf-18e6-49c6-a49e-89e909986660","executionInfo":{"status":"ok","timestamp":1569205088368,"user_tz":-480,"elapsed":890,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# dataset not available"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["495    Today I'm going to unpack for you three exampl...\n","496    Both myself and my brother belong to the under...\n","497    John Hockenberry: It's great to be here with y...\n","498    What you're doing, right now, at this very mom...\n","499    We've got a real problem with math education r...\n","Name: transcript, dtype: object"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"7MuDHKy_6vva","colab_type":"text"},"source":["You have been given a get_recommendations() function that takes in the title of a talk, a similarity matrix and an indices series as its arguments, and outputs a list of most similar talks. indices has already been provided to you.\n","\n","You have also been given a transcripts series that contains the transcripts of around 500 TED talks. Your task is to generate a cosine similarity matrix for the tf-idf vectors of the talk transcripts.\n","\n","- Initialize a TfidfVectorizer with English stopwords. Name it tfidf.\n","- Construct tfidf_matrix by fitting and transforming transcripts.\n","- Generate the cosine similarity matrix cosine_sim using tfidf_matrix.\n","- Use get_recommendations() to generate recommendations for '5 ways to kill your dreams'."]},{"cell_type":"code","metadata":{"id":"SkeFn4BI55Vr","colab_type":"code","colab":{}},"source":["# Initialize the TfidfVectorizer \n","tfidf = TfidfVectorizer(stop_words='english')\n","\n","# Construct the TF-IDF matrix\n","tfidf_matrix = tfidf.fit_transform(transcripts)\n","\n","# Generate the cosine similarity matrix\n","cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n"," \n","# Generate recommendations\n","print(get_recommendations('5 ways to kill your dreams', cosine_sim, indices))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nG-vqMB365Ki","colab_type":"text"},"source":["###Beyond n-grams: word embeddings\n"]},{"cell_type":"markdown","metadata":{"id":"Xbw1Dj5b67hx","colab_type":"text"},"source":["####Generating word vectors\n","In this exercise, we will generate the pairwise similarity scores of all the words in a sentence. The sentence is available as sent and has been printed to the console for your convenience.\n","\n","- Create a Doc object doc for sent.\n","- In the nested loop, compute the similarity between token1 and token2."]},{"cell_type":"code","metadata":{"id":"PkjGjBfY8J1E","colab_type":"code","colab":{}},"source":["# Load the required model\n","nlp = spacy.load('en_core_web_sm')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G-bVKUBd55T3","colab_type":"code","colab":{}},"source":["# Create the doc object\n","doc = nlp(sent)\n","\n","# Compute pairwise similarity scores\n","for token1 in doc:\n","  for token2 in doc:\n","    print(token1.text, token2.text, token1.similarity(token2))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coAZdEKb7Ec0","colab_type":"text"},"source":["####Computing similarity of Pink Floyd songs\n","In this final exercise, you have been given lyrics of three songs by the British band Pink Floyd, namely 'High Hopes', 'Hey You' and 'Mother'. The lyrics to these songs are available as hopes, hey and mother respectively.\n","\n","Your task is to compute the pairwise similarity between mother and hopes, and mother and hey.\n","\n","- Create Doc objects for mother, hopes and hey.\n","- Compute the similarity between mother and hopes.\n","- Compute the similarity between mother and hey."]},{"cell_type":"code","metadata":{"id":"ea6X-Brl7Dbn","colab_type":"code","colab":{}},"source":["hopes = \"\\nBeyond the horizon of the place we lived when we were young\\nIn a world of magnets and miracles\\nOur thoughts strayed constantly and without boundary\\nThe ringing of the division bell had begun\\nAlong the Long Road and on down the Causeway\\nDo they still meet there by the Cut\\nThere was a ragged band that followed in our footsteps\\nRunning before times took our dreams away\\nLeaving the myriad small creatures trying to tie us to the ground\\nTo a life consumed by slow decay\\nThe grass was greener\\nThe light was brighter\\nWhen friends surrounded\\nThe nights of wonder\\nLooking beyond the embers of bridges glowing behind us\\nTo a glimpse of how green it was on the other side\\nSteps taken forwards but sleepwalking back again\\nDragged by the force of some in a tide\\nAt a higher altitude with flag unfurled\\nWe reached the dizzy heights of that dreamed of world\\nEncumbered forever by desire and ambition\\nThere's a hunger still unsatisfied\\nOur weary eyes still stray to the horizon\\nThough down this road we've been so many times\\nThe grass was greener\\nThe light was brighter\\nThe taste was sweeter\\nThe nights of wonder\\nWith friends surrounded\\nThe dawn mist glowing\\nThe water flowing\\nThe endless river\\nForever and ever\\n\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5L6E4Pb37D03","colab_type":"code","colab":{}},"source":["hey = \"\\nHey you, out there in the cold\\nGetting lonely, getting old\\nCan you feel me?\\nHey you, standing in the aisles\\nWith itchy feet and fading smiles\\nCan you feel me?\\nHey you, don't help them to bury the light\\nDon't give in without a fight\\nHey you out there on your own\\nSitting naked by the phone\\nWould you touch me?\\nHey you with you ear against the wall\\nWaiting for someone to call out\\nWould you touch me?\\nHey you, would you help me to carry the stone?\\nOpen your heart, I'm coming home\\nBut it was only fantasy\\nThe wall was too high\\nAs you can see\\nNo matter how he tried\\nHe could not break free\\nAnd the worms ate into his brain\\nHey you, out there on the road\\nAlways doing what you're told\\nCan you help me?\\nHey you, out there beyond the wall\\nBreaking bottles in the hall\\nCan you help me?\\nHey you, don't tell me there's no hope at all\\nTogether we stand, divided we fall\\n\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A9v73KO17DaN","colab_type":"code","colab":{}},"source":["mother = \"\\nMother do you think they'll drop the bomb?\\nMother do you think they'll like this song?\\nMother do you think they'll try to break my balls?\\nOoh, ah\\nMother should I build the wall?\\nMother should I run for President?\\nMother should I trust the government?\\nMother will they put me in the firing mine?\\nOoh ah,\\nIs it just a waste of time?\\nHush now baby, baby, don't you cry.\\nMama's gonna make all your nightmares come true.\\nMama's gonna put all her fears into you.\\nMama's gonna keep you right here under her wing.\\nShe won't let you fly, but she might let you sing.\\nMama's gonna keep baby cozy and warm.\\nOoh baby, ooh baby, ooh baby,\\nOf course mama's gonna help build the wall.\\nMother do you think she's good enough, for me?\\nMother do you think she's dangerous, to me?\\nMother will she tear your little boy apart?\\nOoh ah,\\nMother will she break my heart?\\nHush now baby, baby don't you cry.\\nMama's gonna check out all your girlfriends for you.\\nMama won't let anyone dirty get through.\\nMama's gonna wait up until you get in.\\nMama will always find out where you've been.\\nMama's gonna keep baby healthy and clean.\\nOoh baby, ooh baby, ooh baby,\\nYou'll always be baby to me.\\nMother, did it need to be so high?\\n\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVFZ211T7aoL","colab_type":"code","outputId":"9c4a0696-50db-434d-ff23-c86ac62f5121","executionInfo":{"status":"ok","timestamp":1569205575548,"user_tz":-480,"elapsed":958,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["# Create Doc objects\n","mother_doc = nlp(mother)\n","hopes_doc = nlp(hopes)\n","hey_doc = nlp(hey)\n","\n","# Print similarity between mother and hopes\n","print(mother_doc.similarity(hopes_doc))\n","\n","# Print similarity between mother and hey\n","print(mother_doc.similarity(hey_doc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.6190568827609442\n","0.8959643884817768\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"V4wR5Wsi7arB","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}