{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Machine Translation in Python - solutions.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KN808JjXbwbd","colab_type":"text"},"source":["[datacamp link](https://www.datacamp.com/courses/machine-translation-in-python)"]},{"cell_type":"code","metadata":{"id":"yPJiZ5Cec98n","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HN53mcjJcwQC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":63},"outputId":"7ef54d6b-e16e-4c34-f8e7-bbc1dab77c37","executionInfo":{"status":"ok","timestamp":1573363177377,"user_tz":-480,"elapsed":3056,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}}},"source":["import tensorflow.keras as keras\n","from tensorflow.python.keras.utils import to_categorical\n","from tensorflow.keras.layers import Input, RepeatVector\n","from tensorflow.keras.layers import Dense, TimeDistributed\n","import tensorflow.keras.layers as layers\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics.pairwise import cosine_similarity"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"AGsjGCilWN92","colab_type":"text"},"source":["**Course Description**\n","\n","The need to pack a bilingual dictionary for your European holiday or keeping one on your desk to complete your foreign language homework is a thing of the past. You just hop on the internet and make use of a language translation service to quickly understand what the street sign means or finding out how to greet and thank a foreigner in their language. Behind the language translation services are complex machine translation models. Have you ever wondered how these models work? This course will allow you to explore the inner workings of a machine translation model. You will use Keras, a powerful Python-based deep learning library, to implement a translation model. You will then train the model to perform an English to French translation, and you will be shown techniques to improve your model. At the end of this course, you would have developed an in-depth understanding of machine translation models and appreciate them even more!"]},{"cell_type":"markdown","metadata":{"id":"thI8YoCmWSem","colab_type":"text"},"source":["## 1. Introduction to machine translation\n","\n","In this chapter, you'll understand what the encoder-decoder architecture is and how it is used for machine translation. You will also learn about Gated Recurrent Units (GRUs) and how they are used in the encoder-decoder architecture."]},{"cell_type":"markdown","metadata":{"id":"iT2AmMTBWkuf","colab_type":"text"},"source":["### Introduction to machine translation\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"k7G29RqP4u_J","colab_type":"text"},"source":["#### Understanding one-hot vectors\n"]},{"cell_type":"code","metadata":{"id":"fadacqfI424v","colab_type":"code","colab":{}},"source":["word2index = {'I': 0, 'cats': 2, 'like': 1}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pyO7Cn9F5BG1","colab_type":"text"},"source":["- Create a list of words I, like, cats and convert each word to an integer using `word2index`. Print the resulting integers."]},{"cell_type":"code","metadata":{"id":"QKNvHzyIWU2m","colab_type":"code","outputId":"d512d04c-176d-40a0-9d8e-1534b0200670","executionInfo":{"status":"ok","timestamp":1572778253700,"user_tz":-480,"elapsed":1416,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from tensorflow.python.keras.utils import to_categorical\n","\n","# Create a list of words and convert them to indices\n","words = [\"I\", \"like\", \"cats\"]\n","word_ids = [word2index[w] for w in words]\n","print(word_ids)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0, 1, 2]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uu0MUukJ5Fvv","colab_type":"text"},"source":["- Convert the words to one-hot vectors using the Keras `to_categorical()` function."]},{"cell_type":"code","metadata":{"id":"1Myh4gQyWnsB","colab_type":"code","colab":{}},"source":["# Create onehot vectors using to_categorical function\n","onehot_1 = to_categorical(word_ids)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cawGLkyf5KBJ","colab_type":"text"},"source":["- Print the words and their corresponding one-hot vectors using the `zip()` function.\n"]},{"cell_type":"code","metadata":{"id":"b8bxgBqVXhAA","colab_type":"code","outputId":"e36973ab-61b9-4274-b194-1b80bf63e2e0","executionInfo":{"status":"ok","timestamp":1572778381911,"user_tz":-480,"elapsed":656,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Print words and their corresponding onehot vectors\n","print([(w,ohe.tolist()) for w,ohe in zip(words, onehot_1)])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[('I', [1.0, 0.0, 0.0]), ('like', [0.0, 1.0, 0.0]), ('cats', [0.0, 0.0, 1.0])]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UptUgpHg5Ya6","colab_type":"text"},"source":["- Convert words to one-hot vectors using the number of classes as 5, assign it to the variable `onehot_2` and print the result."]},{"cell_type":"code","metadata":{"id":"fFKjlIra5Ucr","colab_type":"code","outputId":"ecc15a14-5c16-4bc9-9fa1-53653e0e443a","executionInfo":{"status":"ok","timestamp":1572778383804,"user_tz":-480,"elapsed":650,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Create onehot vectors with a fixed number of classes\n","onehot_2 = to_categorical(word_ids, num_classes=5)\n","print([(w,ohe.tolist()) for w,ohe in zip(words, onehot_2)])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[('I', [1.0, 0.0, 0.0, 0.0, 0.0]), ('like', [0.0, 1.0, 0.0, 0.0, 0.0]), ('cats', [0.0, 0.0, 1.0, 0.0, 0.0])]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Sx7mxVK18Hvd","colab_type":"text"},"source":["#### Exploring the `to_categorical()` function"]},{"cell_type":"markdown","metadata":{"id":"Fpmgydv_8VOk","colab_type":"text"},"source":["In part 1, you will implement the function `compute_onehot_length()` that generates one-hot vectors for a given list of words and computes the length of those vectors."]},{"cell_type":"code","metadata":{"id":"1Mhfz8WNXlgJ","colab_type":"code","colab":{}},"source":["def compute_onehot_length(words, word2index):\n","  # Create word IDs for words\n","  word_ids = [word2index[w] for w in words]\n","  # Convert word IDs to onehot vectors\n","  onehot = to_categorical(word_ids)\n","  # Return the length of a single one-hot vector\n","  return onehot.shape[1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pZHNxCjq8Z8L","colab_type":"code","outputId":"7b86ced4-d1de-4ae9-b700-842646670b5a","executionInfo":{"status":"ok","timestamp":1572779183121,"user_tz":-480,"elapsed":704,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["word2index = {\"He\":0, \"drank\": 1, \"milk\": 2}\n","# Compute and print onehot length of a list of words\n","print(compute_onehot_length([\"He\", \"drank\", \"milk\"], word2index))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3l5aTM8y8pRf","colab_type":"text"},"source":["The `num_classes` argument controls the length of the one-hot encoded vectors produced by the `to_categorical()` function. You will see that in situations where you have two different corpora (i.e. collections of texts) with different vocabularies, leaving the `num_classes` undefined can result in one-hot vectors of varying length."]},{"cell_type":"code","metadata":{"id":"YKxhtuC69NOZ","colab_type":"code","colab":{}},"source":["word2index = {'He': 6,\n"," 'I': 0,\n"," 'We': 3,\n"," 'cats': 2,\n"," 'dogs': 5,\n"," 'hates': 7,\n"," 'like': 4,\n"," 'rabbits': 8}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WA9oTo1LXoYR","colab_type":"code","colab":{}},"source":["words_1 = [\"I\", \"like\", \"cats\", \"We\", \"like\", \"dogs\", \"He\", \"hates\", \"rabbits\"]\n","# Call compute_onehot_length on words_1\n","length_1 = compute_onehot_length(words_1, word2index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kBMnA_dQ82aO","colab_type":"code","colab":{}},"source":["words_2 = [\"I\", \"like\", \"cats\", \"We\", \"like\", \"dogs\", \"We\", \"like\", \"cats\"]\n","# Call compute_onehot_length on words_2\n","length_2 = compute_onehot_length(words_2, word2index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QR9mVNz48x0P","colab_type":"code","outputId":"7744e250-8235-4789-c950-03a577b0549d","executionInfo":{"status":"ok","timestamp":1572779398623,"user_tz":-480,"elapsed":672,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Print length_1 and length_2\n","print(\"length_1 =>\", length_1, \" and length_2 => \", length_2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["length_1 => 9  and length_2 =>  6\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P2xqoYXBXsoQ","colab_type":"text"},"source":["### Encoder decoder architecture\n"]},{"cell_type":"markdown","metadata":{"id":"NqHs_g1C9Wpk","colab_type":"text"},"source":["#### Text reversing model - Encoder"]},{"cell_type":"code","metadata":{"id":"m-CkoA5Z9210","colab_type":"code","outputId":"73cced0d-ad43-4477-ccdf-6f29529ec909","executionInfo":{"status":"ok","timestamp":1572779611798,"user_tz":-480,"elapsed":1216,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["import matplotlib.pyplot as plt\n","img = plt.imread('12_encoder_decoder_2.png')\n","fig= plt.figure(figsize=(10,5))\n","plt.imshow(img)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f3dc15305f8>"]},"metadata":{"tags":[]},"execution_count":23},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAh0AAAEyCAYAAABecmKZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUNEhIiIijlDRISIiIo74/0XaMWJgQukRAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 720x360 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"PGOmGbwv-OFp","colab_type":"text"},"source":["The `words2onehot()` function should take in a list of words and a dictionary `word2index` and convert the list of words to an array of one-hot vectors."]},{"cell_type":"code","metadata":{"id":"aoVmFNRY-UOY","colab_type":"code","colab":{}},"source":["word2index = {'I': 0, 'cats': 2, 'like': 1}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mHUqBYAUXrMY","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","def words2onehot(word_list, word2index):\n","  # Convert words to word IDs\n","  word_ids = [word2index[w] for w in word_list]\n","  # Convert word IDs to onehot vectors and return the onehot array\n","  onehot = to_categorical(word_ids, num_classes=3)\n","  return onehot"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QP--sIrV-XfY","colab_type":"code","outputId":"515880bd-1e11-4648-bb1e-36bb88a7d6e2","executionInfo":{"status":"ok","timestamp":1572779717360,"user_tz":-480,"elapsed":847,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["words = [\"I\", \"like\", \"cats\"]\n","# Convert words to onehot vectors using words2onehot\n","onehot = words2onehot(words, word2index)\n","# Print the result as (<word>, <onehot>) tuples\n","print([(w,ohe.tolist()) for w,ohe in zip(words, onehot)])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[('I', [1.0, 0.0, 0.0]), ('like', [0.0, 1.0, 0.0]), ('cats', [0.0, 0.0, 1.0])]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MDmMD8Zo-juL","colab_type":"text"},"source":["Here you will be implementing the `encoder()` function. \n","\n","The `encoder()` function takes in a set of one-hot vectors and converts them to a list of word ids."]},{"cell_type":"code","metadata":{"id":"Rf1RVILEXv1p","colab_type":"code","colab":{}},"source":["def encoder(onehot):\n","  # Get word IDs from onehot vectors and return the IDs\n","  word_ids = np.argmax(onehot, axis=1)\n","  return word_ids"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3KSlwoPM-0H9","colab_type":"code","colab":{}},"source":["word2index = {'We': 0, 'dogs': 2, 'like': 1}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5P7ZdOgH-o4v","colab_type":"code","outputId":"0ff1e27b-8a48-41e7-d3fc-3e79dd2a1478","executionInfo":{"status":"ok","timestamp":1572779815547,"user_tz":-480,"elapsed":723,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Define \"we like dogs\" as words\n","words = [\"We\", \"like\", \"dogs\"]\n","# Convert words to onehot vectors using words2onehot\n","onehot = words2onehot(words, word2index)\n","# Get the context vector by using the encoder function\n","context = encoder(onehot)\n","print(context)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0 1 2]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H7jkZYuK-42Q","colab_type":"text"},"source":["#### Complete text reversing model\n"]},{"cell_type":"code","metadata":{"id":"3Y60ysS3_CVW","colab_type":"code","colab":{}},"source":["index2word = {0: 'We', 1: 'like', 2: 'dogs'}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WTSIjUoP_JDd","colab_type":"text"},"source":["- Define the `onehot2words()` function, which obtains word IDs from one-hot vectors and then convert the IDs to words."]},{"cell_type":"code","metadata":{"id":"6lrKeswhXy1i","colab_type":"code","colab":{}},"source":["# Define the onehot2words function that returns words for a set of onehot vectors\n","def onehot2words(onehot, index2word):\n","  ids = np.argmax(onehot, axis=1)\n","  res = [index2word[id] for id in ids]\n","  return res"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TlVHiu18_NGP","colab_type":"text"},"source":["- Define the `decoder()` function which reverses the word IDs in the context vector and produce one-hot vectors from reversed word IDs."]},{"cell_type":"code","metadata":{"id":"Kl7dOKXYX1Nr","colab_type":"code","colab":{}},"source":["# Define the decoder function that returns reversed onehot vectors\n","def decoder(context_vector):\n","  word_ids_rev = context_vector[::-1]\n","  onehot_rev = to_categorical(word_ids_rev, num_classes=3)\n","  return onehot_rev"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ifn_XYaQ_Rtw","colab_type":"text"},"source":["- Get the one-hot vectors of the reversed word IDs using the `decoder()` function."]},{"cell_type":"code","metadata":{"id":"NclxaUbJX42q","colab_type":"code","colab":{}},"source":["# Convert context to reversed onehot vectors using decoder\n","onehot_rev = decoder(context)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CWZfLdsI_UhX","colab_type":"text"},"source":["- Get the reversed words from the one-hot vectors using the `onehot2words()` function."]},{"cell_type":"code","metadata":{"id":"cV1lwP3U_bAS","colab_type":"code","outputId":"a8cc99ce-ad35-479a-94c0-ee26bcb99f1f","executionInfo":{"status":"ok","timestamp":1572780116256,"user_tz":-480,"elapsed":644,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Get the reversed words using the onehot2words function\n","reversed_words = onehot2words(onehot_rev, index2word)\n","print(reversed_words)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['dogs', 'like', 'We']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"agymJQWEX-Yk","colab_type":"text"},"source":["### Understanding sequential models\n"]},{"cell_type":"markdown","metadata":{"id":"9fvqF6tHADtA","colab_type":"text"},"source":["#### Understanding GRU models"]},{"cell_type":"markdown","metadata":{"id":"Ts85b9AEAJ0D","colab_type":"text"},"source":["- Define a Keras input layer with batch size 2, sequence length 3 and input dimensionality 4. \n","\n","Remember that you can define an input layer using the `keras.layers.Input(<argument>=<value>)` syntax."]},{"cell_type":"code","metadata":{"id":"lDRan-lnX_Mc","colab_type":"code","colab":{}},"source":["import tensorflow.keras as keras\n","import numpy as np\n","# Define an input layer\n","inp = keras.layers.Input(batch_shape=(2,3,4))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F5jhgJh0ASnt","colab_type":"text"},"source":["- Define a Keras GRU layer that has 10 hidden units and feeds on the previous input layer. \n","\n","You can use the `keras.layers.GRU(<parameter>)` to define a GRU layer."]},{"cell_type":"code","metadata":{"id":"ONaccWDyX5QJ","colab_type":"code","outputId":"3e3ac8c2-0086-4037-bddd-373ec1f38415","executionInfo":{"status":"ok","timestamp":1572780205357,"user_tz":-480,"elapsed":880,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["# Define a GRU layer that takes in the input\n","gru_out = keras.layers.GRU(10)(inp)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BnWa5ZjTAZ3f","colab_type":"text"},"source":["- Define a Keras model called model where the input is the input layer and the output is the output of the GRU layer."]},{"cell_type":"code","metadata":{"id":"EnyM5NuHYE6F","colab_type":"code","outputId":"95e93bbe-9f54-4f0f-c893-f56381550324","executionInfo":{"status":"ok","timestamp":1572780272491,"user_tz":-480,"elapsed":1290,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# Define a model that outputs the GRU output\n","model = keras.models.Model(inputs=inp, outputs=gru_out)\n","# batch size*sequence length*input dimensionality\n","x = np.random.normal(size=(2,3,4))\n","# Get the output of the model and print the result\n","y = model.predict(x)\n","print(\"shape (y) =\", y.shape, \"\\ny = \\n\", y)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["shape (y) = (2, 10) \n","y = \n"," [[ 0.01867355 -0.06034218  0.34620747  0.07195392  0.35603932 -0.16428785\n","   0.31989816 -0.05256429  0.4084206   0.33694518]\n"," [ 0.1618832  -0.44526684 -0.68581486 -0.41020092 -0.18813774 -0.21779475\n","  -0.44571614 -0.38682953  0.4116805  -0.20242918]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"skeHKBtKAsgt","colab_type":"text"},"source":["- Define an input layer that accepts arbitrary sized batch of data having sequence length 3 and input size 4.\n","- Define a GRU layer with 10 hidden units that consumes the previous input and produces an output.\n","- Define a Model called model that takes the input layer as the input and produces the output of the GRU layer as the output. Remember that you can use the `keras.models.Model(<argument>=<value>)` syntax to define a model.\n","- Predict the model output for both x1 and x2."]},{"cell_type":"code","metadata":{"id":"oALIPg_0YJZM","colab_type":"code","outputId":"e9098e3a-4428-4f33-ce08-0c4f53b3823e","executionInfo":{"status":"ok","timestamp":1572780324939,"user_tz":-480,"elapsed":866,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Define an input layer\n","inp = keras.layers.Input(shape=(3,4))\n","# Define a GRU layer that takes in the input\n","gru_out = keras.layers.GRU(10)(inp)\n","# Define a model that outputs the GRU output\n","model = keras.models.Model(inputs=inp, outputs=gru_out)\n","\n","x1 = np.random.normal(size=(2,3,4))\n","x2 = np.random.normal(size=(5,3,4))\n","\n","# Get the output of the model and print the result\n","y1 = model.predict(x1)\n","y2 = model.predict(x2)\n","print(\"shape (y1) = \", y1.shape, \" shape (y2) = \", y2.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["shape (y1) =  (2, 10)  shape (y2) =  (5, 10)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZI6qUnitA2IP","colab_type":"text"},"source":["#### Understanding sequential model output"]},{"cell_type":"markdown","metadata":{"id":"qco3YG2hA-KS","colab_type":"text"},"source":["- Create an Input layer of batch size 3, 20 time steps and 5 dimensions and call it inp.\n","- Create a GRU layer of hidden size 10, pass the inp to this layer and print the shape of the output."]},{"cell_type":"code","metadata":{"id":"ac0quXU7YE8c","colab_type":"code","outputId":"cffd6018-f338-4c82-adfb-1d330a2943d8","executionInfo":{"status":"ok","timestamp":1572780353736,"user_tz":-480,"elapsed":644,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Define the Input layer\n","inp = keras.layers.Input(batch_shape=(3,20,5)) # batch size*time steps*dimensions\n","# Define a GRU layer that takes in inp as the input\n","gru_out1 = keras.layers.GRU(10)(inp)\n","print(\"gru_out1.shape = \", gru_out1.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["gru_out1.shape =  (3, 10)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"svfOAI2CBF4r","colab_type":"text"},"source":["- Create a new `GRU` layer with 10 hidden units, which returns the state and pass in `inp` as the input, assign the state to the `gru_state`, and print the shape of `gru_out2` and `gru_state`."]},{"cell_type":"code","metadata":{"id":"Z5F6gM9QYQkd","colab_type":"code","outputId":"7d8093ce-af1e-4746-8064-4034e1c9d85e","executionInfo":{"status":"ok","timestamp":1572780429059,"user_tz":-480,"elapsed":654,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Define the second GRU and print the shape of the outputs\n","gru_out2, gru_state = keras.layers.GRU(10, return_state=True)(inp)\n","print(\"gru_out2.shape = \", gru_out2.shape)\n","print(\"gru_state.shape = \", gru_state.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["gru_out2.shape =  (3, 10)\n","gru_state.shape =  (3, 10)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VZ3qBdEFBQet","colab_type":"text"},"source":["- Create a new `GRU` layer with `return_sequences=True`, pass in `inp` as the input, and print the shape of the output."]},{"cell_type":"code","metadata":{"id":"V4AtzqmQYUDx","colab_type":"code","outputId":"009dcf90-48ff-4449-cd1a-945e93e7c028","executionInfo":{"status":"ok","timestamp":1572780460585,"user_tz":-480,"elapsed":650,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Define the third GRU layer which will return all the outputs\n","gru_out3 = keras.layers.GRU(10, return_sequences=True)(inp)\n","print(\"gru_out3.shape = \", gru_out3.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["gru_out3.shape =  (3, 20, 10)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YF9lkyoEWU7v","colab_type":"text"},"source":["## 2. Implementing an encoder decoder model with Keras\n","\n","In this chapter, you will implement the encoder-decoder model with the Keras functional API. While doing so, you will learn several useful Keras layers such as `RepeatVector` and `TimeDistributed layers`."]},{"cell_type":"markdown","metadata":{"id":"vV9VTP9mYaE8","colab_type":"text"},"source":["### Implementing the encoder\n"]},{"cell_type":"code","metadata":{"id":"mfPciIcYGyhn","colab_type":"code","colab":{}},"source":["# load text file into a list\n","text_file = open(\"vocab_en.txt\", \"r\")\n","en_text = text_file.read().split('\\n')\n","text_file.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUwbqt4ZHK2Y","colab_type":"code","outputId":"fdef4731-9c23-48b5-a971-8b0fc375c07e","executionInfo":{"status":"ok","timestamp":1573265100430,"user_tz":-480,"elapsed":923,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["en_text[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'new jersey is sometimes quiet during autumn , and it is snowy in april .'"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"WrmbUrvXHyt-","colab_type":"code","colab":{}},"source":["text_file = open(\"vocab_fr.txt\", \"r\")\n","fr_text = text_file.read().split('\\n')\n","text_file.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cl1R5mv0H39_","colab_type":"code","outputId":"65cadbe5-5962-4855-ea99-e89b0b15ee29","executionInfo":{"status":"ok","timestamp":1572782181841,"user_tz":-480,"elapsed":446,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["fr_text[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\""]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"KbB-JPG3IJxu","colab_type":"text"},"source":["- Write a `zip()` function that iterates through the first 5 sentences of the English sentences (`en_text`) and French sentences (`fr_text`).\n","\n"]},{"cell_type":"code","metadata":{"id":"0IRaZn5jYXwq","colab_type":"code","outputId":"1486f3b0-e106-47e4-8833-25172c67796a","executionInfo":{"status":"ok","timestamp":1572782348766,"user_tz":-480,"elapsed":649,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["# Iterate through the first 5 English and French sentences in the dataset\n","for en_sent, fr_sent in zip(en_text[:5], fr_text[:5]):  \n","  print(\"English: \", en_sent)\n","  print(\"\\tFrench: \", fr_sent)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["English:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n","\tFrench:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n","English:  the united states is usually chilly during july , and it is usually freezing in november .\n","\tFrench:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n","English:  california is usually quiet during march , and it is usually hot in june .\n","\tFrench:  california est généralement calme en mars , et il est généralement chaud en juin .\n","English:  the united states is sometimes mild during june , and it is cold in september .\n","\tFrench:  les états-unis est parfois légère en juin , et il fait froid en septembre .\n","English:  your least liked fruit is the grape , but my least liked is the apple .\n","\tFrench:  votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y0-J2TxrIfPr","colab_type":"text"},"source":["- Get the first English sentence from `en_text`."]},{"cell_type":"code","metadata":{"id":"4QOlyVSkIamL","colab_type":"code","outputId":"bcf490d1-57b0-4704-cc32-1a3b47838fcc","executionInfo":{"status":"ok","timestamp":1572782478707,"user_tz":-480,"elapsed":667,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Get the first sentence of the English dataset\n","first_sent = en_text[0]\n","print(\"First sentence: \", first_sent)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["First sentence:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nsoDO_tJIcnY","colab_type":"text"},"source":["- Tokenize the obtained sentence using the `split()` function and the space character and assign it to `first_words`.\n","- Print the tokenized words."]},{"cell_type":"code","metadata":{"id":"XET8sQq-IYjD","colab_type":"code","outputId":"3dda3b44-8d57-4c5a-85ba-3d8826765ef8","executionInfo":{"status":"ok","timestamp":1572782487157,"user_tz":-480,"elapsed":683,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Tokenize the first sentence\n","first_words = first_sent.split(\" \")\n","# Print the tokenized words\n","print(\"\\tWords: \", first_words)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\tWords:  ['new', 'jersey', 'is', 'sometimes', 'quiet', 'during', 'autumn', ',', 'and', 'it', 'is', 'snowy', 'in', 'april', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZgB8MUE7JKQ3","colab_type":"text"},"source":["For this exercise, the English dataset en_text containing a list of English sentences has been provided. In this exercise you will be using a Python list-related function called `<list>.extend()` which is a different variant of the function `<list>.append()`. "]},{"cell_type":"markdown","metadata":{"id":"oEXrTpF8JTY1","colab_type":"text"},"source":["- Compute the lengths of each sentence using the `split()` function and the `len()` function, while iterating through `en_text`.\n","- Compute the mean length of sentences using `numpy`."]},{"cell_type":"code","metadata":{"id":"nzLQLHRyWYvu","colab_type":"code","outputId":"9e28a2eb-60c0-4248-ea2a-56caf4bab414","executionInfo":{"status":"ok","timestamp":1572782602285,"user_tz":-480,"elapsed":680,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Compute length of sentences\n","sent_lengths = [len(en_sent.split(\" \")) for en_sent in en_text]\n","# Compute the mean of sentences lengths\n","mean_length = np.mean(sent_lengths)\n","print('(English) Mean sentence length: ', mean_length)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(English) Mean sentence length:  13.225589543090503\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZZpCOPGnJc9a","colab_type":"text"},"source":["- Populate the list all_words, in the for loop body, by adding in all the words found in sentences after tokenizing.\n","- Convert the list all_words, to a set object and compute the length/size of the set."]},{"cell_type":"code","metadata":{"id":"Moa1IjulJZGZ","colab_type":"code","outputId":"28e0cd27-6da1-4249-f782-b49b24b12280","executionInfo":{"status":"ok","timestamp":1572782605683,"user_tz":-480,"elapsed":1062,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["all_words = []\n","for sent in en_text:\n","  # Populate all_words with all the words in sentences\n","  all_words.extend(sent.split(\" \"))\n","# Compute the length of the set containing all_words\n","vocab_size = len(set(all_words))\n","print(\"(English) Vocabulary size: \", vocab_size)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(English) Vocabulary size:  228\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lzQH1wWKJi-9","colab_type":"text"},"source":["#### Defining the encoder\n","The encoder that you will implement is a very simple model compared to the complex models that are used in real-world applications such as the Google machine translation service. "]},{"cell_type":"code","metadata":{"id":"UzwUO6TUYiQe","colab_type":"code","outputId":"e9f1573b-f520-4303-81ca-6e5cce79ee98","executionInfo":{"status":"ok","timestamp":1572782704609,"user_tz":-480,"elapsed":883,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["import tensorflow.keras as keras\n","\n","en_len = 15\n","en_vocab = 150\n","hsize = 48\n","\n","# Define an input layer\n","en_inputs = keras.layers.Input(shape=(en_len, en_vocab))\n","# Define a GRU layer which returns the state\n","en_gru = keras.layers.GRU(hsize, return_state=True)\n","# Get the output and state from the GRU\n","en_out, en_state = en_gru(en_inputs)\n","# Define and print the model summary\n","encoder = keras.models.Model(inputs=en_inputs, outputs=en_state)\n","print(encoder.summary())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         [(None, 15, 150)]         0         \n","_________________________________________________________________\n","gru_5 (GRU)                  [(None, 48), (None, 48)]  28656     \n","=================================================================\n","Total params: 28,656\n","Trainable params: 28,656\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VgHywINuYmR-","colab_type":"text"},"source":["### Implementing the decoder\n"]},{"cell_type":"markdown","metadata":{"id":"2HIiIM5nJ9bw","colab_type":"text"},"source":["#### Understanding the RepeatVector layer\n","\n","The `RepeatVector` layer adds an extra dimension to your dataset. \n","\n","For example if you have an input of shape `(batch size, input size)` and you want to feed that to a `GRU` layer, you can use a `RepeatVector` layer to convert the input to a tensor with shape `(batch size, sequence length, input size)`."]},{"cell_type":"code","metadata":{"id":"vVNbtaEeYng_","colab_type":"code","outputId":"548a8bce-f515-4b2a-8f88-67d4224c493f","executionInfo":{"status":"ok","timestamp":1572782825518,"user_tz":-480,"elapsed":683,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from tensorflow.keras.layers import Input, RepeatVector\n","from tensorflow.keras.models import Model\n","import numpy as np\n","\n","inp = Input(shape=(2,))\n","# Define a RepeatVector that repeats the input 6 times\n","rep = RepeatVector(6)(inp)\n","# Define a model\n","model = Model(inputs=inp, outputs=rep)\n","# Define input x\n","x = np.array([[0,1],[2,3]])\n","# Get model prediction y\n","y = model.predict(x)\n","print('x.shape = ',x.shape,'\\ny.shape = ',y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["x.shape =  (2, 2) \n","y.shape =  (2, 6, 2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ukAvfNEOKaui","colab_type":"text"},"source":["#### Defining the decoder\n","\n","The decoder uses the same model as the encoder. \n","\n","However there are differences in the inputs and states fed to the decoder, compared to the encoder. \n","\n","For example, the decoder consumes the context vector produced by the encoder as inputs as well as the initial state to the decoder."]},{"cell_type":"code","metadata":{"id":"VrAQ5_VEKpo0","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import GRU\n","# from keras.layers import GRU # it's different from tensorflow.keras"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7wvnVFe4YiS2","colab_type":"code","outputId":"e6236b4e-686d-4261-d75c-64dd8e7fc7c1","executionInfo":{"status":"ok","timestamp":1572783183415,"user_tz":-480,"elapsed":647,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["from tensorflow.keras.layers import RepeatVector\n","\n","hsize = 48\n","fr_len = 20\n","# Define a RepeatVector layer\n","de_inputs = RepeatVector(fr_len)(en_state)\n","# Define a GRU model that returns all outputs\n","decoder_gru = GRU(hsize, return_sequences=True)\n","# Get the outputs of the decoder\n","gru_outputs = decoder_gru(de_inputs, initial_state=en_state)\n","# Define a model with the correct inputs and outputs\n","enc_dec = Model(inputs=en_inputs, outputs=gru_outputs)\n","enc_dec.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"model_4\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_4 (InputLayer)            [(None, 15, 150)]    0                                            \n","__________________________________________________________________________________________________\n","gru_5 (GRU)                     [(None, 48), (None,  28656       input_4[0][0]                    \n","__________________________________________________________________________________________________\n","repeat_vector_4 (RepeatVector)  (None, 20, 48)       0           gru_5[0][1]                      \n","__________________________________________________________________________________________________\n","gru_6 (GRU)                     (None, 20, 48)       13968       repeat_vector_4[0][0]            \n","                                                                 gru_5[0][1]                      \n","==================================================================================================\n","Total params: 42,624\n","Trainable params: 42,624\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gLJ1op9IYz5K","colab_type":"text"},"source":["### Dense and TimeDistributed layers\n"]},{"cell_type":"markdown","metadata":{"id":"ATVMc3H1L4pU","colab_type":"text"},"source":["#### Enter to win amazing prizes\n","\n","Imagine there's a game show where prizes are determined by a neural network. The contestant enters\n","\n","- the number of siblings,\n","- the number of coffees had today and\n","- if they like tomatoes or not,\n","\n","and the model predicts what the contestant will win.\n","\n","To implement this, you will be using `Keras`. You will need to create a model with an input layer which accepts three features (the number of siblings as an integer, the number of coffees as an integer and if they like tomatoes or not as a 0 or 1). Then the input goes through a Dense layer which outputs 3 probabilities (i.e. probabilities of winning a car, a gift voucher or nothing)."]},{"cell_type":"code","metadata":{"id":"TTuERqJpMTZD","colab_type":"code","outputId":"1ad0544b-7bd5-4d58-c2f0-c3f42e7ec93f","executionInfo":{"status":"ok","timestamp":1573200846005,"user_tz":-480,"elapsed":761,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["# pre-loaded\n","from tensorflow.python.keras.initializers import RandomNormal\n","init = RandomNormal()\n","init"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.initializers.RandomNormal at 0x7fd617745128>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"ZJCP4LVMYyV4","colab_type":"code","outputId":"0133c45a-3337-4cd4-bed1-cf1584d79448","executionInfo":{"status":"ok","timestamp":1573200857081,"user_tz":-480,"elapsed":1076,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["# Define an input layer with batch size 3 and input size 3\n","inp = Input(batch_shape=(3, 3))\n","# Get the output of the 3 node Dense layer\n","pred = Dense(3, activation='softmax', kernel_initializer=init, bias_initializer=init)(inp)\n","model = Model(inputs=inp, outputs=pred)\n","\n","names = [\"Mark\", \"John\", \"Kelly\"]\n","prizes = [\"Gift voucher\", \"Car\", \"Nothing\"]\n","x = np.array([[5, 0, 1], [0, 3, 1], [2, 2, 1]])\n","# Compute the model prediction for x\n","y = model.predict(x)\n","# Get the most probable class for each sample\n","classes = np.argmax(y, axis=-1)\n","print(\"\\n\".join([\"{} has probabilities {} and wins {}\".format(n,p,prizes[c]) \\\n","                 for n,p,c in zip(names, y, classes)]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Mark has probabilities [0.35032997 0.2268333  0.4228367 ] and wins Nothing\n","John has probabilities [0.38124028 0.3514808  0.26727897] and wins Gift voucher\n","Kelly has probabilities [0.3773006 0.3025755 0.3201239] and wins Gift voucher\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9UWRNC3vNySg","colab_type":"text"},"source":["This time, you will need to simulate multiple game shows hosted over several days. This means that your data will have a time dimension to it. More specifically, your data will have the `shape (number of contestants, game shows, inputs size)`.\n","\n","You will need to extend your model to incorporate this new feature. For this you will be using a `TimeDistributed` layer to allow the Dense layer to accept contestants from multiple game shows.\n","\n","You have been provided with the weight initializer `init`, the prizes list from the previous exercise, a time-series input `x` and names which contains the names of the contestants. `x` is a `(3,2,3)` numpy array where names is a `(2,3)` Python list. \n","\n","In other words, you have 2 game shows (i.e. sequence length), each with 3 contestants (batch size) where each contestant has 3 attributes (input size)."]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"3ad7497d-eb6e-48b8-f0a1-a82f8ab74d44","executionInfo":{"status":"ok","timestamp":1573201003374,"user_tz":-480,"elapsed":699,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"id":"UwmK62-wFjyG","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# pre-loaded weight initialiezer\n","from tensorflow.python.keras.initializers import RandomNormal\n","init = RandomNormal()\n","init"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.initializers.RandomNormal at 0x7fd60e80c5f8>"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"UuhocDoTFYod","colab_type":"code","colab":{}},"source":["names = [['Mark', 'John', 'Kelly'], ['Jenny', 'Shan', 'Sarah']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cyvvCkk6FdEp","colab_type":"code","colab":{}},"source":["prizes = ['Gift voucher', 'Car', 'Nothing']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ITPPWLIHNzx7","colab_type":"code","outputId":"c185ae73-41e2-4a27-e1ff-52e092ab446a","executionInfo":{"status":"ok","timestamp":1573200944100,"user_tz":-480,"elapsed":686,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["x = np.array([[[5, 0, 1],\n","        [1, 1, 0]],\n","\n","       [[0, 3, 1],\n","        [0, 4, 0]],\n","\n","       [[2, 2, 1],\n","        [6, 0, 1]]])\n","x"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[5, 0, 1],\n","        [1, 1, 0]],\n","\n","       [[0, 3, 1],\n","        [0, 4, 0]],\n","\n","       [[2, 2, 1],\n","        [6, 0, 1]]])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"74QuYKTFY4Fg","colab_type":"code","outputId":"a0becb33-0631-4174-cd2f-336dc24b787c","executionInfo":{"status":"ok","timestamp":1573201007197,"user_tz":-480,"elapsed":696,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["# Print names and x\n","print('names=\\n',names, '\\nx=\\n',x, '\\nx.shape=', x.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["names=\n"," [['Mark', 'John', 'Kelly'], ['Jenny', 'Shan', 'Sarah']] \n","x=\n"," [[[5 0 1]\n","  [1 1 0]]\n","\n"," [[0 3 1]\n","  [0 4 0]]\n","\n"," [[2 2 1]\n","  [6 0 1]]] \n","x.shape= (3, 2, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"blv_lYIIFreQ","colab_type":"text"},"source":["- Create a `TimeDistributed` layer which wraps a 3 node `Dense` layer with softmax activation.\n","- Get the most probable classes for all the samples of the predictions.\n","- In the second `for` loop, provide the `t`th slice of `y` and classes (i.e. information of the `t`th game show)."]},{"cell_type":"code","metadata":{"id":"imOvlSD8Y6PD","colab_type":"code","outputId":"c5678826-0493-45c8-9484-7649f4775b6c","executionInfo":{"status":"ok","timestamp":1573201017634,"user_tz":-480,"elapsed":685,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["inp = Input(shape=(2, 3))\n","# Create the TimeDistributed layer (the output of the Dense layer)\n","dense_time = TimeDistributed(Dense(3, activation='softmax', kernel_initializer=init, bias_initializer=init))\n","pred = dense_time(inp)\n","model = Model(inputs=inp, outputs=pred)\n","\n","y = model.predict(x)\n","# Get the most probable class for each sample\n","classes = np.argmax(y, axis=-1)\n","for t in range(2):\n","  # Get the t-th time-dimension slice of y and classes\n","  for n, p, c in zip(names[t], y[:,t,:], classes[:,t]):\n","  \tprint(\"Game {}: {} has probs {} and wins {}\\n\".format(t+1,n,p,prizes[c]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Game 1: Mark has probs [0.23264293 0.4425477  0.3248093 ] and wins Car\n","\n","Game 1: John has probs [0.2933786  0.36400327 0.3426182 ] and wins Car\n","\n","Game 1: Kelly has probs [0.26747108 0.39736548 0.33516338] and wins Car\n","\n","Game 2: Jenny has probs [0.30025524 0.35210475 0.34763998] and wins Car\n","\n","Game 2: Shan has probs [0.3024291  0.3636431  0.33392778] and wins Car\n","\n","Game 2: Sarah has probs [0.21868774 0.46581194 0.31550026] and wins Car\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MHRvj2AQp4Qq","colab_type":"text"},"source":["Dimension:\n","\n","In other words, you have 2 game shows (i.e. sequence length), each with 3 contestants (batch size) where each contestant has 3 attributes (input size)."]},{"cell_type":"markdown","metadata":{"id":"mDhAz_wrY_FA","colab_type":"text"},"source":["### Implementing the full encoder decoder model\n"]},{"cell_type":"markdown","metadata":{"id":"pAPvS0C83hB3","colab_type":"text"},"source":["- encoder"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"956ded06-86bb-4368-dd51-9d7702748a79","executionInfo":{"status":"ok","timestamp":1573264747825,"user_tz":-480,"elapsed":738,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"id":"XxLPZT2O3gZX","colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["import tensorflow.keras as keras\n","\n","en_len = 20\n","en_vocab = 200\n","hsize = 64\n","\n","# Define an input layer\n","en_inputs = keras.layers.Input(shape=(en_len, en_vocab))\n","# Define a GRU layer which returns the state\n","en_gru = keras.layers.GRU(hsize, return_state=True)\n","# Get the output and state from the GRU\n","en_out, en_state = en_gru(en_inputs)\n","# Define and print the model summary\n","encoder = keras.models.Model(inputs=en_inputs, outputs=en_state)\n","print(encoder.summary())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"model_6\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_5 (InputLayer)         [(None, 20, 200)]         0         \n","_________________________________________________________________\n","gru_8 (GRU)                  [(None, 64), (None, 64)]  50880     \n","=================================================================\n","Total params: 50,880\n","Trainable params: 50,880\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hZ0-s4umwqKy","colab_type":"text"},"source":["- decoder"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"a2VH64oW3TH4","colab":{}},"source":["from tensorflow.keras.layers import GRU\n","# from keras.layers import GRU # it's different from tensorflow.keras"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"7751b5d4-9ac5-40ea-ff6b-0f3932c11478","executionInfo":{"status":"ok","timestamp":1573264750397,"user_tz":-480,"elapsed":939,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"id":"w_hj8YpT3TH7","colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["from tensorflow.keras.layers import RepeatVector\n","\n","hsize = 64\n","fr_len = 25\n","# Define a RepeatVector layer\n","de_inputs = RepeatVector(fr_len)(en_state)\n","# Define a GRU model that returns all outputs\n","decoder_gru = GRU(hsize, return_sequences=True)\n","# Get the outputs of the decoder\n","de_out = decoder_gru(de_inputs, initial_state=en_state)\n","# Define a model with the correct inputs and outputs\n","enc_dec = Model(inputs=en_inputs, outputs=de_out)\n","enc_dec.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"model_7\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_5 (InputLayer)            [(None, 20, 200)]    0                                            \n","__________________________________________________________________________________________________\n","gru_8 (GRU)                     [(None, 64), (None,  50880       input_5[0][0]                    \n","__________________________________________________________________________________________________\n","repeat_vector_6 (RepeatVector)  (None, 25, 64)       0           gru_8[0][1]                      \n","__________________________________________________________________________________________________\n","gru_9 (GRU)                     (None, 25, 64)       24768       repeat_vector_6[0][0]            \n","                                                                 gru_8[0][1]                      \n","==================================================================================================\n","Total params: 75,648\n","Trainable params: 75,648\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r7TzcsAD3o4Z","colab_type":"code","outputId":"e0af9b99-e324-4588-c3a9-e7bdd0ec90a9","executionInfo":{"status":"ok","timestamp":1573264752794,"user_tz":-480,"elapsed":720,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["de_out"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'gru_9/transpose_1:0' shape=(?, 25, 64) dtype=float32>"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wiZMlKcS4QuN"},"source":["- GRU layer"]},{"cell_type":"markdown","metadata":{"id":"IYu-jA4_vyhi","colab_type":"text"},"source":["... ready to implement..."]},{"cell_type":"code","metadata":{"id":"l7ax75_zF8P5","colab_type":"code","outputId":"bb9468e1-bef2-4f2e-e271-d2553ae15452","executionInfo":{"status":"ok","timestamp":1573264753775,"user_tz":-480,"elapsed":513,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from tensorflow.python.keras.layers.recurrent import GRU\n","GRU"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensorflow.python.keras.layers.recurrent.GRU"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"OFL228Wl4ekb","colab_type":"code","colab":{}},"source":["fr_vocab = 250"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CFxE44HPZAFI","colab_type":"code","outputId":"8c263066-f21a-482e-bc20-e3b0aa38077d","executionInfo":{"status":"ok","timestamp":1573264756333,"user_tz":-480,"elapsed":783,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Import Dense and TimeDistributed layers\n","from tensorflow.keras.layers import Dense, TimeDistributed\n","# Define a softmax dense layer that has fr_vocab outputs\n","de_dense = Dense(fr_vocab, activation='softmax')\n","# Wrap the dense layer in a TimeDistributed layer\n","de_dense_time = TimeDistributed(de_dense)\n","# Get the final prediction of the model\n","de_pred = de_dense_time(de_out)\n","print(\"Prediction shape: \", de_pred.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Prediction shape:  (?, 25, 250)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lOjqE6Z94lZI","colab_type":"text"},"source":["#### Defining the full model"]},{"cell_type":"code","metadata":{"id":"9Fg9Ge0V4pOw","colab_type":"code","outputId":"9a588962-43be-4d54-e4b3-1ea22f27f94e","executionInfo":{"status":"ok","timestamp":1573264757251,"user_tz":-480,"elapsed":757,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["en_inputs"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'input_5:0' shape=(?, 20, 200) dtype=float32>"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"K7e9sELf4zEB","colab_type":"code","outputId":"5555af92-c67e-4db1-9004-b9a18497e860","executionInfo":{"status":"ok","timestamp":1573264766927,"user_tz":-480,"elapsed":701,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["en_out # different from datacamp"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'gru_8/strided_slice_12:0' shape=(?, 64) dtype=float32>"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"twL_qfH04-P6","colab_type":"code","outputId":"9104a405-639b-401a-a134-be808d22b2e4","executionInfo":{"status":"ok","timestamp":1573264813007,"user_tz":-480,"elapsed":724,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["en_state"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'gru_8/while/Exit_2:0' shape=(?, 64) dtype=float32>"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"NYj52jzA5I29","colab_type":"code","outputId":"2a2505a4-1c17-4a61-9dfc-ff45b57d6e3d","executionInfo":{"status":"ok","timestamp":1573264862457,"user_tz":-480,"elapsed":733,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["de_pred"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'time_distributed_1/Reshape_1:0' shape=(?, 25, 250) dtype=float32>"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"c03-Z7TPZ9Bg","colab_type":"code","outputId":"eaa6d729-4935-4db2-d4c5-7381e92f5745","executionInfo":{"status":"ok","timestamp":1573264886456,"user_tz":-480,"elapsed":698,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["from tensorflow.keras.models import Model\n","# Define a model with encoder input and decoder output\n","nmt = Model(inputs=en_inputs, outputs=de_pred)\n","\n","# Compile the model with an optimizer and a loss\n","nmt.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n","\n","# View the summary of the model \n","nmt.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"model_8\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_5 (InputLayer)            [(None, 20, 200)]    0                                            \n","__________________________________________________________________________________________________\n","gru_8 (GRU)                     [(None, 64), (None,  50880       input_5[0][0]                    \n","__________________________________________________________________________________________________\n","repeat_vector_6 (RepeatVector)  (None, 25, 64)       0           gru_8[0][1]                      \n","__________________________________________________________________________________________________\n","gru_9 (GRU)                     (None, 25, 64)       24768       repeat_vector_6[0][0]            \n","                                                                 gru_8[0][1]                      \n","__________________________________________________________________________________________________\n","time_distributed_1 (TimeDistrib (None, 25, 250)      16250       gru_9[0][0]                      \n","==================================================================================================\n","Total params: 91,898\n","Trainable params: 91,898\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NxzuAy9HWY1w","colab_type":"text"},"source":["## 3. Training and generating translations\n","\n","In this chapter, you will train the previously defined model and then use a well-trained model to generate translations. You will see that our model does a good job when translating sentences."]},{"cell_type":"markdown","metadata":{"id":"Zp1lqOw5aCyY","colab_type":"text"},"source":["### Part 1: Preprocessing the Data\n"]},{"cell_type":"markdown","metadata":{"id":"frYoBPzj5bLe","colab_type":"text"},"source":["#### Tokenizing sentences with Keras\n"]},{"cell_type":"code","metadata":{"id":"-NkUoITo53gR","colab_type":"code","outputId":"bb8d75f7-d571-4dd8-af05-a1f32e2684c8","executionInfo":{"status":"ok","timestamp":1573265107721,"user_tz":-480,"elapsed":879,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# datacamp subsample the data\n","en_text = en_text[:10000]\n","len(en_text)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"mGFKu2IMaAPz","colab_type":"code","outputId":"ab5a0022-1140-42a9-d744-a6c88b024994","executionInfo":{"status":"ok","timestamp":1573265111770,"user_tz":-480,"elapsed":800,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Define a Keras Tokenizer\n","en_tok = Tokenizer()\n","\n","# Fit the tokenizer on some text\n","en_tok.fit_on_texts(en_text)\n","\n","for w in [\"january\", \"apples\", \"summer\"]:\n","  # Get the word ID of word w\n","  id = en_tok.word_index[w]\n","  # Print the word and the word ID\n","  print(w, \" has id: \", id)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["january  has id:  51\n","apples  has id:  80\n","summer  has id:  30\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ct9HQbDb6MO6","colab_type":"text"},"source":["#### Controlling the vocabulary with the Tokenizer"]},{"cell_type":"code","metadata":{"id":"29VBKaZuaGTC","colab_type":"code","outputId":"fa176d2f-3717-4542-f6fc-31557770a3b6","executionInfo":{"status":"ok","timestamp":1573267294588,"user_tz":-480,"elapsed":1662,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# Convert the sentence to a word ID sequence\n","seq = en_tok.texts_to_sequences(['she likes grapefruit , peaches , and lemons .'])\n","print('Word ID sequence: ', seq)\n","\n","# Define a tokenizer with vocabulary size 50 and oov_token 'UNK'\n","en_tok_new = Tokenizer(num_words=50, oov_token='UNK')\n","\n","# Fit the tokenizer on en_text\n","en_tok_new.fit_on_texts(en_text)\n","\n","# Convert the sentence to a word ID sequence\n","seq_new = en_tok_new.texts_to_sequences(['she likes grapefruit , peaches , and lemons .'])\n","print('Word ID sequence (with UNK): ', seq_new)\n","print('The ID 1 represents the word: ', en_tok_new.index_word[1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Word ID sequence:  [[26, 70, 27, 73, 7, 74]]\n","Word ID sequence (with UNK):  [[27, 1, 28, 1, 8, 1]]\n","The ID 1 represents the word:  UNK\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qjbkkrA5aKby","colab_type":"text"},"source":["###Part 2: Preprocessing the Data\n"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"d4ba0bc7-d4a5-475b-c844-9914042285c4","executionInfo":{"status":"ok","timestamp":1573267331087,"user_tz":-480,"elapsed":969,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"id":"adlTX-1tCShu","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# datacamp subsample the data\n","fr_text = fr_text[:10]\n","len(fr_text)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"K5hbt0XlCl6a","colab_type":"code","outputId":"b411c0d0-a242-423e-cfb5-91234786aed5","executionInfo":{"status":"ok","timestamp":1573267337725,"user_tz":-480,"elapsed":709,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["fr_text[-1:]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .']"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"id":"ib8YgqFwWb13","colab_type":"code","outputId":"9c10243c-1f51-48cc-bd80-05b25a5aeea2","executionInfo":{"status":"ok","timestamp":1573267352981,"user_tz":-480,"elapsed":708,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":527}},"source":["fr_text_new = []\n","\n","# Loop through all sentences in fr_text\n","for sent in (fr_text):\n","  \n","  print(\"Before adding tokens: \", sent)\n","  \n","  # Add sos and eos tokens\n","  sent_new = 'sos ' + sent + ' eos' # start token .. end token\n","  # Append the modified sentence to fr_text_new\n","  fr_text_new.append(sent_new)\n","  \n","  # Print sentence after adding tokens\n","  print(\"After adding tokens: \", sent_new, '\\n')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Before adding tokens:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n","After adding tokens:  sos new jersey est parfois calme pendant l' automne , et il est neigeux en avril . eos \n","\n","Before adding tokens:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n","After adding tokens:  sos les états-unis est généralement froid en juillet , et il gèle habituellement en novembre . eos \n","\n","Before adding tokens:  california est généralement calme en mars , et il est généralement chaud en juin .\n","After adding tokens:  sos california est généralement calme en mars , et il est généralement chaud en juin . eos \n","\n","Before adding tokens:  les états-unis est parfois légère en juin , et il fait froid en septembre .\n","After adding tokens:  sos les états-unis est parfois légère en juin , et il fait froid en septembre . eos \n","\n","Before adding tokens:  votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n","After adding tokens:  sos votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme . eos \n","\n","Before adding tokens:  son fruit préféré est l'orange , mais mon préféré est le raisin .\n","After adding tokens:  sos son fruit préféré est l'orange , mais mon préféré est le raisin . eos \n","\n","Before adding tokens:  paris est relaxant en décembre , mais il est généralement froid en juillet .\n","After adding tokens:  sos paris est relaxant en décembre , mais il est généralement froid en juillet . eos \n","\n","Before adding tokens:  new jersey est occupé au printemps , et il est jamais chaude en mars .\n","After adding tokens:  sos new jersey est occupé au printemps , et il est jamais chaude en mars . eos \n","\n","Before adding tokens:  notre fruit est moins aimé le citron , mais mon moins aimé est le raisin .\n","After adding tokens:  sos notre fruit est moins aimé le citron , mais mon moins aimé est le raisin . eos \n","\n","Before adding tokens:  les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n","After adding tokens:  sos les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre . eos \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Nzbr9TkdCtHp","colab_type":"text"},"source":["#### Padding sentences\n","\n","You will now implement a function called `sents2seqs()` which you will later use to transform data conveniently to the format accepted by the neural machine translation (NMT) model. `sents2seqs()` accepts a list of sentence strings and,\n","\n","- Converts the sentences to a list of sequence of IDs,\n","- Pad the sentences so that they have equal length and,\n","- Optionally convert the IDs to onehot vectors."]},{"cell_type":"code","metadata":{"id":"QivGUoLiaOca","colab_type":"code","colab":{}},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","\n","def sents2seqs(input_type, sentences, onehot=False, pad_type='post'):\n","\t# Convert sentences to sequences      \n","    encoded_text = en_tok.texts_to_sequences(sentences)\n","    # Pad sentences to en_len\n","    preproc_text = pad_sequences(encoded_text, padding=pad_type, truncating='post', maxlen=en_len)\n","    if onehot:\n","\t\t# Convert the word IDs to onehot vectors\n","        preproc_text = to_categorical(preproc_text, num_classes=en_vocab)\n","    return preproc_text\n","sentence = 'she likes grapefruit , peaches , and lemons .'  \n","# Convert a sentence to sequence by pre-padding the sentence\n","pad_seq = sents2seqs('source', [sentence], pad_type='pre')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qCaBusXKC64O","colab_type":"text"},"source":["#### Reversing sentences\n"]},{"cell_type":"code","metadata":{"id":"mni1W8bsaT5-","colab_type":"code","outputId":"a9511949-292a-40b5-a443-981fed257e0d","executionInfo":{"status":"ok","timestamp":1573267523427,"user_tz":-480,"elapsed":728,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["sentences = [\"california is never rainy during july .\"]\n","# Add new keyword parameter reverse which defaults to False\n","def sents2seqs(input_type, sentences, onehot=False, pad_type='post', reverse=False):     \n","    encoded_text = en_tok.texts_to_sequences(sentences)\n","    preproc_text = pad_sequences(encoded_text, padding=pad_type, truncating='post', maxlen=en_len)\n","    if reverse:\n","      # Reverse the text using numpy axis reversing\n","      preproc_text = preproc_text[:,::-1]\n","    if onehot:\n","        preproc_text = to_categorical(preproc_text, num_classes=en_vocab)\n","    return preproc_text\n","# Call sents2seqs to get the padded and reversed sequence of IDs\n","pad_seq = sents2seqs('source', sentences, reverse=True)\n","rev_sent = [en_tok.index_word[wid] for wid in pad_seq[0][-6:]] \n","print('\\tReversed: ',' '.join(rev_sent))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\tReversed:  july during rainy never is california\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6qbkn592aXq0","colab_type":"text"},"source":["### Training the NMT model\n"]},{"cell_type":"code","metadata":{"id":"8_ZU0-jQJ8cw","colab_type":"code","colab":{}},"source":["# pre-loaded\n","def sents2seqs(input_type, sentences, onehot=False, pad_type='post', reverse=False):\n","    assert input_type in [\"source\", \"target\"]\n","    if input_type == 'source':\n","      tokenizer = en_tok\n","      pad_length = en_len\n","      vocab_size = en_vocab\n","    elif input_type == 'target':\n","      tokenizer = fr_tok\n","      pad_length = fr_len\n","      vocab_size = fr_vocab\n","    \n","    encoded_text = tokenizer.texts_to_sequences(sentences)\n","    preproc_text = pad_sequences(encoded_text, padding=pad_type, truncating='post', maxlen=pad_length)\n","    if reverse:\n","      preproc_text = preproc_text[:,::-1]\n","      \n","    if onehot:\n","        assert vocab_size is not None, \"Cannot do to_categorical without num_classes for safety\"\n","        preproc_text = to_categorical(preproc_text, num_classes=vocab_size)\n","    return preproc_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRseTKSnPPOy","colab_type":"code","outputId":"4854f923-2237-4c6a-9fbc-0f93a5724882","executionInfo":{"status":"ok","timestamp":1573270651097,"user_tz":-480,"elapsed":2379,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# pre-loaded\n","fr_tok = Tokenizer()\n","fr_tok"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<keras_preprocessing.text.Tokenizer at 0x7f42796a6400>"]},"metadata":{"tags":[]},"execution_count":105}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"a_XTwHtbDVsM","colab":{}},"source":["# load text file into a list\n","text_file = open(\"vocab_en.txt\", \"r\")\n","en_text = text_file.read().split('\\n')\n","text_file.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Udxbh1yJDVsP","colab":{}},"source":["text_file = open(\"vocab_fr.txt\", \"r\")\n","fr_text = text_file.read().split('\\n')\n","text_file.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sXDXhVjIDaMO","colab_type":"text"},"source":["- For this exercise you will be using a small dataset of 1500 sentences (i.e. en_text and fr_text) to train the model. "]},{"cell_type":"code","metadata":{"id":"ErQTlEcwDbBm","colab_type":"code","colab":{}},"source":["# trim dataset\n","en_text = en_text[:1500]\n","fr_text = fr_text[:1500]\n","data_size = 1500"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l4ZvJ_ZDaOU1","colab_type":"code","outputId":"26d58926-64df-4bb1-8e93-cadde72b7fcd","executionInfo":{"status":"ok","timestamp":1573270669117,"user_tz":-480,"elapsed":9370,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":394}},"source":["n_epochs, bsize = 3, 250\n","\n","for ei in range(n_epochs):\n","  for i in range(0,data_size,bsize):\n","    # Get a single batch of encoder inputs\n","    en_x = sents2seqs('source', en_text[i:i+bsize], onehot=True, reverse=True)\n","    # Get a single batch of decoder outputs\n","    de_y = sents2seqs('target', fr_text[i:i+bsize], onehot=True)\n","    \n","    # Train the model on a single batch of data\n","    nmt.train_on_batch(en_x, de_y)    \n","    # Obtain the eval metrics for the training data\n","    res = nmt.evaluate(en_x, de_y, batch_size=bsize, verbose=0)\n","    print(\"{} => Train Loss:{}, Train Acc: {}\".format(ei+1,res[0], res[1]*100.0))  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","1 => Train Loss:5.505444526672363, Train Acc: 0.015999999595806003\n","1 => Train Loss:5.486504554748535, Train Acc: 11.135999858379364\n","1 => Train Loss:5.466458797454834, Train Acc: 33.40800106525421\n","1 => Train Loss:5.448673725128174, Train Acc: 65.26399850845337\n","1 => Train Loss:5.424208164215088, Train Acc: 93.91999840736389\n","1 => Train Loss:5.4000468254089355, Train Acc: 98.72000217437744\n","2 => Train Loss:5.3701982498168945, Train Acc: 98.75199794769287\n","2 => Train Loss:5.331777095794678, Train Acc: 99.58400130271912\n","2 => Train Loss:5.290883541107178, Train Acc: 99.7439980506897\n","2 => Train Loss:5.249695777893066, Train Acc: 99.85600113868713\n","2 => Train Loss:5.186411380767822, Train Acc: 99.91999864578247\n","2 => Train Loss:5.121513843536377, Train Acc: 100.0\n","3 => Train Loss:5.036438465118408, Train Acc: 100.0\n","3 => Train Loss:4.925676345825195, Train Acc: 100.0\n","3 => Train Loss:4.80191707611084, Train Acc: 100.0\n","3 => Train Loss:4.6670756340026855, Train Acc: 100.0\n","3 => Train Loss:4.4661760330200195, Train Acc: 100.0\n","3 => Train Loss:4.246940612792969, Train Acc: 100.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KVm9BopRHosW","colab":{}},"source":["# match dataset to 1000\n","en_text = en_text[:1000]\n","fr_text = fr_text[:1000]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yr6DOQr-abJ0","colab_type":"code","outputId":"38afd859-5742-4796-c533-26b4aa713ab6","executionInfo":{"status":"ok","timestamp":1573270704745,"user_tz":-480,"elapsed":467,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["train_size, valid_size = 800, 200\n","\n","# Define a sequence of indices from 0 to len(en_text)\n","inds = np.arange(len(en_text))\n","np.random.shuffle(inds)\n","train_inds = inds[:train_size]\n","\n","# Define valid_inds: last valid_size indices\n","valid_inds = inds[train_size:train_size+valid_size]\n","\n","# Define tr_en (train EN sentences) and tr_fr (train FR sentences)\n","tr_en = [en_text[ti] for ti in train_inds]\n","tr_fr = [fr_text[ti] for ti in train_inds]\n","\n","# Define v_en (valid EN sentences) and v_fr (valid FR sentences)\n","v_en = [en_text[vi] for vi in valid_inds]\n","v_fr = [fr_text[vi] for vi in valid_inds]\n","print('Training (EN):\\n', tr_en[:3], '\\nTraining (FR):\\n', tr_fr[:3])\n","print('\\nValid (EN):\\n', v_en[:3], '\\nValid (FR):\\n', v_fr[:3])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training (EN):\n"," ['they like lemons , limes , and grapefruit .', 'where is the united states ?', 'india is never cold during april , but it is never nice in october .'] \n","Training (FR):\n"," ['ils aiment les citrons , citrons verts et le pamplemousse .', 'où est les États-unis ?', \"l' inde est jamais froid en avril , mais il est jamais agréable en octobre .\"]\n","\n","Valid (EN):\n"," ['the united states is never nice during february , but it is sometimes pleasant in april .', 'india is usually quiet during june , but it is never snowy in december .', 'the lime is their least liked fruit , but the grape is our least liked.'] \n","Valid (FR):\n"," ['les états-unis est jamais agréable en février , mais il est parfois agréable en avril .', \"l' inde est généralement calme en juin , mais jamais de neige en décembre .\", 'la chaux est leur fruit moins aimé , mais le raisin est notre moins aimé .']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MdVZ_trca9FI","colab_type":"code","outputId":"fcacf975-f8ad-41e5-c09a-66efd5990ab1","executionInfo":{"status":"ok","timestamp":1573270708958,"user_tz":-480,"elapsed":2788,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# Convert validation data to onehot\n","v_en_x = sents2seqs('source', v_en, onehot=True, reverse=True)\n","v_de_y = sents2seqs('target', v_fr, onehot=True)\n","\n","n_epochs, bsize = 3, 250\n","for ei in range(n_epochs):\n","  for i in range(0,train_size,bsize):\n","    # Get a single batch of inputs and outputs\n","    en_x = sents2seqs('source', tr_en[i:i+bsize], onehot=True, reverse=True)\n","    de_y = sents2seqs('target', tr_fr[i:i+bsize], onehot=True)\n","    # Train the model on a single batch of data\n","    nmt.train_on_batch(en_x, de_y)    \n","  # Evaluate the trained model on the validation data\n","  res = nmt.evaluate(v_en_x, v_de_y, batch_size=valid_size, verbose=0)\n","  print(\"{} => Loss:{}, Val Acc: {}\".format(ei+1,res[0], res[1]*100.0))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1 => Loss:2.7142276763916016, Val Acc: 100.0\n","2 => Loss:0.913616418838501, Val Acc: 100.0\n","3 => Loss:0.2680945098400116, Val Acc: 100.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qTW5JkB-bFZI","colab_type":"text"},"source":["###Generating translations with the NMT\n"]},{"cell_type":"markdown","metadata":{"id":"D0IuPgrVIBVK","colab_type":"text"},"source":["#### Part 1: Treasure hunt\n","You recently won a all-paid trip to a lush tropical island. While you were wandering around, you found an ancient treasure map pointing to a great treasure, which had a few secret messages written using 1s and 0s. Having just taken this course, you instantly recognize that it is a `sequence of onehot encoded vectors`. You have also been lucky to find the word to index mapping to know which word refers to which ID.\n","\n","Now you need to decrypt the secret message and find out what this map is saying. You have been provided with a `treasure_map` which is a `number of sentences` by `number of words` by `onehot vector length` matrix. You have also been provided with the `index2word` Python dictionary that maps an ID to a word."]},{"cell_type":"code","metadata":{"id":"WGR4TdtMIRHE","colab_type":"code","colab":{}},"source":["treasure_map = np.array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n","        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n","\n","       [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n","\n","       [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n","        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n","        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=np.float32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H4WrRenQIZwX","colab_type":"code","colab":{}},"source":["index2word = {0: 'PAD',\n"," 1: 'go',\n"," 2: 'to',\n"," 3: 'the',\n"," 4: 'cave',\n"," 5: 'find',\n"," 6: 'stone',\n"," 7: 'dragon',\n"," 8: 'push',\n"," 9: 'head',\n"," 10: 'down'}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gko49IfnwPqY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"a9a90776-bb56-4c9d-b440-63e884833a59","executionInfo":{"status":"ok","timestamp":1573363204477,"user_tz":-480,"elapsed":732,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}}},"source":["# display output for intermediate step\n","np.argmax(treasure_map, axis=-1)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1,  2,  3,  7,  4],\n","       [ 5,  3,  6,  7,  0],\n","       [ 8,  3,  9, 10,  0]])"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"R4LwPTnIbA4A","colab_type":"code","outputId":"33909d99-735c-4781-ad23-347e61b04ace","executionInfo":{"status":"ok","timestamp":1573268865166,"user_tz":-480,"elapsed":701,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# Get the word IDs from the treasure map\n","word_ids = np.argmax(treasure_map, axis=-1)\n","# Get the sequence length from the treasure map\n","seq_len = treasure_map.shape[1]\n","\n","for i in range(treasure_map.shape[0]):\n","\twords = []\n","\tfor t in range(seq_len):\n","      \t# Get the word ID for the i-th sentence and t-th position\n","\t    wid = word_ids[i,t]\n","\t    if wid != 0:\n","          \t# Append the word corresponding to wid\n","\t        words.append(index2word[wid])\n","\tprint(\"Instruction \", i+1, \": \", ' '.join(words))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Instruction  1 :  go to the dragon cave\n","Instruction  2 :  find the stone dragon\n","Instruction  3 :  push the head down\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4iP3Ab_5bJGy","colab_type":"code","outputId":"2d586fdf-bd1e-478f-f922-120a03d676dc","executionInfo":{"status":"ok","timestamp":1573268879298,"user_tz":-480,"elapsed":747,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# Get the word IDs from the treasure map\n","word_ids = np.argmax(treasure_map, axis=-1)\n","# Get the batch size from the treasure map\n","for i in range(treasure_map.shape[0]):\n","  \t# Get all the words of the i-th sentence using list comprehension\n","\twords = [index2word[wid] for wid in word_ids[i] if wid != 0]\n","\tprint(\"Instruction \", i+1, \": \", ' '.join(words))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Instruction  1 :  go to the dragon cave\n","Instruction  2 :  find the stone dragon\n","Instruction  3 :  push the head down\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lidNMhJVIg7A","colab_type":"text"},"source":["####Generating English-French translations"]},{"cell_type":"markdown","metadata":{"id":"JAflitw5QtDF","colab_type":"text"},"source":["We will use the trained model to predict the French translation of an English sentence using `model.predict()`. You will be provided with the trained model (model). This model has been trained for 50 epochs on 100,000 sentences which achieved around 90% accuracy on a 35000+ word validation set."]},{"cell_type":"code","metadata":{"id":"YlEv9fprQwfg","colab_type":"code","cellView":"form","outputId":"840da5d2-8b98-4b52-e05d-b09bbbca86b4","executionInfo":{"status":"ok","timestamp":1573271084982,"user_tz":-480,"elapsed":1884,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@title\n","fr_id2word = {1: 'UNK',\n"," 2: 'est',\n"," 3: 'sos',\n"," 4: 'eos',\n"," 5: 'en',\n"," 6: 'il',\n"," 7: 'les',\n"," 8: 'mais',\n"," 9: 'et',\n"," 10: 'la',\n"," 11: 'parfois',\n"," 12: 'jamais',\n"," 13: 'le',\n"," 14: \"l'\",\n"," 15: 'généralement',\n"," 16: 'moins',\n"," 17: 'aimé',\n"," 18: 'au',\n"," 19: 'fruit',\n"," 20: 'préféré',\n"," 21: 'agréable',\n"," 22: 'froid',\n"," 23: 'son',\n"," 24: 'chaud',\n"," 25: 'de',\n"," 26: 'plus',\n"," 27: 'automne',\n"," 28: 'mois',\n"," 29: 'à',\n"," 30: 'elle',\n"," 31: 'citrons',\n"," 32: 'paris',\n"," 33: 'inde',\n"," 34: 'unis',\n"," 35: 'états',\n"," 36: 'france',\n"," 37: 'jersey',\n"," 38: 'new',\n"," 39: 'chine',\n"," 40: 'pendant',\n"," 41: 'pamplemousse',\n"," 42: 'mon',\n"," 43: 'votre',\n"," 44: 'juin',\n"," 45: 'hiver',\n"," 46: 'printemps',\n"," 47: 'mars',\n"," 48: 'janvier',\n"," 49: 'septembre',\n"," 50: 'février',\n"," 51: 'mai',\n"," 52: 'décembre',\n"," 53: 'été',\n"," 54: 'juillet',\n"," 55: 'novembre',\n"," 56: 'avril',\n"," 57: 'aime',\n"," 58: 'octobre',\n"," 59: 'août',\n"," 60: 'merveilleux',\n"," 61: 'relaxant',\n"," 62: 'humide',\n"," 63: 'doux',\n"," 64: 'notre',\n"," 65: 'californie',\n"," 66: 'sec',\n"," 67: 'leur',\n"," 68: 'pluvieux',\n"," 69: 'occupé',\n"," 70: 'calme',\n"," 71: 'beau',\n"," 72: 'habituellement',\n"," 73: 'oranges',\n"," 74: 'pêches',\n"," 75: 'fraises',\n"," 76: 'bananes',\n"," 77: 'poires',\n"," 78: 'raisins',\n"," 79: 'pommes',\n"," 80: 'mangues',\n"," 81: 'verts',\n"," 82: \"d'\",\n"," 83: 'raisin',\n"," 84: 'pomme',\n"," 85: 'citron',\n"," 86: 'poire',\n"," 87: 'mangue',\n"," 88: 'fraise',\n"," 89: \"l'orange\",\n"," 90: 'gel',\n"," 91: 'pêche',\n"," 92: 'chaux',\n"," 93: 'banane',\n"," 94: 'pas',\n"," 95: 'enneigée',\n"," 96: 'favori',\n"," 97: 'déteste',\n"," 98: 'gèle',\n"," 99: 'voiture',\n"," 100: 'fruits',\n"," 101: \"l'automne\",\n"," 102: 'ils',\n"," 103: \"n'aime\",\n"," 104: 'california',\n"," 105: 'neige',\n"," 106: 'fait',\n"," 107: 'belle',\n"," 108: 'ne',\n"," 109: 'vous',\n"," 110: 'nous',\n"," 111: 'des',\n"," 112: 'animal',\n"," 113: 'cours',\n"," 114: 'camion',\n"," 115: 'neigeux',\n"," 116: 'conduit',\n"," 117: 'prochain',\n"," 118: 'je',\n"," 119: 'ce',\n"," 120: 'tranquille',\n"," 121: 'a',\n"," 122: 'cher',\n"," 123: 'une',\n"," 124: 'cette',\n"," 125: 'était',\n"," 126: 'aller',\n"," 127: 'aiment',\n"," 128: 'chaude',\n"," 129: \"n'aimez\",\n"," 130: 'aimons',\n"," 131: \"n'aiment\",\n"," 132: 'aimez',\n"," 133: 'leurs',\n"," 134: 'détestons',\n"," 135: 'sont',\n"," 136: 'rouge',\n"," 137: \"j'aime\",\n"," 138: 'jaune',\n"," 139: 'visiter',\n"," 140: 'sèche',\n"," 141: 'occupée',\n"," 142: 'frisquet',\n"," 143: 'animaux',\n"," 144: 'préférée',\n"," 145: 'dernier',\n"," 146: 'aimait',\n"," 147: 'un',\n"," 148: 'que',\n"," 149: 'conduisait',\n"," 150: 'nouvelle',\n"," 151: 'vieille',\n"," 152: 'petite',\n"," 153: 'verte',\n"," 154: 'vu',\n"," 155: 'noire',\n"," 156: 'nos',\n"," 157: 'blanche',\n"," 158: 'redouté',\n"," 159: \"n'aimait\",\n"," 160: 'brillant',\n"," 161: 'entre',\n"," 162: 'pense',\n"," 163: 'pamplemousses',\n"," 164: 'pleut',\n"," 165: 'bleue',\n"," 166: 'traduire',\n"," 167: 'nouveau',\n"," 168: 'rouillée',\n"," 169: 'bleu',\n"," 170: 'grande',\n"," 171: 'se',\n"," 172: 'rouillé',\n"," 173: 'ses',\n"," 174: \"qu'il\",\n"," 175: 'aux',\n"," 176: 'brillante',\n"," 177: 'blanc',\n"," 178: 'préférés',\n"," 179: 'noir',\n"," 180: 'étaient',\n"," 181: 'vert',\n"," 182: 'va',\n"," 183: 'envisage',\n"," 184: 'rendre',\n"," 185: 'pluies',\n"," 186: 'petit',\n"," 187: 'vieux',\n"," 188: 'chinois',\n"," 189: 'espagnol',\n"," 190: 'français',\n"," 191: 'anglais',\n"," 192: 'mes',\n"," 193: 'portugais',\n"," 194: 'mouillé',\n"," 195: 'glaciales',\n"," 196: 'traduction',\n"," 197: 'cet',\n"," 198: 'amusant',\n"," 199: 'comme',\n"," 200: 'facile',\n"," 201: 'automobile',\n"," 202: 'gros',\n"," 203: 'pourrait',\n"," 204: 'difficile',\n"," 205: 'veut',\n"," 206: 'voulait',\n"," 207: 'aimés',\n"," 208: 'pourquoi',\n"," 209: 'prévois',\n"," 210: 'souris',\n"," 211: 'prévoyons',\n"," 212: 'vos',\n"," 213: 'intention',\n"," 214: 'lapin',\n"," 215: 'ours',\n"," 216: 'clémentes',\n"," 217: 'singe',\n"," 218: 'lion',\n"," 219: 'serpent',\n"," 220: 'redoutés',\n"," 221: 'allé',\n"," 222: 'ont',\n"," 223: 'chat',\n"," 224: 'chien',\n"," 225: 'grosse',\n"," 226: 'cheval',\n"," 227: 'pluie',\n"," 228: 'requin',\n"," 229: 'trop',\n"," 230: 'monde',\n"," 231: 'maillot',\n"," 232: 'avez',\n"," 233: 'vont',\n"," 234: 'volant',\n"," 235: 'i',\n"," 236: 'allée',\n"," 237: 'allés',\n"," 238: 'veulent',\n"," 239: 'pourraient',\n"," 240: 'oiseau',\n"," 241: 'quand',\n"," 242: 'éléphant',\n"," 243: 'détendre',\n"," 244: 'aimée',\n"," 245: 'voulaient',\n"," 246: 'magnifique',\n"," 247: \"n'aimons\",\n"," 248: 'détestait',\n"," 249: \"l'automobile\",\n"," 250: 'gelé',\n"," 251: 'grand',\n"," 252: 'prévoit',\n"," 253: 'vers',\n"," 254: 'bien',\n"," 255: \"l'éléphant\",\n"," 256: 'chiens',\n"," 257: 'légère',\n"," 258: 'serpents',\n"," 259: 'lui',\n"," 260: 'comment',\n"," 261: 'cépage',\n"," 262: 'prévoient',\n"," 263: 'éléphants',\n"," 264: 'singes',\n"," 265: 'chats',\n"," 266: 'oiseaux',\n"," 267: 'requins',\n"," 268: \"l'école\",\n"," 269: 'chevaux',\n"," 270: 'visite',\n"," 271: \"l'ours\",\n"," 272: 'lions',\n"," 273: 'lapins',\n"," 274: \"l'épicerie\",\n"," 275: 'tour',\n"," 276: 'eiffel',\n"," 277: \"l'animal\",\n"," 278: 'terrain',\n"," 279: 'football',\n"," 280: 'lac',\n"," 281: \"l'oiseau\",\n"," 282: 'pensez',\n"," 283: 'allons',\n"," 284: \"n'est\",\n"," 285: 'pousse',\n"," 286: 'allez',\n"," 287: 'du',\n"," 288: 'temps',\n"," 289: 'at',\n"," 290: 'rouille',\n"," 291: 'peu',\n"," 292: 'sur',\n"," 293: \"qu'elle\",\n"," 294: 'petites',\n"," 295: 'voudrait',\n"," 296: 'êtes',\n"," 297: 'dernière',\n"," 298: 'vais',\n"," 299: 'manguiers',\n"," 300: 'frais',\n"," 301: 'détestez',\n"," 302: 'porcelaine',\n"," 303: 't',\n"," 304: 'préférées',\n"," 305: 'avons',\n"," 306: 'congélation',\n"," 307: \"c'est\",\n"," 308: 'aimeraient',\n"," 309: 'grandes',\n"," 310: 'proches',\n"," 311: 'durant',\n"," 312: 'dans',\n"," 313: 'où',\n"," 314: 'voulez',\n"," 315: 'douce',\n"," 316: 'limes',\n"," 317: \"n'a\",\n"," 318: 'plaît',\n"," 319: 'grosses',\n"," 320: 'enneigé',\n"," 321: 'envisagent',\n"," 322: 'petits',\n"," 323: 'grands',\n"," 324: 'mouillée',\n"," 325: 'tout',\n"," 326: 'conduite',\n"," 327: 'bénigne',\n"," 328: 'moindres',\n"," 329: \"n'êtes\",\n"," 330: 'redoutée',\n"," 331: 'etats',\n"," 332: 'tu',\n"," 333: 'qui',\n"," 334: 'vit',\n"," 335: 'gelés',\n"," 336: 'allions',\n"," 337: 'es',\n"," 338: 'trouvé',\n"," 339: 'ressort',\n"," 340: 'traduis',\n"," 341: 'souvent',\n"," 342: 'faire',\n"," 343: 'as',\n"," 344: 'moteur'}\n","print(\"fr_id2word is loaded...\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["fr_id2word is loaded...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XvwjCME7Q98-","colab_type":"text"},"source":["Furthermore, you will also be provided with a dictionary (`fr_id2word`) which you can use to convert word indices to words. "]},{"cell_type":"code","metadata":{"id":"ynqOHlvrbMHK","colab_type":"code","outputId":"70f3056b-72e2-4c3f-e537-cbe352c47cf0","executionInfo":{"status":"error","timestamp":1573270717372,"user_tz":-480,"elapsed":735,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":249}},"source":["en_st = ['the united states is sometimes chilly during december , but it is sometimes freezing in june .']\n","print('English: {}'.format(en_st))\n","\n","# Convert the English sentence to a sequence\n","en_seq = sents2seqs('source', en_st, onehot=True, reverse=True)\n","\n","# Predict probabilities of words using en_seq\n","fr_pred = model.predict(en_seq)\n","\n","# Get the sequence indices (max argument) of fr_pred\n","fr_seq = np.argmax(fr_pred, axis=-1)[0]\n","\n","# Convert the sequence of IDs to a sentence and print\n","fr_sent = [fr_id2word[i] for i in fr_seq if i != 0]\n","print(\"French (Custom): {}\".format(' '.join(fr_sent)))\n","print(\"French (Google Translate): les etats-unis sont parfois froids en décembre, mais parfois gelés en juin\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["English: ['the united states is sometimes chilly during december , but it is sometimes freezing in june .']\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-112-9d4c4312570f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Predict probabilities of words using en_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfr_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Get the sequence indices (max argument) of fr_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"0phFk33kWdo4","colab_type":"text"},"source":["## 4. Teacher Forcing and word embeddings\n","\n","In this chapter, you will learn about a technique known as Teacher Forcing, which enables translation models to be trained better and faster. Then you will learn how you can use word embeddings to make the model even better.\n"]},{"cell_type":"markdown","metadata":{"id":"me63R5fhbUc5","colab_type":"text"},"source":["### Introduction to Teacher Forcing\n"]},{"cell_type":"code","metadata":{"id":"GeLmXHUWIuc8","colab_type":"code","colab":{}},"source":["en_len = 20\n","fr_len = 25\n","en_vocab = 200\n","fr_vocab = 250\n","hsize = 64"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JPgOlwNMbP7O","colab_type":"code","colab":{}},"source":["# Import the layers submodule from keras\n","import tensorflow.keras.layers as layers\n","\n","en_inputs = layers.Input(shape=(en_len, en_vocab))\n","en_gru = layers.GRU(hsize, return_state=True)\n","# Get the encoder output and state\n","en_out, en_state = en_gru(en_inputs)\n","\n","# Define the decoder input layer\n","de_inputs = layers.Input(shape=(fr_len-1, fr_vocab))\n","de_gru = layers.GRU(hsize, return_sequences=True)\n","de_out = de_gru(de_inputs, initial_state=en_state)\n","# Define a TimeDistributed Dense softmax layer with fr_vocab nodes\n","de_dense = layers.TimeDistributed(layers.Dense(fr_vocab, activation='softmax'))\n","de_pred = de_dense(de_out)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zZkhbcDBbPwk","colab_type":"code","outputId":"451c87d5-7a26-4498-9188-98f150f37596","executionInfo":{"status":"ok","timestamp":1573270726817,"user_tz":-480,"elapsed":758,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["# Import the Keras Model object\n","from tensorflow.keras.models import Model\n","\n","# Define a model\n","nmt_tf = Model(inputs=[en_inputs, de_inputs], outputs=de_pred)\n","# Compile the model with optimizer and loss\n","nmt_tf.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n","# Print the summary of the model\n","nmt_tf.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"model_10\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_8 (InputLayer)            [(None, 20, 200)]    0                                            \n","__________________________________________________________________________________________________\n","input_9 (InputLayer)            [(None, 24, 250)]    0                                            \n","__________________________________________________________________________________________________\n","gru_12 (GRU)                    [(None, 64), (None,  50880       input_8[0][0]                    \n","__________________________________________________________________________________________________\n","gru_13 (GRU)                    (None, 24, 64)       60480       input_9[0][0]                    \n","                                                                 gru_12[0][1]                     \n","__________________________________________________________________________________________________\n","time_distributed_3 (TimeDistrib (None, 24, 250)      16250       gru_13[0][0]                     \n","==================================================================================================\n","Total params: 127,610\n","Trainable params: 127,610\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zWdwsHCBWhrX","colab_type":"code","outputId":"62cc2a79-9ac1-4b8e-95f4-019178c6bfc2","executionInfo":{"status":"ok","timestamp":1573270730442,"user_tz":-480,"elapsed":735,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["bsize = 250\n","for i in range(0, len(en_text), bsize):\n","  # Get the encoder inputs using the sents2seqs() function\n","  en_x = sents2seqs('source', en_text[i:i+bsize], onehot=True, reverse=True)\n","  # Get the decoder inputs/outputs using the sents2seqs() function\n","  de_xy = sents2seqs('target', fr_text[i:i+bsize], onehot=True)\n","  # Separate the decoder inputs from de_xy\n","  de_x = de_xy[:,:-1,:]\n","  # Separate the decoder outputs from de_xy\n","  de_y = de_xy[:,1:,:]\n","  \n","  print(\"Data from \", i, \" to \", i+bsize)\n","  print(\"\\tnp.argmax() => en_x[0]: \", np.argmax(en_x[0], axis=-1))\n","  print(\"\\tnp.argmax() => de_x[0]: \", np.argmax(de_x[0], axis=-1))\n","  print(\"\\tnp.argmax() => de_y[0]: \", np.argmax(de_y[0], axis=-1))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Data from  0  to  250\n","\tnp.argmax() => en_x[0]:  [ 0  0  0  0  0  0  0 53  2 50  1  3  7 45  4 31  9  1 19 17]\n","\tnp.argmax() => de_x[0]:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","\tnp.argmax() => de_y[0]:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","Data from  250  to  500\n","\tnp.argmax() => en_x[0]:  [ 0  0  0  0  0  0  0 64  2 65 10  1  3  7 47  4 68  9  1 25]\n","\tnp.argmax() => de_x[0]:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","\tnp.argmax() => de_y[0]:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","Data from  500  to  750\n","\tnp.argmax() => en_x[0]:  [ 0  0  0  0  0  0  0 37  2 29  9  1  3  6 40  4 46 10  1 20]\n","\tnp.argmax() => de_x[0]:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","\tnp.argmax() => de_y[0]:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","Data from  750  to  1000\n","\tnp.argmax() => en_x[0]:  [ 0  0  0  0  0 39  2 68 10  1  3  6 47  4 29 10  1 23 22  5]\n","\tnp.argmax() => de_x[0]:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","\tnp.argmax() => de_y[0]:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CpGJ3hIvbqbl","colab_type":"text"},"source":["### Training the model with Teacher Forcing\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"L-c2f9OEJsJQ","colab":{}},"source":["# load text file into a list\n","text_file = open(\"vocab_en.txt\", \"r\")\n","en_text = text_file.read().split('\\n')\n","text_file.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yiVC9ia0JsJU","colab":{}},"source":["text_file = open(\"vocab_fr.txt\", \"r\")\n","fr_text = text_file.read().split('\\n')\n","text_file.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"o9nps2ADJsJV","colab":{}},"source":["en_text = en_text[:1500]\n","fr_text = fr_text[:1500]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n_xrfHoCbo-2","colab_type":"code","outputId":"6ec351c5-e433-4757-9c6a-5e607d16edce","executionInfo":{"status":"ok","timestamp":1573270751425,"user_tz":-480,"elapsed":9129,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["n_epochs, bsize = 3, 250\n","\n","for ei in range(n_epochs):\n","  for i in range(0,data_size,bsize):\n","    en_x = sents2seqs('source', en_text[i:i+bsize], onehot=True, reverse=True)\n","    de_xy = sents2seqs('target', fr_text[i:i+bsize], onehot=True)\n","    # Separate the decoder inputs from de_xy\n","    de_x = de_xy[:,:-1,:]\n","    # Separate the decoder outputs from de_xy\n","    de_y = de_xy[:,1:,:]\n","    # Train the model on a single batch of data    \n","    nmt_tf.train_on_batch([en_x,de_x], de_y)    \n","    # Obtain the eval metrics for the training data\n","    res = nmt_tf.evaluate([en_x,de_x], de_y, batch_size=bsize, verbose=0)\n","    print(\"{} => Train Loss:{}, Train Acc: {}\".format(ei+1,res[0], res[1]*100.0))  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["1 => Train Loss:5.509411334991455, Train Acc: 0.0\n","1 => Train Loss:5.484952926635742, Train Acc: 0.0\n","1 => Train Loss:5.460376739501953, Train Acc: 0.0\n","1 => Train Loss:5.435689449310303, Train Acc: 0.3000000026077032\n","1 => Train Loss:5.410383701324463, Train Acc: 83.21666717529297\n","1 => Train Loss:5.384454250335693, Train Acc: 98.7833321094513\n","2 => Train Loss:5.357736110687256, Train Acc: 99.80000257492065\n","2 => Train Loss:5.32982873916626, Train Acc: 99.9666690826416\n","2 => Train Loss:5.300786972045898, Train Acc: 100.0\n","2 => Train Loss:5.270432949066162, Train Acc: 100.0\n","2 => Train Loss:5.237998008728027, Train Acc: 100.0\n","2 => Train Loss:5.203653812408447, Train Acc: 100.0\n","3 => Train Loss:5.16663932800293, Train Acc: 100.0\n","3 => Train Loss:5.126492977142334, Train Acc: 100.0\n","3 => Train Loss:5.083048343658447, Train Acc: 100.0\n","3 => Train Loss:5.0355119705200195, Train Acc: 100.0\n","3 => Train Loss:4.982267379760742, Train Acc: 100.0\n","3 => Train Loss:4.923271656036377, Train Acc: 100.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b9J4tClabtsW","colab_type":"code","outputId":"184f83cf-ef20-4a8b-e30a-81c976895530","executionInfo":{"status":"ok","timestamp":1573270754971,"user_tz":-480,"elapsed":724,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["train_size, valid_size = 800, 200\n","# Define a sequence of indices from 0 to size of en_text\n","inds = np.arange(len(en_text))\n","np.random.shuffle(inds)\n","# Define train_inds: first train_size indices\n","train_inds = inds[:train_size]\n","valid_inds = inds[train_size:train_size+valid_size]\n","# Define tr_en (train EN sentences) and tr_fr (train FR sentences)\n","tr_en = [en_text[ti] for ti in train_inds]\n","tr_fr = [fr_text[ti] for ti in train_inds]\n","# Define v_en (valid EN sentences) and v_fr (valid FR sentences)\n","v_en = [en_text[vi] for vi in valid_inds]\n","v_fr = [fr_text[vi] for vi in valid_inds]\n","print('Training (EN):\\n', tr_en[:3], '\\nTraining (FR):\\n', tr_fr[:3])\n","print('\\nValid (EN):\\n', v_en[:3], '\\nValid (FR):\\n', v_fr[:3])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training (EN):\n"," ['you dislike bananas , mangoes , and limes .', 'he dislikes grapefruit and strawberries .', 'new jersey is usually quiet during march , but it is never pleasant in autumn .'] \n","Training (FR):\n"," [\"vous n'aimez pas les bananes , les mangues et citrons verts .\", 'il aime pas le pamplemousse et les fraises .', \"new jersey est généralement calme au mois de mars , mais il est jamais agréable à l' automne .\"]\n","\n","Valid (EN):\n"," ['the peach is his least liked fruit , but the mango is their least liked .', 'my least liked fruit is the apple , but his least liked is the pear .', 'california is usually snowy during summer , but it is rainy in june .'] \n","Valid (FR):\n"," ['la pêche est son moins aimé des fruits , mais la mangue est leur moins aimé .', 'mon moins aimé fruit est la pomme , mais son moins aimé est la poire .', \"californie est généralement enneigée pendant l' été , mais il est pluvieux en juin .\"]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VfUSShZjb98Y","colab_type":"code","outputId":"5ab5eabc-bb71-47ca-8ed0-d8b3185dcf43","executionInfo":{"status":"ok","timestamp":1573270938279,"user_tz":-480,"elapsed":3170,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["for ei in range(n_epochs):\n","  for i in range(0,train_size,bsize):    \n","    en_x = sents2seqs('source', tr_en[i:i+bsize], onehot=True, reverse=True)\n","    de_xy = sents2seqs('target', tr_fr[i:i+bsize], onehot=True)\n","    # Create a single batch of decoder inputs and outputs\n","    de_x, de_y = de_xy[:,:-1,:], de_xy[:,1:,:]\n","    # Train the model on a single batch of data\n","    nmt_tf.train_on_batch([en_x, de_x], de_y)      \n","  v_en_x = sents2seqs('source', v_en, onehot=True, reverse=True)\n","  # Create a single batch of validation decoder inputs and outputs\n","  v_de_xy = sents2seqs('target', v_fr, onehot=True)\n","  v_de_x, v_de_y = v_de_xy[:,:-1,:], v_de_xy[:,1:,:]\n","  # Evaluate the trained model on the validation data\n","  res = nmt_tf.evaluate([v_en_x, v_de_x], v_de_y, batch_size=valid_size, verbose=0)\n","  print(\"{} => Loss:{}, Val Acc: {}\".format(ei+1,res[0], res[1]*100.0))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1 => Loss:0.7801529765129089, Val Acc: 100.0\n","2 => Loss:0.2447076439857483, Val Acc: 100.0\n","3 => Loss:0.11238637566566467, Val Acc: 100.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"psox-CmwcL2R","colab_type":"text"},"source":["### Generating translations from the model\n"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"9edb49e3-db60-4e66-96ce-33a6dc359389","executionInfo":{"status":"ok","timestamp":1573271571934,"user_tz":-480,"elapsed":1082,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"id":"kxAJY11xSjRa","colab":{"base_uri":"https://localhost:8080/","height":567}},"source":["import matplotlib.pyplot as plt\n","img = plt.imread('ch49_recursive_decoder.png')\n","fig= plt.figure(figsize=(8,10))\n","plt.imshow(img)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f428adf3c88>"]},"metadata":{"tags":[]},"execution_count":132},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAeYAAAIVCAYAAAAEfVzlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 576x720 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"jXmu_gy7cA1I","colab_type":"code","outputId":"75c905c6-e465-4daf-8e07-2d3e5048f5bd","executionInfo":{"status":"ok","timestamp":1573271583459,"user_tz":-480,"elapsed":914,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["import tensorflow.keras.layers as layers\n","from tensorflow.keras.models import Model\n","# Define an input layer that accepts a single onehot encoded word\n","de_inputs = layers.Input(shape=(1, fr_vocab))\n","# Define an input to accept the t-1 state\n","de_state_in = layers.Input(shape=(hsize,))\n","de_gru = layers.GRU(hsize, return_state=True)\n","# Get the output and state from the GRU layer\n","de_out, de_state_out = de_gru(de_inputs, initial_state=de_state_in)\n","de_dense = layers.Dense(fr_vocab, activation='softmax')\n","de_pred = de_dense(de_out)\n","\n","# Define a model\n","decoder = Model(inputs=[de_inputs, de_state_in], outputs=[de_pred, de_state_out])\n","print(decoder.summary())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"model_12\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_14 (InputLayer)           [(None, 1, 250)]     0                                            \n","__________________________________________________________________________________________________\n","input_15 (InputLayer)           [(None, 64)]         0                                            \n","__________________________________________________________________________________________________\n","gru_17 (GRU)                    [(None, 64), (None,  60480       input_14[0][0]                   \n","                                                                 input_15[0][0]                   \n","__________________________________________________________________________________________________\n","dense_8 (Dense)                 (None, 250)          16250       gru_17[0][0]                     \n","==================================================================================================\n","Total params: 76,730\n","Trainable params: 76,730\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UJHzGtErS1Hj","colab_type":"text"},"source":["####Link between the trained and inference model\n","Here you will be transferring the trained weights from the trained model to the inference model. In the encoder decoder model, there are three layers with parameters. They are,\n","\n","- The encoder `GRU` layer\n","- The decoder `GRU` layer\n","- The decoder `Dense` layer\n","\n","The other layers, such as `TimeDistributed` do not have any parameters, thus don't require the copying of weights."]},{"cell_type":"code","metadata":{"id":"jorHGc6UTCYq","colab_type":"code","colab":{}},"source":["tr_en_gru = GRU"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FcXxmtbTTHIU","colab_type":"text"},"source":["For this exercise, you have been provided with \n","\n","1. the trained encoder `GRU` layer (`tr_en_gru`), \n","2. trained decoder `GRU` (`tr_de_gru`) and \n","3. the trained `Dense` layer (`tr_de_dense`). \n","\n","You also have access to all the layers of the inference model (including the encoder) such as the encoder `GRU` layer (`en_gru`), decoder `GRU` (`de_gru`) and the `Dense` layer (`de_dense`)."]},{"cell_type":"code","metadata":{"id":"MKbTOL0mcPj7","colab_type":"code","colab":{}},"source":["# [model not available]\n","# Load the weights to the encoder GRU from the trained model\n","en_gru_w = tr_en_gru.get_weights()\n","# Set the weights of the encoder GRU of the inference model\n","en_gru.set_weights(en_gru_w)\n","# Load and set the weights to the decoder GRU\n","de_gru.set_weights(tr_de_gru.get_weights())\n","# Load and set the weights to the decoder Dense\n","de_dense.set_weights(tr_de_dense.get_weights())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_D-kBKRZTa51","colab_type":"text"},"source":["####Generating translations\n","You will now be generating French translations using an inference model trained using Teacher Forcing."]},{"cell_type":"code","metadata":{"id":"2tbaUaAPTjrI","colab_type":"code","colab":{}},"source":["def word2onehot(tokenizer, word, vocab_size):\n","    de_seq = tokenizer.texts_to_sequences([[word]])\n","    de_onehot = to_categorical(de_seq, num_classes=vocab_size)\n","    de_onehot = np.expand_dims(de_onehot, axis=1)    \n","    return de_onehot"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WcsbMNsVTmw6","colab_type":"code","colab":{}},"source":["def probs2word(probs, tok):\n","    wid = np.argmax(probs[0,:], axis=-1)\n","    w = tok.index_word[wid]\n","    return w"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w9mlDd_KTnep","colab_type":"text"},"source":["This model (`nmt_tf`) has been trained for 50 epochs on 100,000 sentences which achieved around 98% accuracy on a 35000+ validation set. It might take longer for this exercise to initialize as the trained model needs to be loaded."]},{"cell_type":"code","metadata":{"id":"ILFivDy9Trtd","colab_type":"code","colab":{}},"source":["# not available"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-LXCorLcST6","colab_type":"code","colab":{}},"source":["en_sent = ['the united states is sometimes chilly during december , but it is sometimes freezing in june .']\n","print('English: {}'.format(en_sent))\n","en_seq = sents2seqs('source', en_sent, onehot=True, reverse=True)\n","# Predict the initial decoder state with the encoder\n","de_s_t = encoder.predict(en_seq)\n","de_seq = word2onehot(fr_tok, 'sos', fr_vocab)\n","fr_sent = ''\n","for i in range(fr_len):    \n","  # Predict from the decoder and recursively assign the new state to de_s_t\n","  de_prob, de_s_t = decoder.predict([de_seq,de_s_t])\n","  # Get the word from the probability output using probs2word\n","  de_w = probs2word(de_prob, fr_tok)\n","  # Convert the word to a onehot sequence using word2onehot\n","  de_seq = word2onehot(fr_tok, de_w, fr_vocab)\n","  if de_w == 'eos': break\n","  fr_sent += de_w + ' '\n","print(\"French (Ours): {}\".format(fr_sent))\n","print(\"French (Google Translate): les etats-unis sont parfois froids en décembre, mais parfois gelés en juin\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LyS7hd5WcdBl","colab_type":"text"},"source":["### Using word embedding for machine translation\n"]},{"cell_type":"markdown","metadata":{"id":"gBWWzN-DTyDG","colab_type":"text"},"source":["####Measuring word vector similarity\n"]},{"cell_type":"code","metadata":{"id":"ZPg0MsWuT3gK","colab_type":"code","colab":{}},"source":["cat_vector = np.array([[ 0.39394888, -0.26333705,  0.08621896,  0.01091912, -0.32225883,\n","        -0.10658745, -0.25810188,  0.35068503, -0.21361879,  0.04625478,\n","         0.27992395,  0.34801784, -0.16187142, -0.52442133,  0.305441  ,\n","         0.2697149 ,  0.13099045, -0.13699648,  0.22602458,  0.4918219 ,\n","        -0.321026  ,  0.0057017 , -0.04981493, -0.02993789,  0.29878485,\n","        -0.3817054 , -0.05704023, -0.23727393,  0.08146162, -0.40114117,\n","         0.15975171,  0.23145257,  0.3185359 , -0.18709224, -0.2787781 ,\n","         0.21370722,  0.00497514,  0.04430564, -0.02908332, -0.17749819,\n","        -0.30718255, -0.25935414,  0.10115459,  0.01766969, -0.16098014,\n","         0.3519572 , -0.0734664 , -0.3723043 ,  0.37262923, -0.34300864,\n","        -0.07277397, -0.00659163,  0.23748   ,  0.13399196,  0.5749845 ,\n","         0.18539067,  0.20504089, -0.3285161 ,  0.12926023, -0.3795821 ,\n","         0.3177269 ,  0.03064232,  0.1554114 , -0.00926715,  0.02937708,\n","         0.16277929,  0.392671  ,  0.53800666, -0.45575112,  0.38556987,\n","         0.06706808, -0.3802721 ,  0.03560793, -0.46856695, -0.2644591 ,\n","        -0.14335115,  0.10971718, -0.06499432,  0.26991028, -0.32314897,\n","        -0.07378025,  0.3670569 ,  0.15895942,  0.4204117 ,  0.00254397,\n","        -0.31696656,  0.04942322,  0.1372362 ,  0.4173302 , -0.29748875,\n","        -0.0426416 ,  0.2565474 , -0.3946782 ,  0.0325617 ,  0.15673377,\n","         0.38802406]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ln1pvXfUDFz","colab_type":"code","colab":{}},"source":["dog_vector = np.array([[ 0.3996625 , -0.30058518,  0.04715299, -0.05910603, -0.11148381,\n","         0.1093993 ,  0.01090461,  0.23533289,  0.23022532, -0.10736388,\n","         0.27194744,  0.0234511 , -0.31267357, -0.3329223 ,  0.5383476 ,\n","         0.01967983, -0.01314637, -0.47769648,  0.21548632,  0.09172583,\n","        -0.21391751,  0.06045289, -0.33278054, -0.03726481,  0.3942353 ,\n","        -0.22838424, -0.14873335, -0.2946241 , -0.30747628,  0.01201936,\n","         0.3984858 ,  0.17620644,  0.4095585 , -0.27740127, -0.01438677,\n","        -0.12329695, -0.04173272, -0.10615015, -0.07224224, -0.48236308,\n","        -0.53165466, -0.22681382, -0.04323539,  0.36861056,  0.12315456,\n","         0.11744846,  0.10699135, -0.23141243,  0.61972225, -0.3322037 ,\n","        -0.33399817,  0.14830801,  0.5658622 , -0.11296458,  0.16682227,\n","        -0.09137795,  0.10273071, -0.25096542,  0.29488757, -0.2795514 ,\n","         0.30368486, -0.27580926, -0.14012972, -0.24056736,  0.24908563,\n","         0.14914584, -0.11725031,  0.24276581, -0.00545608,  0.45917222,\n","         0.03866611, -0.38174465, -0.36502177, -0.4143763 , -0.1863586 ,\n","        -0.45668072,  0.3324825 ,  0.12192825,  0.47594503, -0.18364649,\n","         0.42104712, -0.01100355, -0.18209179,  0.28632256, -0.00409962,\n","        -0.394309  , -0.00906276, -0.23715773,  0.3950311 , -0.5642396 ,\n","         0.42861956, -0.46919912, -0.3660173 , -0.05302884,  0.11614416,\n","         0.03718512]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wTSuvIpNT9cv","colab_type":"code","colab":{}},"source":["window_vector = np.array([[ 0.13325554,  0.14915967, -0.30721307,  0.09023898, -0.14277737,\n","        -0.3429877 ,  0.6591184 ,  0.22567806,  0.50057876, -0.14418888,\n","         0.7266859 ,  0.31579646, -0.04160513,  0.21206841,  0.53187066,\n","         0.04340999, -0.2023547 , -0.28836393, -0.03026843,  0.09326456,\n","        -0.15379508,  0.41042185, -0.3575508 ,  0.33814973, -0.03736347,\n","        -0.5686376 , -0.35828388, -0.09262004, -0.04406314, -0.30181015,\n","         0.02286413,  0.30441186,  0.05470408, -0.71816754,  0.13778108,\n","        -0.16295761,  0.09613513, -0.55940634, -0.57205784, -0.7506126 ,\n","        -0.31361568,  0.13012886,  0.57220346, -0.13439907, -0.22363257,\n","         0.42024654,  0.01096254, -0.2432713 ,  0.5192242 ,  0.28016105,\n","        -0.4941133 ,  0.39157686,  0.119226  , -0.35288376,  0.05466123,\n","        -0.0957669 , -0.0585629 ,  0.20194787, -0.03544503,  0.12973273,\n","         0.12909019,  0.03881926,  0.41618073, -0.31265515,  0.40158305,\n","         0.16749044, -0.25229704, -0.18045057, -0.11000848,  0.3967997 ,\n","         0.71698666, -0.7267666 , -0.08943647, -0.90438217, -0.07253361,\n","         0.00425288,  0.12160529,  0.02097287,  0.23188019, -0.22092348,\n","        -0.07865301,  0.27505714, -0.38818485, -0.37019807,  0.3313728 ,\n","        -0.03597212,  0.12701605, -0.7077018 ,  0.7482011 ,  0.24647832,\n","        -0.46840447, -0.314954  , -0.85316503, -0.2805526 , -0.08966991,\n","         0.5265477 ]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtzHJ8V9cX-s","colab_type":"code","outputId":"5f17f00d-bcf3-49bc-8775-491fb4228a94","executionInfo":{"status":"ok","timestamp":1573271922707,"user_tz":-480,"elapsed":740,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Print the length of the cat_vector\n","print('Length of the cat_vector: ', cat_vector.size)\n","\n","# Compute and print the similarity between cat and window vectors\n","dist_cat_window = cosine_similarity(cat_vector, window_vector)\n","print('Similarity(cat, window): ', dist_cat_window)\n","\n","# Compute and print the similarity between cat and dog vectors\n","print('Similarity(cat,dog): ', cosine_similarity(cat_vector, dog_vector))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Length of the cat_vector:  96\n","Similarity(cat, window):  [[0.32330843]]\n","Similarity(cat,dog):  [[0.60183563]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9uZ6ynrwUImS","colab_type":"text"},"source":["####Defining the embedding model\n","\n","You will be defining a Keras model that:\n","\n","- Uses `Embedding` layers\n","- Will be trained with Teacher Forcing\n","\n","This model will have two embedding layers; an encoder embedding layer and a decoder embedding layer. Furthermore, as the model is trained using Teacher Forcing, it will use a sequence length of `fr_len-1` in the decoder `Input` layer.\n","\n"]},{"cell_type":"code","metadata":{"id":"10R4-N7_UWFP","colab_type":"code","colab":{}},"source":["from tensorflow.python.keras.layers.embeddings import Embedding"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6LlJpHXecg0E","colab_type":"code","colab":{}},"source":["# Define an input layer which accepts a sequence of word IDs\n","en_inputs = Input(shape=(en_len,))\n","# Define an Embedding layer which accepts en_inputs\n","en_emb = Embedding(en_vocab, 96, input_length=en_len)(en_inputs)\n","en_out, en_state = GRU(hsize, return_state=True)(en_emb)\n","\n","de_inputs = Input(shape=(fr_len-1,))\n","# Define an Embedding layer which accepts de_inputs\n","de_emb = Embedding(fr_vocab, 96, input_length=fr_len-1)(de_inputs)\n","de_out, _ = GRU(hsize, return_sequences=True, return_state=True)(de_emb, initial_state=en_state)\n","de_pred = TimeDistributed(Dense(fr_vocab, activation='softmax'))(de_out)\n","\n","# Define the Model which accepts encoder/decoder inputs and outputs predictions \n","nmt_emb = Model([en_inputs, de_inputs], de_pred)\n","nmt_emb.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Tki55Gjcj1F","colab_type":"code","outputId":"1f9275fd-b771-4905-de9e-8b0de04708ee","executionInfo":{"status":"ok","timestamp":1573272068266,"user_tz":-480,"elapsed":7655,"user":{"displayName":"Zhenna LU","photoUrl":"","userId":"13596549313510787594"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["for ei in range(3):\n","  for i in range(0, train_size, bsize):    \n","    en_x = sents2seqs('source', tr_en[i:i+bsize], onehot=False, reverse=True)\n","    # Get a single batch of French sentences with no onehot encoding\n","    de_xy = sents2seqs('target', tr_fr[i:i+bsize], onehot=False)\n","    # Get all words except the last word in that batch\n","    de_x = de_xy[:,:-1]\n","    de_xy_oh = sents2seqs('target', tr_fr[i:i+bsize], onehot=True)\n","    # Get all words except the first from de_xy_oh\n","    de_y = de_xy_oh[:,1:,:]\n","    # Training the model on a single batch of data\n","    nmt_emb.train_on_batch([en_x, de_x], de_y)    \n","    res = nmt_emb.evaluate([en_x, de_x], de_y, batch_size=bsize, verbose=0)\n","    print(\"{} => Loss:{}, Train Acc: {}\".format(ei+1,res[0], res[1]*100.0))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1 => Loss:5.4861226081848145, Train Acc: 0.9333333000540733\n","1 => Loss:5.459052085876465, Train Acc: 100.0\n","1 => Loss:5.4310994148254395, Train Acc: 100.0\n","1 => Loss:5.402021408081055, Train Acc: 100.0\n","2 => Loss:5.371136665344238, Train Acc: 100.0\n","2 => Loss:5.338072299957275, Train Acc: 100.0\n","2 => Loss:5.3019185066223145, Train Acc: 100.0\n","2 => Loss:5.262421607971191, Train Acc: 100.0\n","3 => Loss:5.218705177307129, Train Acc: 100.0\n","3 => Loss:5.170106410980225, Train Acc: 100.0\n","3 => Loss:5.115215301513672, Train Acc: 100.0\n","3 => Loss:5.053520202636719, Train Acc: 100.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qus9TOZjcmoV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}